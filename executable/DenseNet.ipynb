{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/bamos/densenet.pytorch by Brandon Amos, J. Zico Kolter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a bug of PyTorch, let's wait for a while before reruning this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# default libraries\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# global parameters\n",
    "args = {\n",
    "    \"data\": \"../data/cifar/\",\n",
    "    \"cuda\": True,\n",
    "    \"seed\": 7,\n",
    "    \"workers\": 4,\n",
    "    \"optim\": \"sgd\", # adam, rmsprop\n",
    "    \"epochs\": 300,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 1e-1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"intermediate\": \"../intermediate/densenet/\"\n",
    "}\n",
    "args = argparse.Namespace(**args)\n",
    "\n",
    "if not os.path.isdir(args.data):\n",
    "    !mkdir $args.data\n",
    "    \n",
    "if not os.path.isdir(args.intermediate):\n",
    "    !mkdir $args.intermediate\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some layers\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"BN-ReLU-Conv(1x1)-BN-ReLU-Conv(3x3)\"\"\"\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        # it is necessary to init nn.Module\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        interChannels = 4*growthRate # as in the paper\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels) # batch normalization\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # F is function without weights!\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class SingleLayer(nn.Module):\n",
    "    \"\"\"BN-ReLU-Conv(3x3)\"\"\"\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Transition(nn.Module):\n",
    "    \"\"\"Bn-Conv(1x1)-Pooling(2x2)\"\"\"\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2) # pooling has no weight\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main model\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "            nChannels = 2*growthRate\n",
    "        else:\n",
    "            nChannels = 16\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks,\n",
    "                                       bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks,\n",
    "                                       bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks,\n",
    "                                       bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(nChannels, nClasses)\n",
    "        \n",
    "        # we initialize weights here\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    # dense block\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        # squeeze = flaten\n",
    "        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
    "        out = F.log_softmax(self.fc(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_opt(optAlg, optimizer, epoch):\n",
    "    if optAlg == \"sgd\":\n",
    "        if epoch == 150: lr = 1e-2\n",
    "        elif epoch == 225: lr = 1e-3\n",
    "        else: return\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(args, epoch, net, trainLoader, optimizer, trainF):\n",
    "    net.train() # effect on Dropout or BatchNorm\n",
    "    nProcessed = 0 # number of data has been processed\n",
    "    nTrain = len(trainLoader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(trainLoader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(async=True), target.cuda(async=True)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        output = net(data)\n",
    "        # because output is log-probability, the real loss will be\n",
    "        # the log-probability at target!\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # make_graph.save('/tmp/t.dot', loss.creator); assert(False)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        nProcessed += len(data)\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        incorrect = pred.ne(target.data).cpu().sum()\n",
    "        err = 100.*incorrect/len(data)\n",
    "        partialEpoch = epoch + batch_idx / len(trainLoader) - 1\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\t\"\n",
    "                  \"Loss: {:.6f}\\tError: {:.6f}\"\n",
    "                  .format(partialEpoch, nProcessed, nTrain,\n",
    "                          100. * batch_idx / len(trainLoader),\n",
    "                          loss.data[0], err))\n",
    "\n",
    "        trainF.write(\"{},{},{}\\n\".format(partialEpoch, loss.data[0], err))\n",
    "        trainF.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(args, epoch, net, testLoader, optimizer, testF):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    incorrect = 0\n",
    "    for data, target in testLoader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(async=True), target.cuda(async=True)\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = net(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        incorrect += pred.ne(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    # loss function already averages over batch size\n",
    "    test_loss /= len(testLoader)\n",
    "    nTotal = len(testLoader.dataset)\n",
    "    err = 100.*incorrect/nTotal\n",
    "    print(\"\\nTest set: Average loss: {:.4f}, Error: {}/{} ({:.0f}%)\\n\"\n",
    "          .format(test_loss, incorrect, nTotal, err))\n",
    "\n",
    "    testF.write(\"{},{},{}\\n\".format(epoch, test_loss, err))\n",
    "    testF.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normMean = [0.49139968, 0.48215827, 0.44653124]\n",
    "normStd = [0.24703233, 0.24348505, 0.26158768]\n",
    "normTransform = transforms.Normalize(normMean, normStd)\n",
    "\n",
    "trainTransform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normTransform\n",
    "])\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normTransform\n",
    "])\n",
    "\n",
    "kwargs = {\"pin_memory\": True} if args.cuda else {}\n",
    "trainLoader = DataLoader(\n",
    "    datasets.CIFAR10(root=args.data, train=True, download=True,\n",
    "                     transform=trainTransform),\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=args.workers,\n",
    "    **kwargs\n",
    ")\n",
    "testLoader = DataLoader(\n",
    "    datasets.CIFAR10(root=args.data, train=False,\n",
    "                     transform=testTransform),\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.workers,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  + Number of params: 769162\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "net = DenseNet(growthRate=12, depth=100, reduction=0.5,\n",
    "                        bottleneck=True, nClasses=10)\n",
    "print(\"  + Number of params: {}\".format(\n",
    "        sum([p.data.nelement() for p in net.parameters()])))\n",
    "if args.cuda:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(net.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "elif args.optim == \"adam\":\n",
    "    optimizer = optim.Adam(net.parameters(),\n",
    "                           lr=args.lr,\n",
    "                           weight_decay=args.weight_decay)\n",
    "elif args.optim == \"rmsprop\":\n",
    "    optimizer = optim.RMSprop(net.parameters(),\n",
    "                              lr=args.lr,\n",
    "                              weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save information\n",
    "trainF = open(os.path.join(args.intermediate, \"train.csv\"), \"w\")\n",
    "testF = open(os.path.join(args.intermediate, \"test.csv\"), \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0.00 [64/50000 (0%)]\tLoss: 2.417595\tError: 96.875000\n",
      "Train Epoch: 0.13 [6464/50000 (13%)]\tLoss: 2.267499\tError: 71.875000\n",
      "Train Epoch: 0.26 [12864/50000 (26%)]\tLoss: 2.252385\tError: 84.375000\n",
      "Train Epoch: 0.38 [19264/50000 (38%)]\tLoss: 2.199820\tError: 59.375000\n",
      "Train Epoch: 0.51 [25664/50000 (51%)]\tLoss: 1.846136\tError: 59.375000\n",
      "Train Epoch: 0.64 [32064/50000 (64%)]\tLoss: 1.882694\tError: 64.062500\n",
      "Train Epoch: 0.77 [38464/50000 (77%)]\tLoss: 1.924879\tError: 76.562500\n",
      "Train Epoch: 0.90 [44864/50000 (90%)]\tLoss: 1.999165\tError: 71.875000\n",
      "\n",
      "Test set: Average loss: 1.8923, Error: 7016/10000 (70%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/serialization.py:146: UserWarning: Couldn't retrieve source code for container of type DenseNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/serialization.py:146: UserWarning: Couldn't retrieve source code for container of type Bottleneck. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/serialization.py:146: UserWarning: Couldn't retrieve source code for container of type Transition. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1.00 [64/50000 (0%)]\tLoss: 1.926051\tError: 82.812500\n",
      "Train Epoch: 1.13 [6464/50000 (13%)]\tLoss: 2.009635\tError: 70.312500\n",
      "Train Epoch: 1.26 [12864/50000 (26%)]\tLoss: 1.923923\tError: 68.750000\n",
      "Train Epoch: 1.38 [19264/50000 (38%)]\tLoss: 1.792263\tError: 70.312500\n",
      "Train Epoch: 1.51 [25664/50000 (51%)]\tLoss: 1.943801\tError: 81.250000\n",
      "Train Epoch: 1.64 [32064/50000 (64%)]\tLoss: 1.833892\tError: 67.187500\n",
      "Train Epoch: 1.77 [38464/50000 (77%)]\tLoss: 1.876725\tError: 81.250000\n",
      "Train Epoch: 1.90 [44864/50000 (90%)]\tLoss: 1.746608\tError: 60.937500\n",
      "\n",
      "Test set: Average loss: 1.8319, Error: 6872/10000 (69%)\n",
      "\n",
      "Train Epoch: 2.00 [64/50000 (0%)]\tLoss: 1.875623\tError: 76.562500\n",
      "Train Epoch: 2.13 [6464/50000 (13%)]\tLoss: 1.810665\tError: 62.500000\n",
      "Train Epoch: 2.26 [12864/50000 (26%)]\tLoss: 2.047280\tError: 75.000000\n",
      "Train Epoch: 2.38 [19264/50000 (38%)]\tLoss: 1.973906\tError: 78.125000\n",
      "Train Epoch: 2.51 [25664/50000 (51%)]\tLoss: 1.884309\tError: 70.312500\n",
      "Train Epoch: 2.64 [32064/50000 (64%)]\tLoss: 1.942847\tError: 78.125000\n",
      "Train Epoch: 2.77 [38464/50000 (77%)]\tLoss: 2.071994\tError: 75.000000\n",
      "Train Epoch: 2.90 [44864/50000 (90%)]\tLoss: 1.924927\tError: 75.000000\n",
      "\n",
      "Test set: Average loss: 2.2125, Error: 7778/10000 (78%)\n",
      "\n",
      "Train Epoch: 3.00 [64/50000 (0%)]\tLoss: 1.751264\tError: 71.875000\n",
      "Train Epoch: 3.13 [6464/50000 (13%)]\tLoss: 1.846577\tError: 71.875000\n",
      "Train Epoch: 3.26 [12864/50000 (26%)]\tLoss: 2.116902\tError: 76.562500\n",
      "Train Epoch: 3.38 [19264/50000 (38%)]\tLoss: 2.009357\tError: 73.437500\n",
      "Train Epoch: 3.51 [25664/50000 (51%)]\tLoss: 2.255586\tError: 84.375000\n",
      "Train Epoch: 3.64 [32064/50000 (64%)]\tLoss: 2.304079\tError: 89.062500\n",
      "Train Epoch: 3.77 [38464/50000 (77%)]\tLoss: 2.306286\tError: 85.937500\n",
      "Train Epoch: 3.90 [44864/50000 (90%)]\tLoss: 2.302718\tError: 87.500000\n",
      "\n",
      "Test set: Average loss: 2.3071, Error: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 4.00 [64/50000 (0%)]\tLoss: 2.310016\tError: 95.312500\n",
      "Train Epoch: 4.13 [6464/50000 (13%)]\tLoss: 2.314270\tError: 95.312500\n",
      "Train Epoch: 4.26 [12864/50000 (26%)]\tLoss: 2.299921\tError: 95.312500\n",
      "Train Epoch: 4.38 [19264/50000 (38%)]\tLoss: 2.318760\tError: 93.750000\n",
      "Train Epoch: 4.51 [25664/50000 (51%)]\tLoss: 2.312215\tError: 93.750000\n",
      "Train Epoch: 4.64 [32064/50000 (64%)]\tLoss: 2.318595\tError: 90.625000\n",
      "Train Epoch: 4.77 [38464/50000 (77%)]\tLoss: 2.308638\tError: 85.937500\n",
      "Train Epoch: 4.90 [44864/50000 (90%)]\tLoss: 2.298712\tError: 90.625000\n",
      "\n",
      "Test set: Average loss: 2.3072, Error: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 5.00 [64/50000 (0%)]\tLoss: 2.309679\tError: 92.187500\n",
      "Train Epoch: 5.13 [6464/50000 (13%)]\tLoss: 2.318091\tError: 95.312500\n",
      "Train Epoch: 5.26 [12864/50000 (26%)]\tLoss: 2.306581\tError: 92.187500\n",
      "Train Epoch: 5.38 [19264/50000 (38%)]\tLoss: 2.319096\tError: 96.875000\n",
      "Train Epoch: 5.51 [25664/50000 (51%)]\tLoss: 2.308569\tError: 89.062500\n",
      "Train Epoch: 5.64 [32064/50000 (64%)]\tLoss: 2.313463\tError: 95.312500\n",
      "Train Epoch: 5.77 [38464/50000 (77%)]\tLoss: 2.290342\tError: 93.750000\n",
      "Train Epoch: 5.90 [44864/50000 (90%)]\tLoss: 2.303112\tError: 89.062500\n",
      "\n",
      "Test set: Average loss: 2.3066, Error: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 6.00 [64/50000 (0%)]\tLoss: 2.318485\tError: 90.625000\n",
      "Train Epoch: 6.13 [6464/50000 (13%)]\tLoss: 2.298375\tError: 87.500000\n",
      "Train Epoch: 6.26 [12864/50000 (26%)]\tLoss: 2.298363\tError: 87.500000\n",
      "Train Epoch: 6.38 [19264/50000 (38%)]\tLoss: 2.311685\tError: 89.062500\n",
      "Train Epoch: 6.51 [25664/50000 (51%)]\tLoss: 2.328732\tError: 96.875000\n",
      "Train Epoch: 6.64 [32064/50000 (64%)]\tLoss: 2.311789\tError: 87.500000\n",
      "Train Epoch: 6.77 [38464/50000 (77%)]\tLoss: 2.305522\tError: 87.500000\n",
      "Train Epoch: 6.90 [44864/50000 (90%)]\tLoss: 2.304186\tError: 87.500000\n",
      "\n",
      "Test set: Average loss: 2.3055, Error: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 7.00 [64/50000 (0%)]\tLoss: 2.309877\tError: 84.375000\n",
      "Train Epoch: 7.13 [6464/50000 (13%)]\tLoss: 2.306875\tError: 90.625000\n",
      "Train Epoch: 7.26 [12864/50000 (26%)]\tLoss: 2.314247\tError: 92.187500\n",
      "Train Epoch: 7.38 [19264/50000 (38%)]\tLoss: 2.292609\tError: 89.062500\n",
      "Train Epoch: 7.51 [25664/50000 (51%)]\tLoss: 2.308784\tError: 96.875000\n",
      "Train Epoch: 7.64 [32064/50000 (64%)]\tLoss: 2.284266\tError: 82.812500\n",
      "Train Epoch: 7.77 [38464/50000 (77%)]\tLoss: 2.293816\tError: 87.500000\n",
      "Train Epoch: 7.90 [44864/50000 (90%)]\tLoss: 2.307271\tError: 89.062500\n",
      "\n",
      "Test set: Average loss: 2.3069, Error: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 8.00 [64/50000 (0%)]\tLoss: 2.303473\tError: 87.500000\n",
      "Train Epoch: 8.13 [6464/50000 (13%)]\tLoss: 2.319045\tError: 92.187500\n",
      "Train Epoch: 8.26 [12864/50000 (26%)]\tLoss: 2.304478\tError: 93.750000\n",
      "Train Epoch: 8.38 [19264/50000 (38%)]\tLoss: 2.310031\tError: 90.625000\n",
      "Train Epoch: 8.51 [25664/50000 (51%)]\tLoss: 2.299074\tError: 79.687500\n",
      "Train Epoch: 8.64 [32064/50000 (64%)]\tLoss: 2.293453\tError: 87.500000\n",
      "Train Epoch: 8.77 [38464/50000 (77%)]\tLoss: 2.309785\tError: 92.187500\n",
      "Train Epoch: 8.90 [44864/50000 (90%)]\tLoss: 2.303815\tError: 87.500000\n",
      "\n",
      "Test set: Average loss: 2.3112, Error: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 9.00 [64/50000 (0%)]\tLoss: 2.330086\tError: 92.187500\n",
      "Train Epoch: 9.13 [6464/50000 (13%)]\tLoss: 2.301736\tError: 95.312500\n",
      "Train Epoch: 9.26 [12864/50000 (26%)]\tLoss: 2.295470\tError: 87.500000\n",
      "Train Epoch: 9.38 [19264/50000 (38%)]\tLoss: 2.313963\tError: 93.750000\n",
      "Train Epoch: 9.51 [25664/50000 (51%)]\tLoss: 2.309120\tError: 95.312500\n",
      "Train Epoch: 9.64 [32064/50000 (64%)]\tLoss: 2.301080\tError: 96.875000\n",
      "Train Epoch: 9.77 [38464/50000 (77%)]\tLoss: 2.305204\tError: 85.937500\n",
      "Train Epoch: 9.90 [44864/50000 (90%)]\tLoss: 2.297975\tError: 85.937500\n",
      "\n",
      "Test set: Average loss: 2.3054, Error: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 10.00 [64/50000 (0%)]\tLoss: 2.321634\tError: 95.312500\n",
      "Train Epoch: 10.13 [6464/50000 (13%)]\tLoss: 2.311918\tError: 93.750000\n",
      "Train Epoch: 10.26 [12864/50000 (26%)]\tLoss: 2.310266\tError: 96.875000\n",
      "Train Epoch: 10.38 [19264/50000 (38%)]\tLoss: 2.296719\tError: 90.625000\n",
      "Train Epoch: 10.51 [25664/50000 (51%)]\tLoss: 2.313383\tError: 95.312500\n",
      "Train Epoch: 10.64 [32064/50000 (64%)]\tLoss: 2.313383\tError: 96.875000\n",
      "Train Epoch: 10.77 [38464/50000 (77%)]\tLoss: 2.305885\tError: 90.625000\n",
      "Train Epoch: 10.90 [44864/50000 (90%)]\tLoss: 2.333726\tError: 95.312500\n",
      "\n",
      "Test set: Average loss: 2.3069, Error: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 11.00 [64/50000 (0%)]\tLoss: 2.297235\tError: 93.750000\n",
      "Train Epoch: 11.13 [6464/50000 (13%)]\tLoss: 2.322301\tError: 92.187500\n",
      "Train Epoch: 11.26 [12864/50000 (26%)]\tLoss: 2.332170\tError: 93.750000\n",
      "Train Epoch: 11.38 [19264/50000 (38%)]\tLoss: 2.314274\tError: 85.937500\n",
      "Train Epoch: 11.51 [25664/50000 (51%)]\tLoss: 2.312327\tError: 92.187500\n",
      "Train Epoch: 11.64 [32064/50000 (64%)]\tLoss: 2.286623\tError: 84.375000\n",
      "Train Epoch: 11.77 [38464/50000 (77%)]\tLoss: 2.289444\tError: 87.500000\n",
      "Train Epoch: 11.90 [44864/50000 (90%)]\tLoss: 2.310047\tError: 89.062500\n",
      "\n",
      "Test set: Average loss: 2.3094, Error: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 12.00 [64/50000 (0%)]\tLoss: 2.324024\tError: 95.312500\n",
      "Train Epoch: 12.13 [6464/50000 (13%)]\tLoss: 2.321792\tError: 95.312500\n",
      "Train Epoch: 12.26 [12864/50000 (26%)]\tLoss: 2.293777\tError: 92.187500\n",
      "Train Epoch: 12.38 [19264/50000 (38%)]\tLoss: 2.305449\tError: 90.625000\n",
      "Train Epoch: 12.51 [25664/50000 (51%)]\tLoss: 2.304476\tError: 89.062500\n",
      "Train Epoch: 12.64 [32064/50000 (64%)]\tLoss: 2.298618\tError: 84.375000\n",
      "Train Epoch: 12.77 [38464/50000 (77%)]\tLoss: 2.294064\tError: 87.500000\n",
      "Train Epoch: 12.90 [44864/50000 (90%)]\tLoss: 2.317790\tError: 89.062500\n",
      "\n",
      "Test set: Average loss: 2.3077, Error: 9000/10000 (90%)\n",
      "\n",
      "Train Epoch: 13.00 [64/50000 (0%)]\tLoss: 2.275691\tError: 84.375000\n",
      "Train Epoch: 13.13 [6464/50000 (13%)]\tLoss: 2.296783\tError: 95.312500\n",
      "Train Epoch: 13.26 [12864/50000 (26%)]\tLoss: 2.301864\tError: 87.500000\n",
      "Train Epoch: 13.38 [19264/50000 (38%)]\tLoss: 2.307481\tError: 85.937500\n",
      "Train Epoch: 13.51 [25664/50000 (51%)]\tLoss: 2.304617\tError: 93.750000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-105:\n",
      "Process Process-106:\n",
      "Process Process-108:\n",
      "Process Process-107:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-30e00c63379f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0madjust_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"latest.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2c07c6bf0f34>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, epoch, net, trainLoader, optimizer, trainF)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnProcessed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    adjust_opt(args.optim, optimizer, epoch)\n",
    "    train(args, epoch, net, trainLoader, optimizer, trainF)\n",
    "    test(args, epoch, net, testLoader, optimizer, testF)\n",
    "    torch.save(net, os.path.join(args.intermediate, \"latest.pth\"))\n",
    "\n",
    "trainF.close()\n",
    "testF.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
