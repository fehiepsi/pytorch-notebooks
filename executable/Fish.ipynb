{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T07:16:27.331896Z",
     "start_time": "2017-04-11T16:16:27.327588+09:00"
    },
    "deletable": true,
    "editable": true
   },
   "source": [
    "Source: https://github.com/rdcolema/nc-fish-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import default libraries\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torchnet.meter import AverageValueMeter, ClassErrorMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# global parameters\n",
    "args = {\n",
    "    \"arch\": \"resnet50\", # resnet101, resnet152, inception_v3\n",
    "    \"pretrained\": True,\n",
    "    \"data_path\": \"../data/fish/\",\n",
    "    \"cuda\": True,\n",
    "    \"optim\": \"adam\", # sgd, rmsprop\n",
    "    \"epochs\": 90,\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 1e-3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"seed\": 7,\n",
    "    \"workers\": 4,\n",
    "    \"nb_vals\": 450,\n",
    "    \"nb_augs\": 10,\n",
    "    \"intermediate_path\": '../intermediate/fish/'\n",
    "}\n",
    "args = argparse.Namespace(**args)\n",
    "\n",
    "if not os.path.isdir(args.data_path):\n",
    "    os.makedirs(args.data_path)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data folders\n",
    "traindir_full = os.path.join(args.data_path, \"train\")\n",
    "testdir = os.path.join(args.data_path, \"test_stg1\")\n",
    "\n",
    "# train/val/test/submit folders\n",
    "traindir = os.path.join(args.intermediate_path, \"train\" + str(args.nb_vals))\n",
    "valdir = os.path.join(args.intermediate_path, \"val\" + str(args.nb_vals))\n",
    "submission_path = os.path.join(args.intermediate_path, \"submissions\")\n",
    "# best model path\n",
    "model_best_filename = \"model_best_{0}vals_{1}augs_{2}.pth.tar\".format(\n",
    "    args.nb_vals, args.nb_augs, args.arch)\n",
    "model_best_filepath = os.path.join(args.intermediate_path,\n",
    "                                   model_best_filename)\n",
    "# get classes\n",
    "classes = sorted([x.split(\"/\")[-1] for x in glob.glob(traindir_full+\"/*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create intermediate folders, copy train data, and split\n",
    "if not os.path.isdir(traindir):\n",
    "    shutil.copytree(\"../data/train\", traindir)\n",
    "    \n",
    "if not os.path.isdir(valdir):\n",
    "    np.random.seed(args.seed)\n",
    "    g = glob.glob(traindir + \"/*/*.jpg\")\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(args.nb_vals):\n",
    "        os.renames(shuf[i], shuf[i].replace(\"train\", \"val\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using pre-trained model 'resnet50'\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "if args.pretrained:\n",
    "    print(\"=> Using pre-trained model '{}'\".format(args.arch))\n",
    "    model = models.__dict__[args.arch](pretrained=True)\n",
    "else:\n",
    "    print(\"=> Creating model '{}'\".format(args.arch))\n",
    "    model = models.__dict__[args.arch]()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# parameters of newly constructed modules have requires_grad=True by default\n",
    "# replace the last fully-connected layer\n",
    "model.fc = nn.Linear(2048, len(classes))\n",
    "# for 1 GPU, it is unnecessary to use DataParallel\n",
    "#model = torch.nn.DataParallel(model).cuda()\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename=\"checkpoint.pth.tar\"):\n",
    "    checkpoint_filepath = os.path.join(args.intermediate_path, filename)\n",
    "    torch.save(state, checkpoint_filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(checkpoint_filepath, model_best_filepath)\n",
    "\n",
    "def adjust_learning_rate(args, optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\n",
    "    \"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(args, train_loader, model, criterion, optimizer, epoch):\n",
    "    # turn on train mode\n",
    "    model.train()\n",
    "    \n",
    "    losses = AverageValueMeter()\n",
    "    top1 = ClassErrorMeter(accuracy=True) # accuracy instead of error\n",
    "    start = time.time()\n",
    "    \n",
    "    for i, (input, target) in enumerate(train_loader):      \n",
    "        # here we should call cuda() for input;\n",
    "        # in the ImageNet example, the model is parallel by\n",
    "        # torch.nn.DataParallel(model).cuda(), so no need to call cuda() there;\n",
    "        # the option async=True works with pin_memory of DataLoader\n",
    "        # pin_memory slows down DataLoader but fastens data transfer from\n",
    "        # CPU to GPU\n",
    "        if args.cuda:\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "        input_var = Variable(input)\n",
    "        target_var = Variable(target)\n",
    "\n",
    "        # compute output and loss\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # compute gradient and do backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.add(loss.data[0] * input.size(0), input.size(0))\n",
    "        top1.add(output.data, target)\n",
    "        \n",
    "    print(\"=> EPOCH {0} | Time: {1:.3f} | Accuracy: {2:.3f} | Loss: {3:.4f}\"\n",
    "          .format(epoch, time.time()-start,\n",
    "                  top1.value()[0], losses.value()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# validate function\n",
    "def validate(args, val_loader, model, criterion):\n",
    "    model.train(False) # turn off train mode\n",
    "    losses = AverageValueMeter()\n",
    "    top1 = ClassErrorMeter(accuracy=True)\n",
    "    start = time.time()\n",
    "    \n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        if args.cuda:\n",
    "            input = input.cuda(async=True)\n",
    "            target = target.cuda(async=True)\n",
    "        input_var = Variable(input, volatile=True) # no gradient\n",
    "        target_var = Variable(target, volatile=True)\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        losses.add(loss.data[0] * input.size(0), input.size(0))\n",
    "        top1.add(output.data, target)\n",
    "        \n",
    "    print(\"   * VALIDATE | Time: {0:.3f} | Accuracy: {1:.3f} | Loss: {2:.4f}\"\n",
    "          .format(time.time()-start, top1.value()[0], losses.value()[0]))\n",
    "    return top1.value()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    datasets.ImageFolder(traindir,\n",
    "                         transforms.Compose([\n",
    "                             transforms.Scale(400),\n",
    "                             transforms.RandomSizedCrop(224),\n",
    "                             transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(),\n",
    "                             normalize])),\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.workers,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    datasets.ImageFolder(valdir,\n",
    "                         transforms.Compose([\n",
    "                             transforms.Scale(400),\n",
    "                             transforms.RandomSizedCrop(224),\n",
    "                             transforms.ToTensor(),\n",
    "                             normalize])),\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if args.cuda:\n",
    "    criterion.cuda()\n",
    "\n",
    "# define optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(model.fc.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "elif args.optim == \"adam\":\n",
    "    optimizer = optim.Adam(model.fc.parameters(),\n",
    "                           lr=args.lr,\n",
    "                           weight_decay=args.weight_decay)\n",
    "elif args.optim == \"rmsprop\":\n",
    "    optimizer = optim.RMSprop(model.fc.parameters(),\n",
    "                              lr=args.lr,\n",
    "                              weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting to train on 'resnet50' model\n"
     ]
    }
   ],
   "source": [
    "if 1 == 1:\n",
    "    print(\"=> Starting to train on '{}' model\".format(args.arch))\n",
    "    best_prec1 = 0\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        adjust_learning_rate(args, optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(args, train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(args, val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            \"epoch\": epoch,\n",
    "            \"arch\": args.arch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"best_prec1\": best_prec1,\n",
    "        }, is_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TestImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        images = []\n",
    "        for filepath in sorted(glob.glob(root + \"/*.jpg\")):\n",
    "            images.append(filepath.split(\"/\")[-1])\n",
    "\n",
    "        self.root = root\n",
    "        self.imgs = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root, filename))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    TestImageFolder(testdir, \n",
    "                    transforms.Compose([\n",
    "                        transforms.Scale(400),\n",
    "                        transforms.RandomSizedCrop(224),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                        normalize])),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(args, test_loader, model):\n",
    "    # placeholder arrays for predictions and id column\n",
    "    preds = np.zeros(shape=(len(test_loader), len(classes)))\n",
    "    id_col = []\n",
    "    \n",
    "    # turn off train mode\n",
    "    model.train(False)\n",
    "    \n",
    "    # average predictions across several different augmentations\n",
    "    for aug in range(args.nb_augs):\n",
    "        print(\"   * Predicting on test augmentation {}\".format(aug + 1))\n",
    "        \n",
    "        # iterate through image data, one file at a time\n",
    "        # (assuming batch size set to 1)\n",
    "        for i, (input, filename) in enumerate(test_loader):\n",
    "            # batch_size = 1\n",
    "            filename = filename[0]\n",
    "                     \n",
    "            if args.cuda:\n",
    "                input = input.cuda()\n",
    "            input_var = Variable(input, volatile=True) # no gradient\n",
    "            output = model(input_var)\n",
    "            softmax = F.softmax(output)[0].data.cpu().numpy()\n",
    "            \n",
    "            # add the scaled class probabilities\n",
    "            preds[i] += softmax\n",
    "            if aug == 0:\n",
    "                id_col.append(filename)\n",
    "       \n",
    "    # convert averaged prediction array to pandas dataframe\n",
    "    preds /= args.nb_augs\n",
    "    pred = pd.DataFrame(preds, columns=[classes])\n",
    "    pred[\"image\"] = id_col\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"=> Starting to test on '{}' model\".format(args.arch))\n",
    "if os.path.isfile(model_best_filepath):\n",
    "    print(\"=> Loading checkpoint '{}'\".format(model_best_filename))\n",
    "    checkpoint = torch.load(model_best_filepath)\n",
    "    best_prec1 = checkpoint[\"best_prec1\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    print(\"=> Loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(model_best_filename, checkpoint[\"epoch\"]))\n",
    "    pred = test(args, test_loader, model)\n",
    "    # filename for our submission file w/ extra info about this test run\n",
    "    sub_fn = \"{0}epoches_{1}vals_{2}augs_{3}.csv\".format(\n",
    "        checkpoint[\"epoch\"], args.nb_vals, args.nb_augs, args.arch)\n",
    "    # write predictions to csv\n",
    "    pred.to_csv(os.path.join(submission_path, sub_fn), index=False)\n",
    "else:\n",
    "    print(\"=> No checkpoint found at '{}'\".format(model_best_filepath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
