{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Paper: https://arxiv.org/abs/1506.00196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:54.925299Z",
     "start_time": "2017-04-24T23:32:54.921876+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:54.966569Z",
     "start_time": "2017-04-24T23:32:54.927613+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import Levenshtein  # to compute phoneme error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:55.975071Z",
     "start_time": "2017-04-24T23:32:54.968879+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import torchtext.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:55.989639Z",
     "start_time": "2017-04-24T23:32:55.977844+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/cmudict/',\n",
    "    'epochs': 50,\n",
    "    'batch_size': 100,\n",
    "    'max_len': 20,\n",
    "    'beam_size': 3,\n",
    "    'd_embed': 500,\n",
    "    'd_hidden': 500,\n",
    "    'attention': True,\n",
    "    'log_every': 100,\n",
    "    'lr': 0.007,\n",
    "    'lr_decay': 0.5,\n",
    "    'lr_min': 1e-5,  # stop when lr is too low\n",
    "    'n_bad_loss': 5,  # number of bad loss before decaying\n",
    "    'clip': 2.3,  # clip gradient\n",
    "    'cuda': True,\n",
    "    'seed': 5,\n",
    "    'intermediate_path': '../intermediate/g2p/',\n",
    "}\n",
    "args = argparse.Namespace(**parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.076833Z",
     "start_time": "2017-04-24T23:32:55.992015+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "if not os.path.isdir(args.intermediate_path):\n",
    "    os.makedirs(args.intermediate_path)\n",
    "if not os.path.isdir(args.data_path):\n",
    "    URL = \"https://github.com/cmusphinx/cmudict/archive/master.zip\"\n",
    "    !wget $URL -O ../data/cmudict.zip\n",
    "    !unzip ../data/cmudict.zip -d ../data/\n",
    "    !mv ../data/cmudict-master $args.data_path\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.112244Z",
     "start_time": "2017-04-24T23:32:56.079605+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_embed, d_hidden):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
    "        self.lstm = nn.LSTMCell(d_embed, d_hidden)\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "    def forward(self, x_seq, cuda=False):\n",
    "        o = []\n",
    "        # dim(e_seq): len_seq x batch_size x d_embed\n",
    "        e_seq = self.embedding(x_seq)\n",
    "        tt = torch.cuda if cuda else torch\n",
    "        h = Variable(tt.FloatTensor(e_seq.size(1), self.d_hidden).zero_())\n",
    "        c = Variable(tt.FloatTensor(e_seq.size(1), self.d_hidden).zero_())\n",
    "        for e in e_seq.chunk(e_seq.size(0), 0):\n",
    "            e = e.squeeze(0)\n",
    "            h, c = self.lstm(e, (h, c))\n",
    "            o.append(h)\n",
    "        return torch.stack(o, 0), h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.154214Z",
     "start_time": "2017-04-24T23:32:56.114622+09:00"
    },
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# based on https://github.com/OpenNMT/OpenNMT-py\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Dot global attention from https://arxiv.org/abs/1508.04025\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear = nn.Linear(dim*2, dim, bias=False)\n",
    "        \n",
    "    def forward(self, x, context=None):\n",
    "        if context is None:\n",
    "            return x\n",
    "        assert x.size(0) == context.size(0)  # x: batch x dim\n",
    "        assert x.size(1) == context.size(2)  # context: batch x seq x dim\n",
    "        attn = F.softmax(context.bmm(x.unsqueeze(2)).squeeze(2))\n",
    "        weighted_context = attn.unsqueeze(1).bmm(context).squeeze(1)\n",
    "        o = self.linear(torch.cat((x, weighted_context), 1))\n",
    "        return F.tanh(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:46:07.410001Z",
     "start_time": "2017-04-25T10:46:07.392671+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_embed, d_hidden):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
    "        self.lstm = nn.LSTMCell(d_embed, d_hidden)\n",
    "        self.attn = Attention(d_hidden)\n",
    "        self.linear = nn.Linear(d_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x_seq, h, c, context=None):\n",
    "        o = []\n",
    "        e_seq = self.embedding(x_seq)\n",
    "        for e in e_seq.chunk(e_seq.size(0), 0):\n",
    "            e = e.squeeze(0)\n",
    "            h, c = self.lstm(e, (h, c))\n",
    "            o.append(self.attn(h, context))\n",
    "        o = torch.stack(o, 0)\n",
    "        o = self.linear(o.view(-1, h.size(1)))\n",
    "        return F.log_softmax(o).view(x_seq.size(0), -1, o.size(1)), h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:45:39.562766Z",
     "start_time": "2017-04-25T10:45:39.526240+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class G2P(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(G2P, self).__init__()\n",
    "        self.encoder = Encoder(config.g_size, config.d_embed,\n",
    "                               config.d_hidden)\n",
    "        self.decoder = Decoder(config.p_size, config.d_embed,\n",
    "                               config.d_hidden)\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, g_seq, p_seq=None):\n",
    "        o, h, c = self.encoder(g_seq, self.config.cuda)\n",
    "        context = o.t() if self.config.attention else None\n",
    "        if p_seq is not None:  # not generate\n",
    "            return self.decoder(p_seq, h, c, context)\n",
    "        else:  # start with <os> -> 1\n",
    "            assert g_seq.size(1) == 1  # make sure batch_size = 1\n",
    "            return self._generate(h, c, context)\n",
    "        \n",
    "    def _generate(self, h, c, context):\n",
    "        beam = Beam(self.config.beam_size, cuda=self.config.cuda)\n",
    "        h = h.expand(beam.size, h.size(1))\n",
    "        c = c.expand(beam.size, c.size(1))\n",
    "        context = context.expand(beam.size, context.size(1), context.size(2))\n",
    "        for i in range(self.config.max_len):  # max_len = 20\n",
    "            x = beam.get_current_state()  # beam to batch\n",
    "            o, h, c = self.decoder(Variable(x.unsqueeze(0)), h, c, context)\n",
    "            if beam.advance(o.data.squeeze(0)):\n",
    "                break\n",
    "            h.data.copy_(h.data.index_select(0, beam.get_current_origin()))\n",
    "            c.data.copy_(c.data.index_select(0, beam.get_current_origin()))\n",
    "        tt = torch.cuda if self.config.cuda else torch\n",
    "        return Variable(tt.LongTensor(beam.get_hyp(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.317700Z",
     "start_time": "2017-04-24T23:32:56.246853+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Code borrowed from PyTorch OpenNMT\n",
    "# https://github.com/OpenNMT/OpenNMT-py/\n",
    "# https://github.com/MaximumEntropy/Seq2Seq-PyTorch/\n",
    "\n",
    "class Beam(object):\n",
    "    \"\"\"Ordered beam of candidate outputs.\"\"\"\n",
    "\n",
    "    def __init__(self, size, pad=1, bos=2, eos=3, cuda=False):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        self.size = size\n",
    "        self.done = False\n",
    "        self.pad = pad\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "        self.tt = torch.cuda if cuda else torch\n",
    "\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = self.tt.FloatTensor(size).zero_()\n",
    "\n",
    "        # The backpointers at each time-step.\n",
    "        self.prevKs = []\n",
    "\n",
    "        # The outputs at each time-step.\n",
    "        self.nextYs = [self.tt.LongTensor(size).fill_(self.pad)]\n",
    "        self.nextYs[0][0] = self.bos\n",
    "\n",
    "    # Get the outputs for the current timestep.\n",
    "    def get_current_state(self):\n",
    "        \"\"\"Get state of beam.\"\"\"\n",
    "        return self.nextYs[-1]\n",
    "\n",
    "    # Get the backpointers for the current timestep.\n",
    "    def get_current_origin(self):\n",
    "        \"\"\"Get the backpointer to the beam at this step.\"\"\"\n",
    "        return self.prevKs[-1]\n",
    "\n",
    "    def advance(self, workd_lk):\n",
    "        \"\"\"Advance the beam.\"\"\"\n",
    "        num_words = workd_lk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prevKs) > 0:\n",
    "            beam_lk = workd_lk + self.scores.unsqueeze(1).expand_as(workd_lk)\n",
    "        else:\n",
    "            beam_lk = workd_lk[0]\n",
    "\n",
    "        flat_beam_lk = beam_lk.view(-1)\n",
    "\n",
    "        bestScores, bestScoresId = flat_beam_lk.topk(self.size, 0,\n",
    "                                                     True, True)\n",
    "        self.scores = bestScores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prev_k = bestScoresId / num_words\n",
    "        self.prevKs.append(prev_k)\n",
    "        self.nextYs.append(bestScoresId - prev_k * num_words)\n",
    "        # End condition is when top-of-beam is EOS.\n",
    "        if self.nextYs[-1][0] == self.eos:\n",
    "            self.done = True\n",
    "        return self.done\n",
    "\n",
    "    def get_hyp(self, k):\n",
    "        \"\"\"Get hypotheses.\"\"\"\n",
    "        hyp = []\n",
    "        # print(len(self.prevKs), len(self.nextYs), len(self.attn))\n",
    "        for j in range(len(self.prevKs) - 1, -1, -1):\n",
    "            hyp.append(self.nextYs[j + 1][k])\n",
    "            k = self.prevKs[j][k]\n",
    "        return hyp[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.366423Z",
     "start_time": "2017-04-24T23:32:56.319887+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CMUDict(data.Dataset):\n",
    "\n",
    "    def __init__(self, data_lines, g_field, p_field):\n",
    "        fields = [('grapheme', g_field), ('phoneme', p_field)]\n",
    "        examples = []  # maybe ignore '...-1' grapheme\n",
    "        for line in data_lines:\n",
    "            grapheme, phoneme = line.split(maxsplit=1)\n",
    "            examples.append(data.Example.fromlist([grapheme, phoneme],\n",
    "                                                  fields))\n",
    "        self.sort_key = lambda x: len(x.grapheme)\n",
    "        super(CMUDict, self).__init__(examples, fields)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, path, g_field, p_field, seed=None):\n",
    "        import random\n",
    "        \n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        with open(path) as f:\n",
    "            lines = f.readlines()\n",
    "        random.shuffle(lines)\n",
    "        train_lines, val_lines, test_lines = [], [], []\n",
    "        for i, line in enumerate(lines):\n",
    "            if i % 20 == 0:\n",
    "                val_lines.append(line)\n",
    "            elif i % 20 < 3:\n",
    "                test_lines.append(line)\n",
    "            else:\n",
    "                train_lines.append(line)\n",
    "        train_data = cls(train_lines, g_field, p_field)\n",
    "        val_data = cls(val_lines, g_field, p_field)\n",
    "        test_data = cls(test_lines, g_field, p_field)\n",
    "        return (train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.414698Z",
     "start_time": "2017-04-24T23:32:56.368711+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Code borrowed from https://github.com/SeanNaren/deepspeech.pytorch\n",
    "\n",
    "def phoneme_error_rate(p_seq1, p_seq2):\n",
    "    p_vocab = set(p_seq1 + p_seq2)\n",
    "    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n",
    "    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n",
    "    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n",
    "    return Levenshtein.distance(''.join(c_seq1),\n",
    "                                ''.join(c_seq2)) / len(c_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.453534Z",
     "start_time": "2017-04-24T23:32:56.417059+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr_decay):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.502586Z",
     "start_time": "2017-04-24T23:32:56.455962+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(config, train_iter, model, criterion, optimizer, epoch):\n",
    "    global iteration, n_total, train_loss, n_bad_loss\n",
    "    global init, best_val_loss, stop\n",
    "    \n",
    "    print(\"=> EPOCH {}\".format(epoch))\n",
    "    train_iter.init_epoch()\n",
    "    for batch in train_iter:\n",
    "        iteration += 1\n",
    "        model.train()\n",
    "        \n",
    "        output, _, __ = model(batch.grapheme, batch.phoneme[:-1].detach())\n",
    "        target = batch.phoneme[1:]\n",
    "        loss = criterion(output.view(output.size(0) * output.size(1), -1),\n",
    "                         target.view(target.size(0) * target.size(1)))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), config.clip, 'inf')\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_total += batch.batch_size\n",
    "        train_loss += loss.data[0] * batch.batch_size\n",
    "        \n",
    "        if iteration % config.log_every == 0:\n",
    "            train_loss /= n_total\n",
    "            val_loss = validate(val_iter, model, criterion)\n",
    "            print(\"   % Time: {:5.0f} | Iteration: {:5} | Batch: {:4}/{}\"\n",
    "                  \" | Train loss: {:.4f} | Val loss: {:.4f}\"\n",
    "                  .format(time.time()-init, iteration, train_iter.iterations,\n",
    "                          len(train_iter), train_loss, val_loss))\n",
    "            \n",
    "            n_total = train_loss = 0\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                n_bad_loss = 0\n",
    "                torch.save(model.state_dict(), config.best_model)\n",
    "            else:\n",
    "                n_bad_loss += 1\n",
    "            if n_bad_loss == config.n_bad_loss:\n",
    "                adjust_learning_rate(optimizer, config.lr_decay)\n",
    "                new_lr = optimizer.param_groups[0]['lr']\n",
    "                print(\"=> Adjust learning rate to: {}\".format(new_lr))\n",
    "                if new_lr < config.lr_min:\n",
    "                    stop = True\n",
    "                    break\n",
    "                n_bad_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.554170Z",
     "start_time": "2017-04-24T23:32:56.504939+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def validate(val_iter, model, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_iter.init_epoch()\n",
    "    for batch in val_iter:\n",
    "        output, _, __ = model(batch.grapheme, batch.phoneme[:-1])\n",
    "        target = batch.phoneme[1:]\n",
    "        loss = criterion(output.squeeze(1), target.squeeze(1))\n",
    "        val_loss += loss.data[0] * batch.batch_size\n",
    "    return val_loss / len(val_iter.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.602569Z",
     "start_time": "2017-04-24T23:32:56.556800+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(test_iter, model, criterion):\n",
    "    model.eval()\n",
    "    test_iter.init_epoch()\n",
    "    test_per = test_wer = 0\n",
    "    for batch in test_iter:\n",
    "        output = model(batch.grapheme).data.tolist()\n",
    "        target = batch.phoneme[1:].squeeze(1).data.tolist()\n",
    "        # calculate per, wer here\n",
    "        per = phoneme_error_rate(output, target) \n",
    "        wer = int(output != target)\n",
    "        test_per += per  # batch_size = 1\n",
    "        test_wer += wer\n",
    "        \n",
    "    test_per = test_per / len(test_iter.dataset) * 100\n",
    "    test_wer = test_wer / len(test_iter.dataset) * 100\n",
    "    print(\"Phoneme error rate (PER): {:.2f}\\nWord error rate (WER): {:.2f}\"\n",
    "          .format(test_per, test_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.639727Z",
     "start_time": "2017-04-24T23:32:56.604946+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def show(batch, model):\n",
    "    assert batch.batch_size == 1\n",
    "    g_field = batch.dataset.fields['grapheme']\n",
    "    p_field = batch.dataset.fields['phoneme']\n",
    "    prediction = model(batch.grapheme).data.tolist()[:-1]\n",
    "    grapheme = batch.grapheme.squeeze(1).data.tolist()[1:][::-1]\n",
    "    phoneme = batch.phoneme.squeeze(1).data.tolist()[1:-1]\n",
    "    print(\"> {}\\n= {}\\n< {}\\n\".format(\n",
    "        ''.join([g_field.vocab.itos[g] for g in grapheme]),\n",
    "        ' '.join([p_field.vocab.itos[p] for p in phoneme]),\n",
    "        ' '.join([p_field.vocab.itos[p] for p in prediction])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:32:56.681704Z",
     "start_time": "2017-04-24T23:32:56.642173+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g_field = data.Field(init_token='<s>',\n",
    "                     tokenize=(lambda x: list(x.split('(')[0])[::-1]))\n",
    "p_field = data.Field(init_token='<os>', eos_token='</os>',\n",
    "                     tokenize=(lambda x: x.split('#')[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:33:02.876140Z",
     "start_time": "2017-04-24T23:32:56.684170+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(args.data_path, 'cmudict.dict')\n",
    "train_data, val_data, test_data = CMUDict.splits(filepath, g_field, p_field,\n",
    "                                                 args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:33:04.772355Z",
     "start_time": "2017-04-24T23:33:02.878832+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g_field.build_vocab(train_data, val_data, test_data)\n",
    "p_field.build_vocab(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T14:33:04.780633Z",
     "start_time": "2017-04-24T23:33:04.774848+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "device = None if args.cuda else -1  # None is current gpu\n",
    "train_iter = data.BucketIterator(train_data, batch_size=args.batch_size,\n",
    "                                 repeat=False, device=device)\n",
    "val_iter = data.Iterator(val_data, batch_size=1,\n",
    "                         train=False, sort=False, device=device)\n",
    "test_iter = data.Iterator(test_data, batch_size=1,\n",
    "                          train=False, shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:46:17.443606Z",
     "start_time": "2017-04-25T10:46:17.365497+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = args\n",
    "config.g_size = len(g_field.vocab)\n",
    "config.p_size = len(p_field.vocab)\n",
    "config.best_model = os.path.join(config.intermediate_path,\n",
    "                                 \"best_model_adagrad_attn.pth\")\n",
    "\n",
    "model = G2P(config)\n",
    "criterion = nn.NLLLoss()\n",
    "if config.cuda:\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=config.lr)  # use Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T20:09:00.911873Z",
     "start_time": "2017-04-24T23:33:07.148276+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1\n",
      "   % Time:   107 | Iteration:   100 | Batch:  100/1148 | Train loss: 1.2808 | Val loss: 0.8004\n",
      "   % Time:   213 | Iteration:   200 | Batch:  200/1148 | Train loss: 0.5093 | Val loss: 0.5768\n",
      "   % Time:   320 | Iteration:   300 | Batch:  300/1148 | Train loss: 0.4187 | Val loss: 0.5115\n",
      "   % Time:   426 | Iteration:   400 | Batch:  400/1148 | Train loss: 0.3665 | Val loss: 0.4574\n",
      "   % Time:   533 | Iteration:   500 | Batch:  500/1148 | Train loss: 0.3454 | Val loss: 0.4275\n",
      "   % Time:   639 | Iteration:   600 | Batch:  600/1148 | Train loss: 0.3197 | Val loss: 0.4042\n",
      "   % Time:   745 | Iteration:   700 | Batch:  700/1148 | Train loss: 0.3122 | Val loss: 0.3906\n",
      "   % Time:   852 | Iteration:   800 | Batch:  800/1148 | Train loss: 0.3087 | Val loss: 0.3916\n",
      "   % Time:   959 | Iteration:   900 | Batch:  900/1148 | Train loss: 0.2912 | Val loss: 0.3749\n",
      "   % Time:  1065 | Iteration:  1000 | Batch: 1000/1148 | Train loss: 0.2893 | Val loss: 0.3565\n",
      "   % Time:  1172 | Iteration:  1100 | Batch: 1100/1148 | Train loss: 0.2731 | Val loss: 0.3559\n",
      "=> EPOCH 2\n",
      "   % Time:  1281 | Iteration:  1200 | Batch:   52/1148 | Train loss: 0.2726 | Val loss: 0.3450\n",
      "   % Time:  1391 | Iteration:  1300 | Batch:  152/1148 | Train loss: 0.2462 | Val loss: 0.3395\n",
      "   % Time:  1500 | Iteration:  1400 | Batch:  252/1148 | Train loss: 0.2456 | Val loss: 0.3348\n",
      "   % Time:  1607 | Iteration:  1500 | Batch:  352/1148 | Train loss: 0.2504 | Val loss: 0.3272\n",
      "   % Time:  1713 | Iteration:  1600 | Batch:  452/1148 | Train loss: 0.2486 | Val loss: 0.3255\n",
      "   % Time:  1819 | Iteration:  1700 | Batch:  552/1148 | Train loss: 0.2413 | Val loss: 0.3240\n",
      "   % Time:  1925 | Iteration:  1800 | Batch:  652/1148 | Train loss: 0.2412 | Val loss: 0.3156\n",
      "   % Time:  2035 | Iteration:  1900 | Batch:  752/1148 | Train loss: 0.2407 | Val loss: 0.3130\n",
      "   % Time:  2142 | Iteration:  2000 | Batch:  852/1148 | Train loss: 0.2262 | Val loss: 0.3124\n",
      "   % Time:  2248 | Iteration:  2100 | Batch:  952/1148 | Train loss: 0.2413 | Val loss: 0.3071\n",
      "   % Time:  2355 | Iteration:  2200 | Batch: 1052/1148 | Train loss: 0.2243 | Val loss: 0.3060\n",
      "=> EPOCH 3\n",
      "   % Time:  2462 | Iteration:  2300 | Batch:    4/1148 | Train loss: 0.2267 | Val loss: 0.3027\n",
      "   % Time:  2569 | Iteration:  2400 | Batch:  104/1148 | Train loss: 0.2165 | Val loss: 0.2989\n",
      "   % Time:  2676 | Iteration:  2500 | Batch:  204/1148 | Train loss: 0.2071 | Val loss: 0.2993\n",
      "   % Time:  2785 | Iteration:  2600 | Batch:  304/1148 | Train loss: 0.2120 | Val loss: 0.2944\n",
      "   % Time:  2895 | Iteration:  2700 | Batch:  404/1148 | Train loss: 0.2170 | Val loss: 0.2950\n",
      "   % Time:  3004 | Iteration:  2800 | Batch:  504/1148 | Train loss: 0.2077 | Val loss: 0.2910\n",
      "   % Time:  3113 | Iteration:  2900 | Batch:  604/1148 | Train loss: 0.2127 | Val loss: 0.2912\n",
      "   % Time:  3223 | Iteration:  3000 | Batch:  704/1148 | Train loss: 0.2109 | Val loss: 0.2882\n",
      "   % Time:  3332 | Iteration:  3100 | Batch:  804/1148 | Train loss: 0.2124 | Val loss: 0.2872\n",
      "   % Time:  3441 | Iteration:  3200 | Batch:  904/1148 | Train loss: 0.2104 | Val loss: 0.2845\n",
      "   % Time:  3550 | Iteration:  3300 | Batch: 1004/1148 | Train loss: 0.2104 | Val loss: 0.2845\n",
      "   % Time:  3660 | Iteration:  3400 | Batch: 1104/1148 | Train loss: 0.2064 | Val loss: 0.2823\n",
      "=> EPOCH 4\n",
      "   % Time:  3769 | Iteration:  3500 | Batch:   56/1148 | Train loss: 0.1979 | Val loss: 0.2796\n",
      "   % Time:  3879 | Iteration:  3600 | Batch:  156/1148 | Train loss: 0.1944 | Val loss: 0.2798\n",
      "   % Time:  3987 | Iteration:  3700 | Batch:  256/1148 | Train loss: 0.1906 | Val loss: 0.2819\n",
      "   % Time:  4096 | Iteration:  3800 | Batch:  356/1148 | Train loss: 0.1970 | Val loss: 0.2794\n",
      "   % Time:  4205 | Iteration:  3900 | Batch:  456/1148 | Train loss: 0.1920 | Val loss: 0.2773\n",
      "   % Time:  4314 | Iteration:  4000 | Batch:  556/1148 | Train loss: 0.1922 | Val loss: 0.2756\n",
      "   % Time:  4420 | Iteration:  4100 | Batch:  656/1148 | Train loss: 0.1961 | Val loss: 0.2736\n",
      "   % Time:  4527 | Iteration:  4200 | Batch:  756/1148 | Train loss: 0.1955 | Val loss: 0.2734\n",
      "   % Time:  4633 | Iteration:  4300 | Batch:  856/1148 | Train loss: 0.1884 | Val loss: 0.2730\n",
      "   % Time:  4741 | Iteration:  4400 | Batch:  956/1148 | Train loss: 0.1940 | Val loss: 0.2714\n",
      "   % Time:  4850 | Iteration:  4500 | Batch: 1056/1148 | Train loss: 0.1920 | Val loss: 0.2722\n",
      "=> EPOCH 5\n",
      "   % Time:  4960 | Iteration:  4600 | Batch:    8/1148 | Train loss: 0.1920 | Val loss: 0.2699\n",
      "   % Time:  5069 | Iteration:  4700 | Batch:  108/1148 | Train loss: 0.1798 | Val loss: 0.2684\n",
      "   % Time:  5178 | Iteration:  4800 | Batch:  208/1148 | Train loss: 0.1815 | Val loss: 0.2692\n",
      "   % Time:  5288 | Iteration:  4900 | Batch:  308/1148 | Train loss: 0.1787 | Val loss: 0.2684\n",
      "   % Time:  5397 | Iteration:  5000 | Batch:  408/1148 | Train loss: 0.1776 | Val loss: 0.2674\n",
      "   % Time:  5506 | Iteration:  5100 | Batch:  508/1148 | Train loss: 0.1831 | Val loss: 0.2685\n",
      "   % Time:  5616 | Iteration:  5200 | Batch:  608/1148 | Train loss: 0.1812 | Val loss: 0.2652\n",
      "   % Time:  5725 | Iteration:  5300 | Batch:  708/1148 | Train loss: 0.1821 | Val loss: 0.2644\n",
      "   % Time:  5834 | Iteration:  5400 | Batch:  808/1148 | Train loss: 0.1836 | Val loss: 0.2644\n",
      "   % Time:  5943 | Iteration:  5500 | Batch:  908/1148 | Train loss: 0.1813 | Val loss: 0.2634\n",
      "   % Time:  6052 | Iteration:  5600 | Batch: 1008/1148 | Train loss: 0.1782 | Val loss: 0.2643\n",
      "   % Time:  6161 | Iteration:  5700 | Batch: 1108/1148 | Train loss: 0.1808 | Val loss: 0.2622\n",
      "=> EPOCH 6\n",
      "   % Time:  6271 | Iteration:  5800 | Batch:   60/1148 | Train loss: 0.1816 | Val loss: 0.2601\n",
      "   % Time:  6381 | Iteration:  5900 | Batch:  160/1148 | Train loss: 0.1695 | Val loss: 0.2604\n",
      "   % Time:  6489 | Iteration:  6000 | Batch:  260/1148 | Train loss: 0.1700 | Val loss: 0.2590\n",
      "   % Time:  6599 | Iteration:  6100 | Batch:  360/1148 | Train loss: 0.1665 | Val loss: 0.2613\n",
      "   % Time:  6708 | Iteration:  6200 | Batch:  460/1148 | Train loss: 0.1727 | Val loss: 0.2591\n",
      "   % Time:  6817 | Iteration:  6300 | Batch:  560/1148 | Train loss: 0.1713 | Val loss: 0.2600\n",
      "   % Time:  6924 | Iteration:  6400 | Batch:  660/1148 | Train loss: 0.1691 | Val loss: 0.2606\n",
      "   % Time:  7031 | Iteration:  6500 | Batch:  760/1148 | Train loss: 0.1689 | Val loss: 0.2588\n",
      "   % Time:  7140 | Iteration:  6600 | Batch:  860/1148 | Train loss: 0.1703 | Val loss: 0.2572\n",
      "   % Time:  7250 | Iteration:  6700 | Batch:  960/1148 | Train loss: 0.1713 | Val loss: 0.2569\n",
      "   % Time:  7358 | Iteration:  6800 | Batch: 1060/1148 | Train loss: 0.1683 | Val loss: 0.2558\n",
      "=> EPOCH 7\n",
      "   % Time:  7469 | Iteration:  6900 | Batch:   12/1148 | Train loss: 0.1715 | Val loss: 0.2555\n",
      "   % Time:  7579 | Iteration:  7000 | Batch:  112/1148 | Train loss: 0.1575 | Val loss: 0.2556\n",
      "   % Time:  7688 | Iteration:  7100 | Batch:  212/1148 | Train loss: 0.1613 | Val loss: 0.2590\n",
      "   % Time:  7797 | Iteration:  7200 | Batch:  312/1148 | Train loss: 0.1611 | Val loss: 0.2563\n",
      "   % Time:  7907 | Iteration:  7300 | Batch:  412/1148 | Train loss: 0.1637 | Val loss: 0.2565\n",
      "   % Time:  8016 | Iteration:  7400 | Batch:  512/1148 | Train loss: 0.1630 | Val loss: 0.2534\n",
      "   % Time:  8125 | Iteration:  7500 | Batch:  612/1148 | Train loss: 0.1627 | Val loss: 0.2541\n",
      "   % Time:  8234 | Iteration:  7600 | Batch:  712/1148 | Train loss: 0.1620 | Val loss: 0.2527\n",
      "   % Time:  8344 | Iteration:  7700 | Batch:  812/1148 | Train loss: 0.1653 | Val loss: 0.2536\n",
      "   % Time:  8453 | Iteration:  7800 | Batch:  912/1148 | Train loss: 0.1611 | Val loss: 0.2523\n",
      "   % Time:  8563 | Iteration:  7900 | Batch: 1012/1148 | Train loss: 0.1650 | Val loss: 0.2521\n",
      "   % Time:  8672 | Iteration:  8000 | Batch: 1112/1148 | Train loss: 0.1614 | Val loss: 0.2515\n",
      "=> EPOCH 8\n",
      "   % Time:  8779 | Iteration:  8100 | Batch:   64/1148 | Train loss: 0.1547 | Val loss: 0.2515\n",
      "   % Time:  8887 | Iteration:  8200 | Batch:  164/1148 | Train loss: 0.1583 | Val loss: 0.2501\n",
      "   % Time:  8996 | Iteration:  8300 | Batch:  264/1148 | Train loss: 0.1521 | Val loss: 0.2502\n",
      "   % Time:  9105 | Iteration:  8400 | Batch:  364/1148 | Train loss: 0.1542 | Val loss: 0.2509\n",
      "   % Time:  9213 | Iteration:  8500 | Batch:  464/1148 | Train loss: 0.1537 | Val loss: 0.2518\n",
      "   % Time:  9322 | Iteration:  8600 | Batch:  564/1148 | Train loss: 0.1556 | Val loss: 0.2502\n",
      "   % Time:  9432 | Iteration:  8700 | Batch:  664/1148 | Train loss: 0.1505 | Val loss: 0.2493\n",
      "   % Time:  9541 | Iteration:  8800 | Batch:  764/1148 | Train loss: 0.1582 | Val loss: 0.2514\n",
      "   % Time:  9651 | Iteration:  8900 | Batch:  864/1148 | Train loss: 0.1557 | Val loss: 0.2489\n",
      "   % Time:  9759 | Iteration:  9000 | Batch:  964/1148 | Train loss: 0.1544 | Val loss: 0.2488\n",
      "   % Time:  9866 | Iteration:  9100 | Batch: 1064/1148 | Train loss: 0.1543 | Val loss: 0.2483\n",
      "=> EPOCH 9\n",
      "   % Time:  9974 | Iteration:  9200 | Batch:   16/1148 | Train loss: 0.1578 | Val loss: 0.2490\n",
      "   % Time: 10085 | Iteration:  9300 | Batch:  116/1148 | Train loss: 0.1441 | Val loss: 0.2484\n",
      "   % Time: 10193 | Iteration:  9400 | Batch:  216/1148 | Train loss: 0.1507 | Val loss: 0.2470\n",
      "   % Time: 10300 | Iteration:  9500 | Batch:  316/1148 | Train loss: 0.1434 | Val loss: 0.2470\n",
      "   % Time: 10408 | Iteration:  9600 | Batch:  416/1148 | Train loss: 0.1477 | Val loss: 0.2470\n",
      "   % Time: 10516 | Iteration:  9700 | Batch:  516/1148 | Train loss: 0.1496 | Val loss: 0.2474\n",
      "   % Time: 10624 | Iteration:  9800 | Batch:  616/1148 | Train loss: 0.1475 | Val loss: 0.2493\n",
      "   % Time: 10736 | Iteration:  9900 | Batch:  716/1148 | Train loss: 0.1535 | Val loss: 0.2464\n",
      "   % Time: 10847 | Iteration: 10000 | Batch:  816/1148 | Train loss: 0.1472 | Val loss: 0.2476\n",
      "   % Time: 10956 | Iteration: 10100 | Batch:  916/1148 | Train loss: 0.1484 | Val loss: 0.2453\n",
      "   % Time: 11063 | Iteration: 10200 | Batch: 1016/1148 | Train loss: 0.1469 | Val loss: 0.2466\n",
      "   % Time: 11170 | Iteration: 10300 | Batch: 1116/1148 | Train loss: 0.1477 | Val loss: 0.2450\n",
      "=> EPOCH 10\n",
      "   % Time: 11277 | Iteration: 10400 | Batch:   68/1148 | Train loss: 0.1384 | Val loss: 0.2457\n",
      "   % Time: 11384 | Iteration: 10500 | Batch:  168/1148 | Train loss: 0.1414 | Val loss: 0.2464\n",
      "   % Time: 11492 | Iteration: 10600 | Batch:  268/1148 | Train loss: 0.1401 | Val loss: 0.2460\n",
      "   % Time: 11602 | Iteration: 10700 | Batch:  368/1148 | Train loss: 0.1423 | Val loss: 0.2446\n",
      "   % Time: 11710 | Iteration: 10800 | Batch:  468/1148 | Train loss: 0.1404 | Val loss: 0.2441\n",
      "   % Time: 11820 | Iteration: 10900 | Batch:  568/1148 | Train loss: 0.1430 | Val loss: 0.2459\n",
      "   % Time: 11929 | Iteration: 11000 | Batch:  668/1148 | Train loss: 0.1475 | Val loss: 0.2461\n",
      "   % Time: 12039 | Iteration: 11100 | Batch:  768/1148 | Train loss: 0.1423 | Val loss: 0.2444\n",
      "   % Time: 12147 | Iteration: 11200 | Batch:  868/1148 | Train loss: 0.1398 | Val loss: 0.2440\n",
      "   % Time: 12257 | Iteration: 11300 | Batch:  968/1148 | Train loss: 0.1413 | Val loss: 0.2447\n",
      "   % Time: 12363 | Iteration: 11400 | Batch: 1068/1148 | Train loss: 0.1403 | Val loss: 0.2439\n",
      "=> EPOCH 11\n",
      "   % Time: 12473 | Iteration: 11500 | Batch:   20/1148 | Train loss: 0.1462 | Val loss: 0.2445\n",
      "   % Time: 12581 | Iteration: 11600 | Batch:  120/1148 | Train loss: 0.1382 | Val loss: 0.2446\n",
      "   % Time: 12691 | Iteration: 11700 | Batch:  220/1148 | Train loss: 0.1347 | Val loss: 0.2443\n",
      "   % Time: 12800 | Iteration: 11800 | Batch:  320/1148 | Train loss: 0.1337 | Val loss: 0.2442\n",
      "   % Time: 12909 | Iteration: 11900 | Batch:  420/1148 | Train loss: 0.1400 | Val loss: 0.2447\n",
      "=> Adjust learning rate to: 0.0035\n",
      "   % Time: 13018 | Iteration: 12000 | Batch:  520/1148 | Train loss: 0.1351 | Val loss: 0.2416\n",
      "   % Time: 13127 | Iteration: 12100 | Batch:  620/1148 | Train loss: 0.1329 | Val loss: 0.2404\n",
      "   % Time: 13236 | Iteration: 12200 | Batch:  720/1148 | Train loss: 0.1333 | Val loss: 0.2412\n",
      "   % Time: 13345 | Iteration: 12300 | Batch:  820/1148 | Train loss: 0.1369 | Val loss: 0.2406\n",
      "   % Time: 13454 | Iteration: 12400 | Batch:  920/1148 | Train loss: 0.1315 | Val loss: 0.2406\n",
      "   % Time: 13561 | Iteration: 12500 | Batch: 1020/1148 | Train loss: 0.1343 | Val loss: 0.2400\n",
      "   % Time: 13668 | Iteration: 12600 | Batch: 1120/1148 | Train loss: 0.1332 | Val loss: 0.2384\n",
      "=> EPOCH 12\n",
      "   % Time: 13775 | Iteration: 12700 | Batch:   72/1148 | Train loss: 0.1328 | Val loss: 0.2397\n",
      "   % Time: 13881 | Iteration: 12800 | Batch:  172/1148 | Train loss: 0.1255 | Val loss: 0.2403\n",
      "   % Time: 13988 | Iteration: 12900 | Batch:  272/1148 | Train loss: 0.1280 | Val loss: 0.2400\n",
      "   % Time: 14095 | Iteration: 13000 | Batch:  372/1148 | Train loss: 0.1290 | Val loss: 0.2396\n",
      "   % Time: 14202 | Iteration: 13100 | Batch:  472/1148 | Train loss: 0.1283 | Val loss: 0.2398\n",
      "=> Adjust learning rate to: 0.00175\n",
      "   % Time: 14308 | Iteration: 13200 | Batch:  572/1148 | Train loss: 0.1280 | Val loss: 0.2392\n",
      "   % Time: 14417 | Iteration: 13300 | Batch:  672/1148 | Train loss: 0.1307 | Val loss: 0.2388\n",
      "   % Time: 14527 | Iteration: 13400 | Batch:  772/1148 | Train loss: 0.1281 | Val loss: 0.2389\n",
      "   % Time: 14636 | Iteration: 13500 | Batch:  872/1148 | Train loss: 0.1295 | Val loss: 0.2387\n",
      "   % Time: 14745 | Iteration: 13600 | Batch:  972/1148 | Train loss: 0.1292 | Val loss: 0.2383\n",
      "   % Time: 14855 | Iteration: 13700 | Batch: 1072/1148 | Train loss: 0.1254 | Val loss: 0.2384\n",
      "=> EPOCH 13\n",
      "   % Time: 14965 | Iteration: 13800 | Batch:   24/1148 | Train loss: 0.1291 | Val loss: 0.2379\n",
      "   % Time: 15074 | Iteration: 13900 | Batch:  124/1148 | Train loss: 0.1233 | Val loss: 0.2382\n",
      "   % Time: 15183 | Iteration: 14000 | Batch:  224/1148 | Train loss: 0.1220 | Val loss: 0.2384\n",
      "   % Time: 15292 | Iteration: 14100 | Batch:  324/1148 | Train loss: 0.1240 | Val loss: 0.2385\n",
      "   % Time: 15401 | Iteration: 14200 | Batch:  424/1148 | Train loss: 0.1264 | Val loss: 0.2385\n",
      "   % Time: 15509 | Iteration: 14300 | Batch:  524/1148 | Train loss: 0.1220 | Val loss: 0.2387\n",
      "=> Adjust learning rate to: 0.000875\n",
      "   % Time: 15615 | Iteration: 14400 | Batch:  624/1148 | Train loss: 0.1270 | Val loss: 0.2384\n",
      "   % Time: 15723 | Iteration: 14500 | Batch:  724/1148 | Train loss: 0.1257 | Val loss: 0.2383\n",
      "   % Time: 15830 | Iteration: 14600 | Batch:  824/1148 | Train loss: 0.1234 | Val loss: 0.2381\n",
      "   % Time: 15936 | Iteration: 14700 | Batch:  924/1148 | Train loss: 0.1254 | Val loss: 0.2379\n",
      "   % Time: 16043 | Iteration: 14800 | Batch: 1024/1148 | Train loss: 0.1233 | Val loss: 0.2381\n",
      "=> Adjust learning rate to: 0.0004375\n",
      "   % Time: 16150 | Iteration: 14900 | Batch: 1124/1148 | Train loss: 0.1230 | Val loss: 0.2379\n",
      "=> EPOCH 14\n",
      "   % Time: 16257 | Iteration: 15000 | Batch:   76/1148 | Train loss: 0.1202 | Val loss: 0.2379\n",
      "   % Time: 16364 | Iteration: 15100 | Batch:  176/1148 | Train loss: 0.1247 | Val loss: 0.2379\n",
      "   % Time: 16470 | Iteration: 15200 | Batch:  276/1148 | Train loss: 0.1212 | Val loss: 0.2379\n",
      "   % Time: 16577 | Iteration: 15300 | Batch:  376/1148 | Train loss: 0.1240 | Val loss: 0.2379\n",
      "   % Time: 16684 | Iteration: 15400 | Batch:  476/1148 | Train loss: 0.1237 | Val loss: 0.2379\n",
      "   % Time: 16790 | Iteration: 15500 | Batch:  576/1148 | Train loss: 0.1213 | Val loss: 0.2379\n",
      "   % Time: 16900 | Iteration: 15600 | Batch:  676/1148 | Train loss: 0.1218 | Val loss: 0.2378\n",
      "   % Time: 17009 | Iteration: 15700 | Batch:  776/1148 | Train loss: 0.1198 | Val loss: 0.2379\n",
      "   % Time: 17118 | Iteration: 15800 | Batch:  876/1148 | Train loss: 0.1225 | Val loss: 0.2379\n",
      "   % Time: 17228 | Iteration: 15900 | Batch:  976/1148 | Train loss: 0.1214 | Val loss: 0.2380\n",
      "   % Time: 17337 | Iteration: 16000 | Batch: 1076/1148 | Train loss: 0.1227 | Val loss: 0.2379\n",
      "=> EPOCH 15\n",
      "   % Time: 17447 | Iteration: 16100 | Batch:   28/1148 | Train loss: 0.1206 | Val loss: 0.2381\n",
      "=> Adjust learning rate to: 0.00021875\n",
      "   % Time: 17556 | Iteration: 16200 | Batch:  128/1148 | Train loss: 0.1215 | Val loss: 0.2381\n",
      "   % Time: 17665 | Iteration: 16300 | Batch:  228/1148 | Train loss: 0.1226 | Val loss: 0.2381\n",
      "   % Time: 17774 | Iteration: 16400 | Batch:  328/1148 | Train loss: 0.1206 | Val loss: 0.2381\n",
      "   % Time: 17883 | Iteration: 16500 | Batch:  428/1148 | Train loss: 0.1199 | Val loss: 0.2381\n",
      "   % Time: 17993 | Iteration: 16600 | Batch:  528/1148 | Train loss: 0.1198 | Val loss: 0.2381\n",
      "=> Adjust learning rate to: 0.000109375\n",
      "   % Time: 18101 | Iteration: 16700 | Batch:  628/1148 | Train loss: 0.1201 | Val loss: 0.2381\n",
      "   % Time: 18211 | Iteration: 16800 | Batch:  728/1148 | Train loss: 0.1248 | Val loss: 0.2380\n",
      "   % Time: 18321 | Iteration: 16900 | Batch:  828/1148 | Train loss: 0.1238 | Val loss: 0.2380\n",
      "   % Time: 18430 | Iteration: 17000 | Batch:  928/1148 | Train loss: 0.1235 | Val loss: 0.2380\n",
      "   % Time: 18539 | Iteration: 17100 | Batch: 1028/1148 | Train loss: 0.1219 | Val loss: 0.2379\n",
      "=> Adjust learning rate to: 5.46875e-05\n",
      "   % Time: 18648 | Iteration: 17200 | Batch: 1128/1148 | Train loss: 0.1245 | Val loss: 0.2379\n",
      "=> EPOCH 16\n",
      "   % Time: 18758 | Iteration: 17300 | Batch:   80/1148 | Train loss: 0.1197 | Val loss: 0.2380\n",
      "   % Time: 18867 | Iteration: 17400 | Batch:  180/1148 | Train loss: 0.1226 | Val loss: 0.2380\n",
      "   % Time: 18976 | Iteration: 17500 | Batch:  280/1148 | Train loss: 0.1215 | Val loss: 0.2380\n",
      "   % Time: 19082 | Iteration: 17600 | Batch:  380/1148 | Train loss: 0.1186 | Val loss: 0.2380\n",
      "=> Adjust learning rate to: 2.734375e-05\n",
      "   % Time: 19189 | Iteration: 17700 | Batch:  480/1148 | Train loss: 0.1214 | Val loss: 0.2380\n",
      "   % Time: 19298 | Iteration: 17800 | Batch:  580/1148 | Train loss: 0.1226 | Val loss: 0.2380\n",
      "   % Time: 19407 | Iteration: 17900 | Batch:  680/1148 | Train loss: 0.1219 | Val loss: 0.2380\n",
      "   % Time: 19514 | Iteration: 18000 | Batch:  780/1148 | Train loss: 0.1209 | Val loss: 0.2380\n",
      "   % Time: 19620 | Iteration: 18100 | Batch:  880/1148 | Train loss: 0.1213 | Val loss: 0.2380\n",
      "=> Adjust learning rate to: 1.3671875e-05\n",
      "   % Time: 19726 | Iteration: 18200 | Batch:  980/1148 | Train loss: 0.1212 | Val loss: 0.2380\n",
      "   % Time: 19834 | Iteration: 18300 | Batch: 1080/1148 | Train loss: 0.1229 | Val loss: 0.2380\n",
      "=> EPOCH 17\n",
      "   % Time: 19940 | Iteration: 18400 | Batch:   32/1148 | Train loss: 0.1222 | Val loss: 0.2380\n",
      "   % Time: 20047 | Iteration: 18500 | Batch:  132/1148 | Train loss: 0.1220 | Val loss: 0.2380\n",
      "   % Time: 20154 | Iteration: 18600 | Batch:  232/1148 | Train loss: 0.1201 | Val loss: 0.2380\n",
      "=> Adjust learning rate to: 6.8359375e-06\n"
     ]
    }
   ],
   "source": [
    "if 1 == 1:  # change to True to train\n",
    "    iteration = n_total = train_loss = n_bad_loss = 0\n",
    "    stop = False\n",
    "    best_val_loss = 10\n",
    "    init = time.time()\n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        train(config, train_iter, model, criterion, optimizer, epoch)\n",
    "        if stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:50:54.452231Z",
     "start_time": "2017-04-25T10:46:25.583270+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme error rate (PER): 9.13\n",
      "Word error rate (WER): 39.78\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(config.best_model))\n",
    "test(test_iter, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:50:54.749653Z",
     "start_time": "2017-04-25T10:50:54.454565+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> conclude\n",
      "= K AH0 N K L UW1 D\n",
      "< K AH0 N K L UW1 D\n",
      "\n",
      "> chafe\n",
      "= CH EY1 F\n",
      "< CH EY1 F\n",
      "\n",
      "> methanex\n",
      "= M EH1 TH AH0 N EH2 K S\n",
      "< M EH1 TH AH0 N EH2 K S\n",
      "\n",
      "> contrariness\n",
      "= K AA1 N T R EH0 R IY0 N AH0 S\n",
      "< K AH0 N T R EH1 R IY0 N AH0 S\n",
      "\n",
      "> majcher\n",
      "= M AE1 JH K ER0\n",
      "< M AE1 JH K ER0\n",
      "\n",
      "> geac\n",
      "= JH IY1 IY1 EY1 S IY1\n",
      "< G IY1 K\n",
      "\n",
      "> cholesterol\n",
      "= K AH0 L EH1 S T ER0 AH0 L\n",
      "< K AH0 L EH1 S T ER0 AO2 L\n",
      "\n",
      "> lawes\n",
      "= L AO1 Z\n",
      "< L AO1 Z\n",
      "\n",
      "> encyclopedias\n",
      "= IH0 N S AY2 K L AH0 P IY1 D IY0 AH0 Z\n",
      "< IH0 N S AY2 K L AH0 P IY1 D IY0 AH0 Z\n",
      "\n",
      "> pedantry\n",
      "= P EH1 D AH0 N T R IY0\n",
      "< P EH1 D AH0 N T R IY0\n",
      "\n",
      "> wale\n",
      "= W EY1 L\n",
      "< W EY1 L\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_iter.init_epoch()\n",
    "for i, batch in enumerate(test_iter):\n",
    "    show(batch, model)\n",
    "    if i == 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
