{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Paper: https://arxiv.org/abs/1506.00196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:17.562088Z",
     "start_time": "2017-04-24T01:00:17.558659+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:17.620850Z",
     "start_time": "2017-04-24T01:00:17.564469+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import Levenshtein  # to compute phoneme error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:18.628885Z",
     "start_time": "2017-04-24T01:00:17.623373+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import torchtext.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:18.642887Z",
     "start_time": "2017-04-24T01:00:18.631628+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/cmudict/',\n",
    "    'epochs': 50,\n",
    "    'batch_size': 100,\n",
    "    'max_len': 20,\n",
    "    'beam_size': 3,\n",
    "    'd_embed': 500,\n",
    "    'd_hidden': 500,\n",
    "    'log_every': 100,\n",
    "    'lr': 0.007,\n",
    "    'lr_decay': 0.5,\n",
    "    'lr_min': 1e-5,  # stop when lr is too low\n",
    "    'n_bad_loss': 5,  # number of bad loss before decaying\n",
    "    'clip': 2.3,  # clip gradient\n",
    "    'cuda': True,\n",
    "    'seed': 5,\n",
    "    'intermediate_path': '../intermediate/g2p/',\n",
    "}\n",
    "args = argparse.Namespace(**parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:18.735485Z",
     "start_time": "2017-04-24T01:00:18.645603+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "if not os.path.isdir(args.intermediate_path):\n",
    "    os.makedirs(args.intermediate_path)\n",
    "if not os.path.isdir(args.data_path):\n",
    "    URL = \"https://github.com/cmusphinx/cmudict/archive/master.zip\"\n",
    "    !wget $URL -O ../data/cmudict.zip\n",
    "    !unzip ../data/cmudict.zip -d ../data/\n",
    "    !mv ../data/cmudict-master $args.data_path\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:18.765842Z",
     "start_time": "2017-04-24T01:00:18.738385+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_embed, d_hidden):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
    "        self.lstm = nn.LSTMCell(d_embed, d_hidden)\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "    def forward(self, x_seq, cuda=False):\n",
    "        # dim(e_seq): len_seq x batch_size x d_embed\n",
    "        e_seq = self.embedding(x_seq)\n",
    "        tt = torch.cuda if config.cuda else torch\n",
    "        h = Variable(tt.FloatTensor(e_seq.size(1), self.d_hidden).zero_())\n",
    "        c = Variable(tt.FloatTensor(e_seq.size(1), self.d_hidden).zero_())\n",
    "        for e in e_seq.chunk(e_seq.size(0), 0):\n",
    "            e = e.squeeze(0)\n",
    "            h, c = self.lstm(e, (h, c))\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:18.800399Z",
     "start_time": "2017-04-24T01:00:18.768201+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_embed, d_hidden):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
    "        self.lstm = nn.LSTMCell(d_embed, d_hidden)\n",
    "        self.linear = nn.Linear(d_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x_seq, h, c):\n",
    "        o = []\n",
    "        e_seq = self.embedding(x_seq)\n",
    "        for e in e_seq.chunk(e_seq.size(0), 0):\n",
    "            e = e.squeeze(0)\n",
    "            h, c = self.lstm(e, (h, c))\n",
    "            o.append(h)\n",
    "        o = torch.stack(o, 0)\n",
    "        o = self.linear(o.view(-1, h.size(1)))\n",
    "        return F.log_softmax(o).view(x_seq.size(0), -1, o.size(1)), h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:18.853000Z",
     "start_time": "2017-04-24T01:00:18.802660+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class G2P(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(G2P, self).__init__()\n",
    "        self.encoder = Encoder(config.g_size, config.d_embed,\n",
    "                               config.d_hidden)\n",
    "        self.decoder = Decoder(config.p_size, config.d_embed,\n",
    "                               config.d_hidden)\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, g_seq, p_seq=None):\n",
    "        h, c = self.encoder(g_seq, self.config.cuda)\n",
    "        if p_seq is not None:  # not generate\n",
    "            return self.decoder(p_seq, h, c)\n",
    "        else:  # start with <os> -> 1\n",
    "            assert g_seq.size(1) == 1  # make sure batch_size = 1\n",
    "            return self.generate(h, c)\n",
    "        \n",
    "    def generate(self, h, c):\n",
    "        beam = Beam(self.config.beam_size, cuda=self.config.cuda)\n",
    "        h = h.expand(beam.size, h.size(1))\n",
    "        c = c.expand(beam.size, c.size(1))\n",
    "        for i in range(self.config.max_len):  # max_len = 20\n",
    "            x = beam.get_current_state()  # beam to batch\n",
    "            o, h, c = self.decoder(Variable(x.unsqueeze(0)), h, c)\n",
    "            if beam.advance(o.data.squeeze(0)):\n",
    "                break\n",
    "            h.data.copy_(h.data.index_select(0, beam.get_current_origin()))\n",
    "            c.data.copy_(c.data.index_select(0, beam.get_current_origin()))\n",
    "        tt = torch.cuda if config.cuda else torch\n",
    "        return Variable(tt.LongTensor(beam.get_hyp(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:18.926212Z",
     "start_time": "2017-04-24T01:00:18.855355+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Code borrowed from PyTorch OpenNMT\n",
    "# https://github.com/OpenNMT/OpenNMT-py/\n",
    "# https://github.com/MaximumEntropy/Seq2Seq-PyTorch/\n",
    "\n",
    "class Beam(object):\n",
    "    \"\"\"Ordered beam of candidate outputs.\"\"\"\n",
    "\n",
    "    def __init__(self, size, pad=1, bos=2, eos=3, cuda=False):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        self.size = size\n",
    "        self.done = False\n",
    "        self.pad = pad\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "        self.tt = torch.cuda if cuda else torch\n",
    "\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = self.tt.FloatTensor(size).zero_()\n",
    "\n",
    "        # The backpointers at each time-step.\n",
    "        self.prevKs = []\n",
    "\n",
    "        # The outputs at each time-step.\n",
    "        self.nextYs = [self.tt.LongTensor(size).fill_(self.pad)]\n",
    "        self.nextYs[0][0] = self.bos\n",
    "\n",
    "    # Get the outputs for the current timestep.\n",
    "    def get_current_state(self):\n",
    "        \"\"\"Get state of beam.\"\"\"\n",
    "        return self.nextYs[-1]\n",
    "\n",
    "    # Get the backpointers for the current timestep.\n",
    "    def get_current_origin(self):\n",
    "        \"\"\"Get the backpointer to the beam at this step.\"\"\"\n",
    "        return self.prevKs[-1]\n",
    "\n",
    "    def advance(self, workd_lk):\n",
    "        \"\"\"Advance the beam.\"\"\"\n",
    "        num_words = workd_lk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prevKs) > 0:\n",
    "            beam_lk = workd_lk + self.scores.unsqueeze(1).expand_as(workd_lk)\n",
    "        else:\n",
    "            beam_lk = workd_lk[0]\n",
    "\n",
    "        flat_beam_lk = beam_lk.view(-1)\n",
    "\n",
    "        bestScores, bestScoresId = flat_beam_lk.topk(self.size, 0,\n",
    "                                                     True, True)\n",
    "        self.scores = bestScores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prev_k = bestScoresId / num_words\n",
    "        self.prevKs.append(prev_k)\n",
    "        self.nextYs.append(bestScoresId - prev_k * num_words)\n",
    "        # End condition is when top-of-beam is EOS.\n",
    "        if self.nextYs[-1][0] == self.eos:\n",
    "            self.done = True\n",
    "        return self.done\n",
    "\n",
    "    def get_hyp(self, k):\n",
    "        \"\"\"Get hypotheses.\"\"\"\n",
    "        hyp = []\n",
    "        # print(len(self.prevKs), len(self.nextYs), len(self.attn))\n",
    "        for j in range(len(self.prevKs) - 1, -1, -1):\n",
    "            hyp.append(self.nextYs[j + 1][k])\n",
    "            k = self.prevKs[j][k]\n",
    "        return hyp[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:18.974005Z",
     "start_time": "2017-04-24T01:00:18.928569+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CMUDict(data.Dataset):\n",
    "\n",
    "    def __init__(self, data_lines, g_field, p_field):\n",
    "        fields = [('grapheme', g_field), ('phoneme', p_field)]\n",
    "        examples = []  # maybe ignore '...-1' grapheme\n",
    "        for line in data_lines:\n",
    "            grapheme, phoneme = line.split(maxsplit=1)\n",
    "            examples.append(data.Example.fromlist([grapheme, phoneme],\n",
    "                                                  fields))\n",
    "        self.sort_key = lambda x: len(x.grapheme)\n",
    "        super(CMUDict, self).__init__(examples, fields)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, path, g_field, p_field, seed=None):\n",
    "        import random\n",
    "        \n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        with open(path) as f:\n",
    "            lines = f.readlines()\n",
    "        random.shuffle(lines)\n",
    "        train_lines, val_lines, test_lines = [], [], []\n",
    "        for i, line in enumerate(lines):\n",
    "            if i % 20 == 0:\n",
    "                val_lines.append(line)\n",
    "            elif i % 20 < 3:\n",
    "                test_lines.append(line)\n",
    "            else:\n",
    "                train_lines.append(line)\n",
    "        train_data = cls(train_lines, g_field, p_field)\n",
    "        val_data = cls(val_lines, g_field, p_field)\n",
    "        test_data = cls(test_lines, g_field, p_field)\n",
    "        return (train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:19.022228Z",
     "start_time": "2017-04-24T01:00:18.976208+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Code borrowed from https://github.com/SeanNaren/deepspeech.pytorch\n",
    "\n",
    "def phoneme_error_rate(p_seq1, p_seq2):\n",
    "    p_vocab = set(p_seq1 + p_seq2)\n",
    "    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n",
    "    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n",
    "    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n",
    "    return Levenshtein.distance(''.join(c_seq1),\n",
    "                                ''.join(c_seq2)) / len(c_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:19.059529Z",
     "start_time": "2017-04-24T01:00:19.024401+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr_decay):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:19.109612Z",
     "start_time": "2017-04-24T01:00:19.061951+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(config, train_iter, model, criterion, optimizer, epoch):\n",
    "    global iteration, n_total, train_loss, n_bad_loss\n",
    "    global init, best_val_loss, stop\n",
    "    \n",
    "    print(\"=> EPOCH {}\".format(epoch))\n",
    "    train_iter.init_epoch()\n",
    "    for batch in train_iter:\n",
    "        iteration += 1\n",
    "        model.train()\n",
    "        \n",
    "        output, _, __ = model(batch.grapheme, batch.phoneme[:-1].detach())\n",
    "        target = batch.phoneme[1:]\n",
    "        loss = criterion(output.view(output.size(0) * output.size(1), -1),\n",
    "                         target.view(target.size(0) * target.size(1)))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), config.clip, 'inf')\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_total += batch.batch_size\n",
    "        train_loss += loss.data[0] * batch.batch_size\n",
    "        \n",
    "        if iteration % config.log_every == 0:\n",
    "            train_loss /= n_total\n",
    "            val_loss = validate(val_iter, model, criterion)\n",
    "            print(\"   % Time: {:5.0f} | Iteration: {:5} | Batch: {:4}/{}\"\n",
    "                  \" | Train loss: {:.4f} | Val loss: {:.4f}\"\n",
    "                  .format(time.time()-init, iteration, train_iter.iterations,\n",
    "                          len(train_iter), train_loss, val_loss))\n",
    "            \n",
    "            n_total = train_loss = 0\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                n_bad_loss = 0\n",
    "                torch.save(model.state_dict(), config.best_model)\n",
    "            else:\n",
    "                n_bad_loss += 1\n",
    "            if n_bad_loss == config.n_bad_loss:\n",
    "                adjust_learning_rate(optimizer, config.lr_decay)\n",
    "                new_lr = optimizer.param_groups[0]['lr']\n",
    "                print(\"=> Adjust learning rate to: {}\".format(new_lr))\n",
    "                if new_lr < config.lr_min:\n",
    "                    stop = True\n",
    "                    break\n",
    "                n_bad_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:19.161266Z",
     "start_time": "2017-04-24T01:00:19.112067+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def validate(val_iter, model, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_iter.init_epoch()\n",
    "    for batch in val_iter:\n",
    "        output, _, __ = model(batch.grapheme, batch.phoneme[:-1])\n",
    "        target = batch.phoneme[1:]\n",
    "        loss = criterion(output.squeeze(1), target.squeeze(1))\n",
    "        val_loss += loss.data[0] * batch.batch_size\n",
    "    return val_loss / len(val_iter.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T00:36:02.236943Z",
     "start_time": "2017-04-24T09:36:02.224375+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(test_iter, model, criterion):\n",
    "    model.eval()\n",
    "    test_iter.init_epoch()\n",
    "    test_per = test_wer = 0\n",
    "    for batch in test_iter:\n",
    "        output = model(batch.grapheme).data.tolist()\n",
    "        target = batch.phoneme[1:].squeeze(1).data.tolist()\n",
    "        # calculate per, wer here\n",
    "        per = phoneme_error_rate(output, target) \n",
    "        wer = int(output != target)\n",
    "        test_per += per  # batch_size = 1\n",
    "        test_wer += wer\n",
    "        \n",
    "    test_per = test_per / len(test_iter.dataset) * 100\n",
    "    test_wer = test_wer / len(test_iter.dataset) * 100\n",
    "    print(\"Phoneme error rate (PER): {:.2f}\\nWord error rate (WER): {:.2f}\"\n",
    "          .format(test_per, test_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:19.258189Z",
     "start_time": "2017-04-24T01:00:19.218235+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def show(batch, model):\n",
    "    assert batch.batch_size == 1\n",
    "    g_field = batch.dataset.fields['grapheme']\n",
    "    p_field = batch.dataset.fields['phoneme']\n",
    "    prediction = model(batch.grapheme).data.tolist()[:-1]\n",
    "    grapheme = batch.grapheme.squeeze(1).data.tolist()[1:][::-1]\n",
    "    phoneme = batch.phoneme.squeeze(1).data.tolist()[1:-1]\n",
    "    print(\"> {}\\n= {}\\n< {}\\n\".format(\n",
    "        ''.join([g_field.vocab.itos[g] for g in grapheme]),\n",
    "        ' '.join([p_field.vocab.itos[p] for p in phoneme]),\n",
    "        ' '.join([p_field.vocab.itos[p] for p in prediction])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:19.293850Z",
     "start_time": "2017-04-24T01:00:19.260532+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g_field = data.Field(init_token='<s>',\n",
    "                     tokenize=(lambda x: list(x.split('(')[0])[::-1]))\n",
    "p_field = data.Field(init_token='<os>', eos_token='</os>',\n",
    "                     tokenize=(lambda x: x.split('#')[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:24.994636Z",
     "start_time": "2017-04-24T01:00:19.296125+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(args.data_path, 'cmudict.dict')\n",
    "train_data, val_data, test_data = CMUDict.splits(filepath, g_field, p_field,\n",
    "                                                 args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:26.870318Z",
     "start_time": "2017-04-24T01:00:24.997243+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g_field.build_vocab(train_data, val_data, test_data)\n",
    "p_field.build_vocab(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:26.878699Z",
     "start_time": "2017-04-24T01:00:26.872881+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "device = None if args.cuda else -1  # None is current gpu\n",
    "train_iter = data.BucketIterator(train_data, batch_size=args.batch_size,\n",
    "                                 repeat=False, device=device)\n",
    "val_iter = data.Iterator(val_data, batch_size=1,\n",
    "                         train=False, device=device)\n",
    "test_iter = data.Iterator(test_data, batch_size=1,\n",
    "                          train=False, shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T16:00:29.223292Z",
     "start_time": "2017-04-24T01:00:26.881197+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = args\n",
    "config.g_size = len(g_field.vocab)\n",
    "config.p_size = len(p_field.vocab)\n",
    "config.best_model = os.path.join(config.intermediate_path,\n",
    "                                 \"best_model.pth\")\n",
    "\n",
    "model = G2P(config)\n",
    "criterion = nn.NLLLoss()\n",
    "if config.cuda:\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=config.lr)  # use Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T20:12:09.315799Z",
     "start_time": "2017-04-24T01:00:29.225686+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1\n",
      "   % Time:    82 | Iteration:   100 | Batch:  100/1148 | Train loss: 1.3299 | Val loss: 1.0419\n",
      "   % Time:   164 | Iteration:   200 | Batch:  200/1148 | Train loss: 0.6915 | Val loss: 0.7983\n",
      "   % Time:   245 | Iteration:   300 | Batch:  300/1148 | Train loss: 0.5534 | Val loss: 0.6517\n",
      "   % Time:   327 | Iteration:   400 | Batch:  400/1148 | Train loss: 0.4902 | Val loss: 0.5884\n",
      "   % Time:   408 | Iteration:   500 | Batch:  500/1148 | Train loss: 0.4505 | Val loss: 0.5449\n",
      "   % Time:   489 | Iteration:   600 | Batch:  600/1148 | Train loss: 0.4167 | Val loss: 0.5103\n",
      "   % Time:   570 | Iteration:   700 | Batch:  700/1148 | Train loss: 0.3899 | Val loss: 0.4865\n",
      "   % Time:   651 | Iteration:   800 | Batch:  800/1148 | Train loss: 0.3724 | Val loss: 0.4741\n",
      "   % Time:   733 | Iteration:   900 | Batch:  900/1148 | Train loss: 0.3576 | Val loss: 0.4532\n",
      "   % Time:   814 | Iteration:  1000 | Batch: 1000/1148 | Train loss: 0.3508 | Val loss: 0.4334\n",
      "   % Time:   895 | Iteration:  1100 | Batch: 1100/1148 | Train loss: 0.3401 | Val loss: 0.4206\n",
      "=> EPOCH 2\n",
      "   % Time:   978 | Iteration:  1200 | Batch:   52/1148 | Train loss: 0.3256 | Val loss: 0.4098\n",
      "   % Time:  1059 | Iteration:  1300 | Batch:  152/1148 | Train loss: 0.2895 | Val loss: 0.4005\n",
      "   % Time:  1141 | Iteration:  1400 | Batch:  252/1148 | Train loss: 0.2929 | Val loss: 0.3954\n",
      "   % Time:  1222 | Iteration:  1500 | Batch:  352/1148 | Train loss: 0.2957 | Val loss: 0.3863\n",
      "   % Time:  1304 | Iteration:  1600 | Batch:  452/1148 | Train loss: 0.2965 | Val loss: 0.3802\n",
      "   % Time:  1386 | Iteration:  1700 | Batch:  552/1148 | Train loss: 0.2748 | Val loss: 0.3731\n",
      "   % Time:  1467 | Iteration:  1800 | Batch:  652/1148 | Train loss: 0.2773 | Val loss: 0.3684\n",
      "   % Time:  1549 | Iteration:  1900 | Batch:  752/1148 | Train loss: 0.2796 | Val loss: 0.3645\n",
      "   % Time:  1630 | Iteration:  2000 | Batch:  852/1148 | Train loss: 0.2736 | Val loss: 0.3596\n",
      "   % Time:  1712 | Iteration:  2100 | Batch:  952/1148 | Train loss: 0.2761 | Val loss: 0.3535\n",
      "   % Time:  1793 | Iteration:  2200 | Batch: 1052/1148 | Train loss: 0.2562 | Val loss: 0.3497\n",
      "=> EPOCH 3\n",
      "   % Time:  1875 | Iteration:  2300 | Batch:    4/1148 | Train loss: 0.2591 | Val loss: 0.3443\n",
      "   % Time:  1957 | Iteration:  2400 | Batch:  104/1148 | Train loss: 0.2424 | Val loss: 0.3434\n",
      "   % Time:  2038 | Iteration:  2500 | Batch:  204/1148 | Train loss: 0.2381 | Val loss: 0.3429\n",
      "   % Time:  2118 | Iteration:  2600 | Batch:  304/1148 | Train loss: 0.2368 | Val loss: 0.3377\n",
      "   % Time:  2197 | Iteration:  2700 | Batch:  404/1148 | Train loss: 0.2408 | Val loss: 0.3334\n",
      "   % Time:  2277 | Iteration:  2800 | Batch:  504/1148 | Train loss: 0.2411 | Val loss: 0.3320\n",
      "   % Time:  2356 | Iteration:  2900 | Batch:  604/1148 | Train loss: 0.2430 | Val loss: 0.3296\n",
      "   % Time:  2436 | Iteration:  3000 | Batch:  704/1148 | Train loss: 0.2286 | Val loss: 0.3262\n",
      "   % Time:  2516 | Iteration:  3100 | Batch:  804/1148 | Train loss: 0.2295 | Val loss: 0.3245\n",
      "   % Time:  2596 | Iteration:  3200 | Batch:  904/1148 | Train loss: 0.2325 | Val loss: 0.3233\n",
      "   % Time:  2676 | Iteration:  3300 | Batch: 1004/1148 | Train loss: 0.2336 | Val loss: 0.3211\n",
      "   % Time:  2755 | Iteration:  3400 | Batch: 1104/1148 | Train loss: 0.2272 | Val loss: 0.3206\n",
      "=> EPOCH 4\n",
      "   % Time:  2835 | Iteration:  3500 | Batch:   56/1148 | Train loss: 0.2198 | Val loss: 0.3160\n",
      "   % Time:  2917 | Iteration:  3600 | Batch:  156/1148 | Train loss: 0.2225 | Val loss: 0.3137\n",
      "   % Time:  2998 | Iteration:  3700 | Batch:  256/1148 | Train loss: 0.2112 | Val loss: 0.3126\n",
      "   % Time:  3079 | Iteration:  3800 | Batch:  356/1148 | Train loss: 0.2101 | Val loss: 0.3121\n",
      "   % Time:  3161 | Iteration:  3900 | Batch:  456/1148 | Train loss: 0.2121 | Val loss: 0.3098\n",
      "   % Time:  3242 | Iteration:  4000 | Batch:  556/1148 | Train loss: 0.2069 | Val loss: 0.3082\n",
      "   % Time:  3324 | Iteration:  4100 | Batch:  656/1148 | Train loss: 0.2159 | Val loss: 0.3044\n",
      "   % Time:  3404 | Iteration:  4200 | Batch:  756/1148 | Train loss: 0.2064 | Val loss: 0.3046\n",
      "   % Time:  3483 | Iteration:  4300 | Batch:  856/1148 | Train loss: 0.2087 | Val loss: 0.3039\n",
      "   % Time:  3562 | Iteration:  4400 | Batch:  956/1148 | Train loss: 0.2122 | Val loss: 0.3034\n",
      "   % Time:  3642 | Iteration:  4500 | Batch: 1056/1148 | Train loss: 0.2092 | Val loss: 0.3029\n",
      "=> EPOCH 5\n",
      "   % Time:  3724 | Iteration:  4600 | Batch:    8/1148 | Train loss: 0.2021 | Val loss: 0.3002\n",
      "   % Time:  3805 | Iteration:  4700 | Batch:  108/1148 | Train loss: 0.1900 | Val loss: 0.2991\n",
      "   % Time:  3885 | Iteration:  4800 | Batch:  208/1148 | Train loss: 0.1931 | Val loss: 0.2990\n",
      "   % Time:  3966 | Iteration:  4900 | Batch:  308/1148 | Train loss: 0.1964 | Val loss: 0.2995\n",
      "   % Time:  4047 | Iteration:  5000 | Batch:  408/1148 | Train loss: 0.1935 | Val loss: 0.2953\n",
      "   % Time:  4129 | Iteration:  5100 | Batch:  508/1148 | Train loss: 0.1930 | Val loss: 0.2943\n",
      "   % Time:  4210 | Iteration:  5200 | Batch:  608/1148 | Train loss: 0.1968 | Val loss: 0.2934\n",
      "   % Time:  4292 | Iteration:  5300 | Batch:  708/1148 | Train loss: 0.1929 | Val loss: 0.2928\n",
      "   % Time:  4373 | Iteration:  5400 | Batch:  808/1148 | Train loss: 0.1877 | Val loss: 0.2922\n",
      "   % Time:  4455 | Iteration:  5500 | Batch:  908/1148 | Train loss: 0.1924 | Val loss: 0.2907\n",
      "   % Time:  4537 | Iteration:  5600 | Batch: 1008/1148 | Train loss: 0.1879 | Val loss: 0.2896\n",
      "   % Time:  4619 | Iteration:  5700 | Batch: 1108/1148 | Train loss: 0.1862 | Val loss: 0.2890\n",
      "=> EPOCH 6\n",
      "   % Time:  4701 | Iteration:  5800 | Batch:   60/1148 | Train loss: 0.1844 | Val loss: 0.2876\n",
      "   % Time:  4783 | Iteration:  5900 | Batch:  160/1148 | Train loss: 0.1760 | Val loss: 0.2885\n",
      "   % Time:  4864 | Iteration:  6000 | Batch:  260/1148 | Train loss: 0.1750 | Val loss: 0.2863\n",
      "   % Time:  4946 | Iteration:  6100 | Batch:  360/1148 | Train loss: 0.1742 | Val loss: 0.2877\n",
      "   % Time:  5027 | Iteration:  6200 | Batch:  460/1148 | Train loss: 0.1768 | Val loss: 0.2851\n",
      "   % Time:  5109 | Iteration:  6300 | Batch:  560/1148 | Train loss: 0.1756 | Val loss: 0.2843\n",
      "   % Time:  5191 | Iteration:  6400 | Batch:  660/1148 | Train loss: 0.1809 | Val loss: 0.2847\n",
      "   % Time:  5270 | Iteration:  6500 | Batch:  760/1148 | Train loss: 0.1787 | Val loss: 0.2841\n",
      "   % Time:  5350 | Iteration:  6600 | Batch:  860/1148 | Train loss: 0.1847 | Val loss: 0.2816\n",
      "   % Time:  5429 | Iteration:  6700 | Batch:  960/1148 | Train loss: 0.1801 | Val loss: 0.2812\n",
      "   % Time:  5510 | Iteration:  6800 | Batch: 1060/1148 | Train loss: 0.1722 | Val loss: 0.2817\n",
      "=> EPOCH 7\n",
      "   % Time:  5591 | Iteration:  6900 | Batch:   12/1148 | Train loss: 0.1806 | Val loss: 0.2803\n",
      "   % Time:  5671 | Iteration:  7000 | Batch:  112/1148 | Train loss: 0.1653 | Val loss: 0.2794\n",
      "   % Time:  5750 | Iteration:  7100 | Batch:  212/1148 | Train loss: 0.1672 | Val loss: 0.2803\n",
      "   % Time:  5830 | Iteration:  7200 | Batch:  312/1148 | Train loss: 0.1656 | Val loss: 0.2795\n",
      "   % Time:  5911 | Iteration:  7300 | Batch:  412/1148 | Train loss: 0.1625 | Val loss: 0.2794\n",
      "   % Time:  5993 | Iteration:  7400 | Batch:  512/1148 | Train loss: 0.1667 | Val loss: 0.2777\n",
      "   % Time:  6074 | Iteration:  7500 | Batch:  612/1148 | Train loss: 0.1685 | Val loss: 0.2782\n",
      "   % Time:  6156 | Iteration:  7600 | Batch:  712/1148 | Train loss: 0.1649 | Val loss: 0.2767\n",
      "   % Time:  6237 | Iteration:  7700 | Batch:  812/1148 | Train loss: 0.1697 | Val loss: 0.2764\n",
      "   % Time:  6317 | Iteration:  7800 | Batch:  912/1148 | Train loss: 0.1665 | Val loss: 0.2771\n",
      "   % Time:  6397 | Iteration:  7900 | Batch: 1012/1148 | Train loss: 0.1717 | Val loss: 0.2781\n",
      "   % Time:  6477 | Iteration:  8000 | Batch: 1112/1148 | Train loss: 0.1683 | Val loss: 0.2743\n",
      "=> EPOCH 8\n",
      "   % Time:  6556 | Iteration:  8100 | Batch:   64/1148 | Train loss: 0.1588 | Val loss: 0.2740\n",
      "   % Time:  6636 | Iteration:  8200 | Batch:  164/1148 | Train loss: 0.1585 | Val loss: 0.2754\n",
      "   % Time:  6715 | Iteration:  8300 | Batch:  264/1148 | Train loss: 0.1532 | Val loss: 0.2738\n",
      "   % Time:  6797 | Iteration:  8400 | Batch:  364/1148 | Train loss: 0.1587 | Val loss: 0.2733\n",
      "   % Time:  6879 | Iteration:  8500 | Batch:  464/1148 | Train loss: 0.1511 | Val loss: 0.2737\n",
      "   % Time:  6961 | Iteration:  8600 | Batch:  564/1148 | Train loss: 0.1581 | Val loss: 0.2723\n",
      "   % Time:  7042 | Iteration:  8700 | Batch:  664/1148 | Train loss: 0.1615 | Val loss: 0.2725\n",
      "   % Time:  7123 | Iteration:  8800 | Batch:  764/1148 | Train loss: 0.1602 | Val loss: 0.2718\n",
      "   % Time:  7202 | Iteration:  8900 | Batch:  864/1148 | Train loss: 0.1593 | Val loss: 0.2718\n",
      "   % Time:  7282 | Iteration:  9000 | Batch:  964/1148 | Train loss: 0.1566 | Val loss: 0.2709\n",
      "   % Time:  7361 | Iteration:  9100 | Batch: 1064/1148 | Train loss: 0.1576 | Val loss: 0.2705\n",
      "=> EPOCH 9\n",
      "   % Time:  7441 | Iteration:  9200 | Batch:   16/1148 | Train loss: 0.1541 | Val loss: 0.2694\n",
      "   % Time:  7521 | Iteration:  9300 | Batch:  116/1148 | Train loss: 0.1466 | Val loss: 0.2711\n",
      "   % Time:  7602 | Iteration:  9400 | Batch:  216/1148 | Train loss: 0.1498 | Val loss: 0.2709\n",
      "   % Time:  7684 | Iteration:  9500 | Batch:  316/1148 | Train loss: 0.1473 | Val loss: 0.2706\n",
      "   % Time:  7766 | Iteration:  9600 | Batch:  416/1148 | Train loss: 0.1483 | Val loss: 0.2704\n",
      "   % Time:  7847 | Iteration:  9700 | Batch:  516/1148 | Train loss: 0.1473 | Val loss: 0.2690\n",
      "   % Time:  7928 | Iteration:  9800 | Batch:  616/1148 | Train loss: 0.1476 | Val loss: 0.2699\n",
      "   % Time:  8009 | Iteration:  9900 | Batch:  716/1148 | Train loss: 0.1517 | Val loss: 0.2698\n",
      "   % Time:  8089 | Iteration: 10000 | Batch:  816/1148 | Train loss: 0.1483 | Val loss: 0.2691\n",
      "   % Time:  8169 | Iteration: 10100 | Batch:  916/1148 | Train loss: 0.1471 | Val loss: 0.2671\n",
      "   % Time:  8248 | Iteration: 10200 | Batch: 1016/1148 | Train loss: 0.1512 | Val loss: 0.2678\n",
      "   % Time:  8328 | Iteration: 10300 | Batch: 1116/1148 | Train loss: 0.1547 | Val loss: 0.2663\n",
      "=> EPOCH 10\n",
      "   % Time:  8408 | Iteration: 10400 | Batch:   68/1148 | Train loss: 0.1392 | Val loss: 0.2664\n",
      "   % Time:  8488 | Iteration: 10500 | Batch:  168/1148 | Train loss: 0.1419 | Val loss: 0.2672\n",
      "   % Time:  8567 | Iteration: 10600 | Batch:  268/1148 | Train loss: 0.1416 | Val loss: 0.2673\n",
      "   % Time:  8649 | Iteration: 10700 | Batch:  368/1148 | Train loss: 0.1395 | Val loss: 0.2667\n",
      "   % Time:  8730 | Iteration: 10800 | Batch:  468/1148 | Train loss: 0.1393 | Val loss: 0.2667\n",
      "=> Adjust learning rate to: 0.0035\n",
      "   % Time:  8812 | Iteration: 10900 | Batch:  568/1148 | Train loss: 0.1387 | Val loss: 0.2638\n",
      "   % Time:  8893 | Iteration: 11000 | Batch:  668/1148 | Train loss: 0.1386 | Val loss: 0.2640\n",
      "   % Time:  8975 | Iteration: 11100 | Batch:  768/1148 | Train loss: 0.1381 | Val loss: 0.2628\n",
      "   % Time:  9056 | Iteration: 11200 | Batch:  868/1148 | Train loss: 0.1401 | Val loss: 0.2628\n",
      "   % Time:  9136 | Iteration: 11300 | Batch:  968/1148 | Train loss: 0.1408 | Val loss: 0.2632\n",
      "   % Time:  9215 | Iteration: 11400 | Batch: 1068/1148 | Train loss: 0.1429 | Val loss: 0.2625\n",
      "=> EPOCH 11\n",
      "   % Time:  9295 | Iteration: 11500 | Batch:   20/1148 | Train loss: 0.1345 | Val loss: 0.2614\n",
      "   % Time:  9374 | Iteration: 11600 | Batch:  120/1148 | Train loss: 0.1277 | Val loss: 0.2618\n",
      "   % Time:  9454 | Iteration: 11700 | Batch:  220/1148 | Train loss: 0.1300 | Val loss: 0.2620\n",
      "   % Time:  9533 | Iteration: 11800 | Batch:  320/1148 | Train loss: 0.1319 | Val loss: 0.2623\n",
      "   % Time:  9614 | Iteration: 11900 | Batch:  420/1148 | Train loss: 0.1325 | Val loss: 0.2624\n",
      "   % Time:  9696 | Iteration: 12000 | Batch:  520/1148 | Train loss: 0.1291 | Val loss: 0.2627\n",
      "=> Adjust learning rate to: 0.00175\n",
      "   % Time:  9777 | Iteration: 12100 | Batch:  620/1148 | Train loss: 0.1367 | Val loss: 0.2616\n",
      "   % Time:  9857 | Iteration: 12200 | Batch:  720/1148 | Train loss: 0.1306 | Val loss: 0.2612\n",
      "   % Time:  9936 | Iteration: 12300 | Batch:  820/1148 | Train loss: 0.1294 | Val loss: 0.2608\n",
      "   % Time: 10018 | Iteration: 12400 | Batch:  920/1148 | Train loss: 0.1290 | Val loss: 0.2605\n",
      "   % Time: 10100 | Iteration: 12500 | Batch: 1020/1148 | Train loss: 0.1321 | Val loss: 0.2602\n",
      "   % Time: 10181 | Iteration: 12600 | Batch: 1120/1148 | Train loss: 0.1302 | Val loss: 0.2606\n",
      "=> EPOCH 12\n",
      "   % Time: 10263 | Iteration: 12700 | Batch:   72/1148 | Train loss: 0.1234 | Val loss: 0.2607\n",
      "   % Time: 10345 | Iteration: 12800 | Batch:  172/1148 | Train loss: 0.1245 | Val loss: 0.2609\n",
      "   % Time: 10426 | Iteration: 12900 | Batch:  272/1148 | Train loss: 0.1279 | Val loss: 0.2606\n",
      "   % Time: 10506 | Iteration: 13000 | Batch:  372/1148 | Train loss: 0.1279 | Val loss: 0.2603\n",
      "=> Adjust learning rate to: 0.000875\n",
      "   % Time: 10585 | Iteration: 13100 | Batch:  472/1148 | Train loss: 0.1283 | Val loss: 0.2602\n",
      "   % Time: 10665 | Iteration: 13200 | Batch:  572/1148 | Train loss: 0.1246 | Val loss: 0.2597\n",
      "   % Time: 10744 | Iteration: 13300 | Batch:  672/1148 | Train loss: 0.1311 | Val loss: 0.2597\n",
      "   % Time: 10824 | Iteration: 13400 | Batch:  772/1148 | Train loss: 0.1269 | Val loss: 0.2599\n",
      "   % Time: 10903 | Iteration: 13500 | Batch:  872/1148 | Train loss: 0.1251 | Val loss: 0.2599\n",
      "   % Time: 10983 | Iteration: 13600 | Batch:  972/1148 | Train loss: 0.1278 | Val loss: 0.2598\n",
      "   % Time: 11062 | Iteration: 13700 | Batch: 1072/1148 | Train loss: 0.1255 | Val loss: 0.2598\n",
      "=> Adjust learning rate to: 0.0004375\n",
      "=> EPOCH 13\n",
      "   % Time: 11142 | Iteration: 13800 | Batch:   24/1148 | Train loss: 0.1274 | Val loss: 0.2597\n",
      "   % Time: 11221 | Iteration: 13900 | Batch:  124/1148 | Train loss: 0.1250 | Val loss: 0.2597\n",
      "   % Time: 11301 | Iteration: 14000 | Batch:  224/1148 | Train loss: 0.1233 | Val loss: 0.2597\n",
      "   % Time: 11380 | Iteration: 14100 | Batch:  324/1148 | Train loss: 0.1229 | Val loss: 0.2596\n",
      "   % Time: 11460 | Iteration: 14200 | Batch:  424/1148 | Train loss: 0.1203 | Val loss: 0.2596\n",
      "   % Time: 11539 | Iteration: 14300 | Batch:  524/1148 | Train loss: 0.1249 | Val loss: 0.2597\n",
      "   % Time: 11618 | Iteration: 14400 | Batch:  624/1148 | Train loss: 0.1244 | Val loss: 0.2597\n",
      "   % Time: 11700 | Iteration: 14500 | Batch:  724/1148 | Train loss: 0.1294 | Val loss: 0.2597\n",
      "   % Time: 11781 | Iteration: 14600 | Batch:  824/1148 | Train loss: 0.1220 | Val loss: 0.2597\n",
      "   % Time: 11861 | Iteration: 14700 | Batch:  924/1148 | Train loss: 0.1257 | Val loss: 0.2597\n",
      "=> Adjust learning rate to: 0.00021875\n",
      "   % Time: 11942 | Iteration: 14800 | Batch: 1024/1148 | Train loss: 0.1232 | Val loss: 0.2597\n",
      "   % Time: 12024 | Iteration: 14900 | Batch: 1124/1148 | Train loss: 0.1271 | Val loss: 0.2597\n",
      "=> EPOCH 14\n",
      "   % Time: 12105 | Iteration: 15000 | Batch:   76/1148 | Train loss: 0.1222 | Val loss: 0.2597\n",
      "   % Time: 12186 | Iteration: 15100 | Batch:  176/1148 | Train loss: 0.1243 | Val loss: 0.2598\n",
      "   % Time: 12268 | Iteration: 15200 | Batch:  276/1148 | Train loss: 0.1244 | Val loss: 0.2597\n",
      "=> Adjust learning rate to: 0.000109375\n",
      "   % Time: 12350 | Iteration: 15300 | Batch:  376/1148 | Train loss: 0.1234 | Val loss: 0.2597\n",
      "   % Time: 12432 | Iteration: 15400 | Batch:  476/1148 | Train loss: 0.1253 | Val loss: 0.2596\n",
      "   % Time: 12513 | Iteration: 15500 | Batch:  576/1148 | Train loss: 0.1232 | Val loss: 0.2596\n",
      "   % Time: 12595 | Iteration: 15600 | Batch:  676/1148 | Train loss: 0.1225 | Val loss: 0.2596\n",
      "   % Time: 12677 | Iteration: 15700 | Batch:  776/1148 | Train loss: 0.1234 | Val loss: 0.2596\n",
      "=> Adjust learning rate to: 5.46875e-05\n",
      "   % Time: 12758 | Iteration: 15800 | Batch:  876/1148 | Train loss: 0.1257 | Val loss: 0.2596\n",
      "   % Time: 12839 | Iteration: 15900 | Batch:  976/1148 | Train loss: 0.1249 | Val loss: 0.2596\n",
      "   % Time: 12920 | Iteration: 16000 | Batch: 1076/1148 | Train loss: 0.1247 | Val loss: 0.2596\n",
      "=> EPOCH 15\n",
      "   % Time: 13000 | Iteration: 16100 | Batch:   28/1148 | Train loss: 0.1235 | Val loss: 0.2596\n",
      "   % Time: 13079 | Iteration: 16200 | Batch:  128/1148 | Train loss: 0.1245 | Val loss: 0.2596\n",
      "=> Adjust learning rate to: 2.734375e-05\n",
      "   % Time: 13159 | Iteration: 16300 | Batch:  228/1148 | Train loss: 0.1233 | Val loss: 0.2596\n",
      "   % Time: 13238 | Iteration: 16400 | Batch:  328/1148 | Train loss: 0.1262 | Val loss: 0.2596\n",
      "   % Time: 13318 | Iteration: 16500 | Batch:  428/1148 | Train loss: 0.1215 | Val loss: 0.2596\n",
      "   % Time: 13400 | Iteration: 16600 | Batch:  528/1148 | Train loss: 0.1241 | Val loss: 0.2596\n",
      "   % Time: 13481 | Iteration: 16700 | Batch:  628/1148 | Train loss: 0.1242 | Val loss: 0.2596\n",
      "=> Adjust learning rate to: 1.3671875e-05\n",
      "   % Time: 13563 | Iteration: 16800 | Batch:  728/1148 | Train loss: 0.1229 | Val loss: 0.2596\n",
      "   % Time: 13645 | Iteration: 16900 | Batch:  828/1148 | Train loss: 0.1212 | Val loss: 0.2596\n",
      "   % Time: 13726 | Iteration: 17000 | Batch:  928/1148 | Train loss: 0.1243 | Val loss: 0.2596\n",
      "   % Time: 13807 | Iteration: 17100 | Batch: 1028/1148 | Train loss: 0.1234 | Val loss: 0.2596\n",
      "   % Time: 13889 | Iteration: 17200 | Batch: 1128/1148 | Train loss: 0.1232 | Val loss: 0.2596\n",
      "=> Adjust learning rate to: 6.8359375e-06\n",
      "=> EPOCH 16\n",
      "   % Time: 13969 | Iteration: 17300 | Batch:   80/1148 | Train loss: 0.1231 | Val loss: 0.2596\n",
      "   % Time: 14048 | Iteration: 17400 | Batch:  180/1148 | Train loss: 0.1221 | Val loss: 0.2596\n",
      "   % Time: 14128 | Iteration: 17500 | Batch:  280/1148 | Train loss: 0.1244 | Val loss: 0.2596\n",
      "   % Time: 14208 | Iteration: 17600 | Batch:  380/1148 | Train loss: 0.1254 | Val loss: 0.2596\n",
      "   % Time: 14287 | Iteration: 17700 | Batch:  480/1148 | Train loss: 0.1222 | Val loss: 0.2596\n",
      "=> Adjust learning rate to: 3.41796875e-06\n",
      "   % Time: 14369 | Iteration: 17800 | Batch:  580/1148 | Train loss: 0.1226 | Val loss: 0.2596\n",
      "   % Time: 14450 | Iteration: 17900 | Batch:  680/1148 | Train loss: 0.1258 | Val loss: 0.2596\n",
      "   % Time: 14531 | Iteration: 18000 | Batch:  780/1148 | Train loss: 0.1231 | Val loss: 0.2596\n",
      "   % Time: 14612 | Iteration: 18100 | Batch:  880/1148 | Train loss: 0.1231 | Val loss: 0.2596\n",
      "   % Time: 14694 | Iteration: 18200 | Batch:  980/1148 | Train loss: 0.1214 | Val loss: 0.2596\n",
      "=> Adjust learning rate to: 1.708984375e-06\n",
      "   % Time: 14775 | Iteration: 18300 | Batch: 1080/1148 | Train loss: 0.1237 | Val loss: 0.2596\n",
      "=> EPOCH 17\n",
      "   % Time: 14857 | Iteration: 18400 | Batch:   32/1148 | Train loss: 0.1203 | Val loss: 0.2596\n",
      "   % Time: 14938 | Iteration: 18500 | Batch:  132/1148 | Train loss: 0.1260 | Val loss: 0.2596\n",
      "   % Time: 15019 | Iteration: 18600 | Batch:  232/1148 | Train loss: 0.1225 | Val loss: 0.2596\n",
      "   % Time: 15100 | Iteration: 18700 | Batch:  332/1148 | Train loss: 0.1242 | Val loss: 0.2596\n",
      "=> Adjust learning rate to: 8.544921875e-07\n"
     ]
    }
   ],
   "source": [
    "if 1 == 1:  # change to True to train\n",
    "    iteration = n_total = train_loss = n_bad_loss = 0\n",
    "    stop = False\n",
    "    best_val_loss = 10\n",
    "    init = time.time()\n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        train(config, train_iter, model, criterion, optimizer, epoch)\n",
    "        if stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T00:39:51.870190Z",
     "start_time": "2017-04-24T09:36:16.987388+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme error rate (PER): 10.01\n",
      "Word error rate (WER): 42.04\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(config.best_model))\n",
    "test(test_iter, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T20:15:50.247874Z",
     "start_time": "2017-04-24T05:15:50.026042+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> joystick\n",
      "= JH OY1 S T IH2 K\n",
      "< JH OY1 S T IH0 K\n",
      "\n",
      "> asmus\n",
      "= AH0 Z M UW1 S\n",
      "< AE1 Z M AH0 S\n",
      "\n",
      "> doxology\n",
      "= D AA0 K S AA1 L AH0 JH IY0\n",
      "< D AA1 K S AH0 L AH0 JH IY0\n",
      "\n",
      "> pang\n",
      "= P AE1 NG\n",
      "< P AE1 NG\n",
      "\n",
      "> moad\n",
      "= M OW1 D\n",
      "< M OW1 D\n",
      "\n",
      "> strangely\n",
      "= S T R EY1 N JH L IY0\n",
      "< S T R EY1 N JH L IY0\n",
      "\n",
      "> buckley\n",
      "= B AH1 K L IY0\n",
      "< B AH1 K L IY0\n",
      "\n",
      "> borja\n",
      "= B AO1 R Y AH0\n",
      "< B AO1 R Y AH0\n",
      "\n",
      "> innovates\n",
      "= IH1 N AH0 V EY2 T S\n",
      "< IH1 N AH0 V EY2 T S\n",
      "\n",
      "> lear\n",
      "= L IH1 R\n",
      "< L IH1 R\n",
      "\n",
      "> hardinger\n",
      "= HH AA1 R D IH0 NG ER0\n",
      "< HH AA1 R D IH0 NG ER0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_iter.init_epoch()\n",
    "for i, batch in enumerate(test_iter):\n",
    "    show(batch, model)\n",
    "    if i == 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
