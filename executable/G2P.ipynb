{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn how to do Grapheme-to-Phoneme using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Grapheme-to-Phoneme (G2P) model is one of the core components of a typical Text-to-Speech (TTS) system, e.g. [WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) and [Deep Voice](http://research.baidu.com/deep-voice-production-quality-text-speech-system-constructed-entirely-deep-neural-networks/). In this notebook, we will try to replicate the Encoder-decoder LSTM model from the paper https://arxiv.org/abs/1506.00196."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this tutorial, we will learn how to implement:\n",
    "+ Sequence-to-Sequence model\n",
    "+ Global attention\n",
    "+ Beam-search decoder\n",
    "+ Phoneme-error-rate (PER) metric\n",
    "+ Torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import necessary modules. You can install PyTorch as suggested in its [main page](http://pytorch.org/). To install [torchtext](https://github.com/pytorch/text), simply do `git install git+https://github.com/pytorch/text.git`. Due to this [bug](https://github.com/pytorch/text/pull/28), it is important to update your `torchtext` to the lastest version (using the above installing method is enough). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:21.713940Z",
     "start_time": "2017-04-25T10:55:21.710637+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import torchtext.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[argparse](https://docs.python.org/3/library/argparse.html) is a default python module which is used for command-line script parsing. To run this notebook as a python script, simply comment out all the markdown cell and change the following code cell to the real `argparse` [code](https://docs.python.org/3/howto/argparse.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:23.583132Z",
     "start_time": "2017-04-25T10:55:23.571525+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/cmudict/',\n",
    "    'epochs': 50,\n",
    "    'batch_size': 100,\n",
    "    'max_len': 20,  # max length of grapheme/phoneme sequences\n",
    "    'beam_size': 3,  # size of beam for beam-search\n",
    "    'd_embed': 500,  # embedding dimension\n",
    "    'd_hidden': 500,  # hidden dimension\n",
    "    'attention': True,  # use attention or not\n",
    "    'log_every': 100,  # number of iterations to log and validate training\n",
    "    'lr': 0.007,  # initial learning rate\n",
    "    'lr_decay': 0.5,  # decay lr when not observing improvement in val_loss\n",
    "    'lr_min': 1e-5,  # stop when lr is too low\n",
    "    'n_bad_loss': 5,  # number of bad val_loss before decaying\n",
    "    'clip': 2.3,  # clip gradient, to avoid exploding gradient\n",
    "    'cuda': True,  # using gpu or not\n",
    "    'seed': 5,  # initial seed\n",
    "    'intermediate_path': '../intermediate/g2p/',  # path to save models\n",
    "}\n",
    "args = argparse.Namespace(**parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to download the data. We will use the free [CMUdict](https://github.com/cmusphinx/cmudict) dataset. The seed `5` is used to generate random numbers for the purpose of replicating the result. However, we still observe distinct scores for different runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:23.680809Z",
     "start_time": "2017-04-25T10:55:23.585697+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "if not os.path.isdir(args.intermediate_path):\n",
    "    os.makedirs(args.intermediate_path)\n",
    "if not os.path.isdir(args.data_path):\n",
    "    URL = \"https://github.com/cmusphinx/cmudict/archive/master.zip\"\n",
    "    !wget $URL -O ../data/cmudict.zip\n",
    "    !unzip ../data/cmudict.zip -d ../data/\n",
    "    !mv ../data/cmudict-master $args.data_path\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to define our model. The following figure (taken from the paper) is a two layer Encoder-Decoder LSTM model (which is a variant of Sequence-to-Sequence model). To understand how LSTM works, we can look at the excellent blog post http://colah.github.io/posts/2015-08-Understanding-LSTMs/. Each rectangle in the figure will be an `LSTMCell` in our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T10:27:01.071937Z",
     "start_time": "2017-04-25T19:27:00.921492+09:00"
    }
   },
   "source": [
    "![](https://www.researchgate.net/profile/Kaisheng_Yao/publication/277603654/figure/fig1/AS:294343429640195@1447188350615/Figure-1-An-encoder-decoder-LSTM-with-two-layers-The-encoder-LSTM-to-the-left-of-the.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking into the code, we need to review some PyTorch modules and functions:\n",
    "+ [nn.Embedding](http://pytorch.org/docs/nn.html#embedding): a lookup table to convert a tensor of indices to a tensor of vectors. Theoretically, it does one-hot encoding followed by a fully connected layer (with no bias).\n",
    "+ [nn.Linear](http://pytorch.org/docs/nn.html#linear): nothing but a fully connected (neural network) layer.\n",
    "+ [nn.LSTMCell](...): a recurrent neural network cell\n",
    "+ [size](): get the size of tensor.\n",
    "+ [unsqueeze](http://pytorch.org/docs/torch.html#torch.unsqueeze): create a new dimension (size 1).\n",
    "+ [squeeze](http://pytorch.org/docs/torch.html#torch.squeeze): drop a (size 1) dimension.\n",
    "+ [stack](): \n",
    "+ [bmm]():\n",
    "+ [index_select]():\n",
    "\n",
    "+ [F.softmax], [F.tanh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:23.719827Z",
     "start_time": "2017-04-25T10:55:23.683371+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_embed, d_hidden):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
    "        self.lstm = nn.LSTMCell(d_embed, d_hidden)\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "    def forward(self, x_seq, cuda=False):\n",
    "        o = []\n",
    "        e_seq = self.embedding(x_seq)  # seq x batch x dim\n",
    "        tt = torch.cuda if cuda else torch  # use cuda tensor or not\n",
    "        # create initial hidden state and initial cell state\n",
    "        h = Variable(tt.FloatTensor(e_seq.size(1), self.d_hidden).zero_())\n",
    "        c = Variable(tt.FloatTensor(e_seq.size(1), self.d_hidden).zero_())\n",
    "        \n",
    "        for e in e_seq.chunk(e_seq.size(0), 0):\n",
    "            e = e.squeeze(0)\n",
    "            h, c = self.lstm(e, (h, c))\n",
    "            o.append(h)\n",
    "        return torch.stack(o, 0), h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T10:28:15.020816Z",
     "start_time": "2017-04-25T19:28:15.006678+09:00"
    },
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Based on https://github.com/OpenNMT/OpenNMT-py\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Dot global attention from https://arxiv.org/abs/1508.04025\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear = nn.Linear(dim*2, dim, bias=False)\n",
    "        \n",
    "    def forward(self, x, context=None):\n",
    "        if context is None:\n",
    "            return x\n",
    "        assert x.size(0) == context.size(0)  # x: batch x dim\n",
    "        assert x.size(1) == context.size(2)  # context: batch x seq x dim\n",
    "        attn = F.softmax(context.bmm(x.unsqueeze(2)).squeeze(2))\n",
    "        weighted_context = attn.unsqueeze(1).bmm(context).squeeze(1)\n",
    "        o = self.linear(torch.cat((x, weighted_context), 1))\n",
    "        return F.tanh(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:23.808780Z",
     "start_time": "2017-04-25T10:55:23.758361+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_embed, d_hidden):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
    "        self.lstm = nn.LSTMCell(d_embed, d_hidden)\n",
    "        self.attn = Attention(d_hidden)\n",
    "        self.linear = nn.Linear(d_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x_seq, h, c, context=None):\n",
    "        o = []\n",
    "        e_seq = self.embedding(x_seq)\n",
    "        for e in e_seq.chunk(e_seq.size(0), 0):\n",
    "            e = e.squeeze(0)\n",
    "            h, c = self.lstm(e, (h, c))\n",
    "            o.append(self.attn(h, context))\n",
    "        o = torch.stack(o, 0)\n",
    "        o = self.linear(o.view(-1, h.size(1)))\n",
    "        return F.log_softmax(o).view(x_seq.size(0), -1, o.size(1)), h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:23.846303Z",
     "start_time": "2017-04-25T10:55:23.811053+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class G2P(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(G2P, self).__init__()\n",
    "        self.encoder = Encoder(config.g_size, config.d_embed,\n",
    "                               config.d_hidden)\n",
    "        self.decoder = Decoder(config.p_size, config.d_embed,\n",
    "                               config.d_hidden)\n",
    "        self.config = config\n",
    "        \n",
    "    def forward(self, g_seq, p_seq=None):\n",
    "        o, h, c = self.encoder(g_seq, self.config.cuda)\n",
    "        context = o.t() if self.config.attention else None\n",
    "        if p_seq is not None:  # not generate\n",
    "            return self.decoder(p_seq, h, c, context)\n",
    "        else:\n",
    "            assert g_seq.size(1) == 1  # make sure batch_size = 1\n",
    "            return self._generate(h, c, context)\n",
    "        \n",
    "    def _generate(self, h, c, context):\n",
    "        beam = Beam(self.config.beam_size, cuda=self.config.cuda)\n",
    "        # Make a beam_size batch.\n",
    "        h = h.expand(beam.size, h.size(1))  \n",
    "        c = c.expand(beam.size, c.size(1))\n",
    "        context = context.expand(beam.size, context.size(1), context.size(2))\n",
    "        \n",
    "        for i in range(self.config.max_len):  # max_len = 20\n",
    "            x = beam.get_current_state()\n",
    "            o, h, c = self.decoder(Variable(x.unsqueeze(0)), h, c, context)\n",
    "            if beam.advance(o.data.squeeze(0)):\n",
    "                break\n",
    "            h.data.copy_(h.data.index_select(0, beam.get_current_origin()))\n",
    "            c.data.copy_(c.data.index_select(0, beam.get_current_origin()))\n",
    "        tt = torch.cuda if self.config.cuda else torch\n",
    "        return Variable(tt.LongTensor(beam.get_hyp(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:23.919202Z",
     "start_time": "2017-04-25T10:55:23.848795+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Based on https://github.com/MaximumEntropy/Seq2Seq-PyTorch/.\n",
    "class Beam(object):\n",
    "    \"\"\"Ordered beam of candidate outputs.\"\"\"\n",
    "\n",
    "    def __init__(self, size, pad=1, bos=2, eos=3, cuda=False):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        self.size = size\n",
    "        self.done = False\n",
    "        self.pad = pad\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "        self.tt = torch.cuda if cuda else torch\n",
    "\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = self.tt.FloatTensor(size).zero_()\n",
    "\n",
    "        # The backpointers at each time-step.\n",
    "        self.prevKs = []\n",
    "\n",
    "        # The outputs at each time-step.\n",
    "        self.nextYs = [self.tt.LongTensor(size).fill_(self.pad)]\n",
    "        self.nextYs[0][0] = self.bos\n",
    "\n",
    "    # Get the outputs for the current timestep.\n",
    "    def get_current_state(self):\n",
    "        \"\"\"Get state of beam.\"\"\"\n",
    "        return self.nextYs[-1]\n",
    "\n",
    "    # Get the backpointers for the current timestep.\n",
    "    def get_current_origin(self):\n",
    "        \"\"\"Get the backpointer to the beam at this step.\"\"\"\n",
    "        return self.prevKs[-1]\n",
    "\n",
    "    def advance(self, workd_lk):\n",
    "        \"\"\"Advance the beam.\"\"\"\n",
    "        num_words = workd_lk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prevKs) > 0:\n",
    "            beam_lk = workd_lk + self.scores.unsqueeze(1).expand_as(workd_lk)\n",
    "        else:\n",
    "            beam_lk = workd_lk[0]\n",
    "\n",
    "        flat_beam_lk = beam_lk.view(-1)\n",
    "\n",
    "        bestScores, bestScoresId = flat_beam_lk.topk(self.size, 0,\n",
    "                                                     True, True)\n",
    "        self.scores = bestScores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prev_k = bestScoresId / num_words\n",
    "        self.prevKs.append(prev_k)\n",
    "        self.nextYs.append(bestScoresId - prev_k * num_words)\n",
    "        # End condition is when top-of-beam is EOS.\n",
    "        if self.nextYs[-1][0] == self.eos:\n",
    "            self.done = True\n",
    "        return self.done\n",
    "\n",
    "    def get_hyp(self, k):\n",
    "        \"\"\"Get hypotheses.\"\"\"\n",
    "        hyp = []\n",
    "        # print(len(self.prevKs), len(self.nextYs), len(self.attn))\n",
    "        for j in range(len(self.prevKs) - 1, -1, -1):\n",
    "            hyp.append(self.nextYs[j + 1][k])\n",
    "            k = self.prevKs[j][k]\n",
    "        return hyp[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) is used to compute `phoneme-error-rate` (PER) for phoneme sequences (similar to [word-error-rate](https://en.wikipedia.org/wiki/Word_error_rate) for word sequences). In the paper, there is another metric named `word-error-rate`, which is obtained by calculating the number of wrong predictions. For example, the phoneme sequence \"S W AY1 G ER0 D\" is a wrong prediction for the word \"sweigard\" (real phoneme sequence is \"S W EY1 G ER0 D\"). Please not to be confused between these two metrics which have the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:24.014215Z",
     "start_time": "2017-04-25T10:55:23.968091+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Based on https://github.com/SeanNaren/deepspeech.pytorch/blob/master/decoder.py.\n",
    "import Levenshtein  # https://github.com/ztane/python-Levenshtein/\n",
    "\n",
    "def phoneme_error_rate(p_seq1, p_seq2):\n",
    "    p_vocab = set(p_seq1 + p_seq2)\n",
    "    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n",
    "    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n",
    "    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n",
    "    return Levenshtein.distance(''.join(c_seq1),\n",
    "                                ''.join(c_seq2)) / len(c_seq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:24.049372Z",
     "start_time": "2017-04-25T10:55:24.016873+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr_decay):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:24.100314Z",
     "start_time": "2017-04-25T10:55:24.051952+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(config, train_iter, model, criterion, optimizer, epoch):\n",
    "    global iteration, n_total, train_loss, n_bad_loss\n",
    "    global init, best_val_loss, stop\n",
    "    \n",
    "    print(\"=> EPOCH {}\".format(epoch))\n",
    "    train_iter.init_epoch()\n",
    "    for batch in train_iter:\n",
    "        iteration += 1\n",
    "        model.train()\n",
    "        \n",
    "        output, _, __ = model(batch.grapheme, batch.phoneme[:-1].detach())\n",
    "        target = batch.phoneme[1:]\n",
    "        loss = criterion(output.view(output.size(0) * output.size(1), -1),\n",
    "                         target.view(target.size(0) * target.size(1)))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), config.clip, 'inf')\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_total += batch.batch_size\n",
    "        train_loss += loss.data[0] * batch.batch_size\n",
    "        \n",
    "        if iteration % config.log_every == 0:\n",
    "            train_loss /= n_total\n",
    "            val_loss = validate(val_iter, model, criterion)\n",
    "            print(\"   % Time: {:5.0f} | Iteration: {:5} | Batch: {:4}/{}\"\n",
    "                  \" | Train loss: {:.4f} | Val loss: {:.4f}\"\n",
    "                  .format(time.time()-init, iteration, train_iter.iterations,\n",
    "                          len(train_iter), train_loss, val_loss))\n",
    "            \n",
    "            n_total = train_loss = 0\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                n_bad_loss = 0\n",
    "                torch.save(model.state_dict(), config.best_model)\n",
    "            else:\n",
    "                n_bad_loss += 1\n",
    "            if n_bad_loss == config.n_bad_loss:\n",
    "                best_val_loss = val_loss\n",
    "                n_bad_loss = 0\n",
    "                adjust_learning_rate(optimizer, config.lr_decay)\n",
    "                new_lr = optimizer.param_groups[0]['lr']\n",
    "                print(\"=> Adjust learning rate to: {}\".format(new_lr))\n",
    "                if new_lr < config.lr_min:\n",
    "                    stop = True\n",
    "                    break                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:24.153595Z",
     "start_time": "2017-04-25T10:55:24.102484+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def validate(val_iter, model, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_iter.init_epoch()\n",
    "    for batch in val_iter:\n",
    "        output, _, __ = model(batch.grapheme, batch.phoneme[:-1])\n",
    "        target = batch.phoneme[1:]\n",
    "        loss = criterion(output.squeeze(1), target.squeeze(1))\n",
    "        val_loss += loss.data[0] * batch.batch_size\n",
    "    return val_loss / len(val_iter.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:24.201719Z",
     "start_time": "2017-04-25T10:55:24.156027+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(test_iter, model, criterion):\n",
    "    model.eval()\n",
    "    test_iter.init_epoch()\n",
    "    test_per = test_wer = 0\n",
    "    for batch in test_iter:\n",
    "        output = model(batch.grapheme).data.tolist()\n",
    "        target = batch.phoneme[1:].squeeze(1).data.tolist()\n",
    "        # calculate per, wer here\n",
    "        per = phoneme_error_rate(output, target) \n",
    "        wer = int(output != target)\n",
    "        test_per += per  # batch_size = 1\n",
    "        test_wer += wer\n",
    "        \n",
    "    test_per = test_per / len(test_iter.dataset) * 100\n",
    "    test_wer = test_wer / len(test_iter.dataset) * 100\n",
    "    print(\"Phoneme error rate (PER): {:.2f}\\nWord error rate (WER): {:.2f}\"\n",
    "          .format(test_per, test_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:24.229556Z",
     "start_time": "2017-04-25T10:55:24.204048+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def show(batch, model):\n",
    "    assert batch.batch_size == 1\n",
    "    g_field = batch.dataset.fields['grapheme']\n",
    "    p_field = batch.dataset.fields['phoneme']\n",
    "    prediction = model(batch.grapheme).data.tolist()[:-1]\n",
    "    grapheme = batch.grapheme.squeeze(1).data.tolist()[1:][::-1]\n",
    "    phoneme = batch.phoneme.squeeze(1).data.tolist()[1:-1]\n",
    "    print(\"> {}\\n= {}\\n< {}\\n\".format(\n",
    "        ''.join([g_field.vocab.itos[g] for g in grapheme]),\n",
    "        ' '.join([p_field.vocab.itos[p] for p in phoneme]),\n",
    "        ' '.join([p_field.vocab.itos[p] for p in prediction])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:23.965928Z",
     "start_time": "2017-04-25T10:55:23.921740+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CMUDict(data.Dataset):\n",
    "\n",
    "    def __init__(self, data_lines, g_field, p_field):\n",
    "        fields = [('grapheme', g_field), ('phoneme', p_field)]\n",
    "        examples = []  # maybe ignore '...-1' grapheme\n",
    "        for line in data_lines:\n",
    "            grapheme, phoneme = line.split(maxsplit=1)\n",
    "            examples.append(data.Example.fromlist([grapheme, phoneme],\n",
    "                                                  fields))\n",
    "        self.sort_key = lambda x: len(x.grapheme)\n",
    "        super(CMUDict, self).__init__(examples, fields)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, path, g_field, p_field, seed=None):\n",
    "        import random\n",
    "        \n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        with open(path) as f:\n",
    "            lines = f.readlines()\n",
    "        random.shuffle(lines)\n",
    "        train_lines, val_lines, test_lines = [], [], []\n",
    "        for i, line in enumerate(lines):\n",
    "            if i % 20 == 0:\n",
    "                val_lines.append(line)\n",
    "            elif i % 20 < 3:\n",
    "                test_lines.append(line)\n",
    "            else:\n",
    "                train_lines.append(line)\n",
    "        train_data = cls(train_lines, g_field, p_field)\n",
    "        val_data = cls(val_lines, g_field, p_field)\n",
    "        test_data = cls(test_lines, g_field, p_field)\n",
    "        return (train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:24.261265Z",
     "start_time": "2017-04-25T10:55:24.232147+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g_field = data.Field(init_token='<s>',\n",
    "                     tokenize=(lambda x: list(x.split('(')[0])[::-1]))\n",
    "p_field = data.Field(init_token='<os>', eos_token='</os>',\n",
    "                     tokenize=(lambda x: x.split('#')[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:30.466394Z",
     "start_time": "2017-04-25T10:55:24.263752+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(args.data_path, 'cmudict.dict')\n",
    "train_data, val_data, test_data = CMUDict.splits(filepath, g_field, p_field,\n",
    "                                                 args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:32.369090Z",
     "start_time": "2017-04-25T10:55:30.469072+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g_field.build_vocab(train_data, val_data, test_data)\n",
    "p_field.build_vocab(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T01:55:32.377595Z",
     "start_time": "2017-04-25T10:55:32.371707+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "device = None if args.cuda else -1  # None is current gpu\n",
    "train_iter = data.BucketIterator(train_data, batch_size=args.batch_size,\n",
    "                                 repeat=False, device=device)\n",
    "val_iter = data.Iterator(val_data, batch_size=1,\n",
    "                         train=False, sort=False, device=device)\n",
    "test_iter = data.Iterator(test_data, batch_size=1,\n",
    "                          train=False, shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T10:20:25.868036Z",
     "start_time": "2017-04-25T19:20:25.789991+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = args\n",
    "config.g_size = len(g_field.vocab)\n",
    "config.p_size = len(p_field.vocab)\n",
    "config.best_model = os.path.join(config.intermediate_path,\n",
    "                                 \"best_model_adagrad_attn.pth\")\n",
    "\n",
    "model = G2P(config)\n",
    "criterion = nn.NLLLoss()\n",
    "if config.cuda:\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=config.lr)  # use Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T08:03:37.790634Z",
     "start_time": "2017-04-25T10:55:34.781378+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1\n",
      "   % Time:   111 | Iteration:   100 | Batch:  100/1148 | Train loss: 1.3790 | Val loss: 0.9130\n",
      "   % Time:   220 | Iteration:   200 | Batch:  200/1148 | Train loss: 0.5389 | Val loss: 0.6045\n",
      "   % Time:   329 | Iteration:   300 | Batch:  300/1148 | Train loss: 0.4361 | Val loss: 0.5267\n",
      "   % Time:   438 | Iteration:   400 | Batch:  400/1148 | Train loss: 0.3824 | Val loss: 0.4721\n",
      "   % Time:   548 | Iteration:   500 | Batch:  500/1148 | Train loss: 0.3564 | Val loss: 0.4457\n",
      "   % Time:   658 | Iteration:   600 | Batch:  600/1148 | Train loss: 0.3337 | Val loss: 0.4177\n",
      "   % Time:   768 | Iteration:   700 | Batch:  700/1148 | Train loss: 0.3253 | Val loss: 0.4045\n",
      "   % Time:   878 | Iteration:   800 | Batch:  800/1148 | Train loss: 0.3199 | Val loss: 0.4316\n",
      "   % Time:   988 | Iteration:   900 | Batch:  900/1148 | Train loss: 0.2961 | Val loss: 0.3766\n",
      "   % Time:  1095 | Iteration:  1000 | Batch: 1000/1148 | Train loss: 0.2952 | Val loss: 0.3648\n",
      "   % Time:  1201 | Iteration:  1100 | Batch: 1100/1148 | Train loss: 0.2807 | Val loss: 0.3615\n",
      "=> EPOCH 2\n",
      "   % Time:  1312 | Iteration:  1200 | Batch:   52/1148 | Train loss: 0.2808 | Val loss: 0.3508\n",
      "   % Time:  1421 | Iteration:  1300 | Batch:  152/1148 | Train loss: 0.2528 | Val loss: 0.3471\n",
      "   % Time:  1531 | Iteration:  1400 | Batch:  252/1148 | Train loss: 0.2536 | Val loss: 0.3375\n",
      "   % Time:  1640 | Iteration:  1500 | Batch:  352/1148 | Train loss: 0.2555 | Val loss: 0.3299\n",
      "   % Time:  1750 | Iteration:  1600 | Batch:  452/1148 | Train loss: 0.2530 | Val loss: 0.3297\n",
      "   % Time:  1860 | Iteration:  1700 | Batch:  552/1148 | Train loss: 0.2457 | Val loss: 0.3291\n",
      "   % Time:  1970 | Iteration:  1800 | Batch:  652/1148 | Train loss: 0.2451 | Val loss: 0.3223\n",
      "   % Time:  2080 | Iteration:  1900 | Batch:  752/1148 | Train loss: 0.2464 | Val loss: 0.3180\n",
      "   % Time:  2189 | Iteration:  2000 | Batch:  852/1148 | Train loss: 0.2322 | Val loss: 0.3182\n",
      "   % Time:  2299 | Iteration:  2100 | Batch:  952/1148 | Train loss: 0.2470 | Val loss: 0.3139\n",
      "   % Time:  2409 | Iteration:  2200 | Batch: 1052/1148 | Train loss: 0.2308 | Val loss: 0.3084\n",
      "=> EPOCH 3\n",
      "   % Time:  2516 | Iteration:  2300 | Batch:    4/1148 | Train loss: 0.2320 | Val loss: 0.3070\n",
      "   % Time:  2623 | Iteration:  2400 | Batch:  104/1148 | Train loss: 0.2233 | Val loss: 0.3060\n",
      "   % Time:  2734 | Iteration:  2500 | Batch:  204/1148 | Train loss: 0.2132 | Val loss: 0.3029\n",
      "   % Time:  2840 | Iteration:  2600 | Batch:  304/1148 | Train loss: 0.2182 | Val loss: 0.3017\n",
      "   % Time:  2947 | Iteration:  2700 | Batch:  404/1148 | Train loss: 0.2229 | Val loss: 0.2998\n",
      "   % Time:  3055 | Iteration:  2800 | Batch:  504/1148 | Train loss: 0.2146 | Val loss: 0.2962\n",
      "   % Time:  3162 | Iteration:  2900 | Batch:  604/1148 | Train loss: 0.2185 | Val loss: 0.2975\n",
      "   % Time:  3270 | Iteration:  3000 | Batch:  704/1148 | Train loss: 0.2165 | Val loss: 0.2963\n",
      "   % Time:  3380 | Iteration:  3100 | Batch:  804/1148 | Train loss: 0.2173 | Val loss: 0.2933\n",
      "   % Time:  3489 | Iteration:  3200 | Batch:  904/1148 | Train loss: 0.2155 | Val loss: 0.2909\n",
      "   % Time:  3599 | Iteration:  3300 | Batch: 1004/1148 | Train loss: 0.2166 | Val loss: 0.2912\n",
      "   % Time:  3710 | Iteration:  3400 | Batch: 1104/1148 | Train loss: 0.2112 | Val loss: 0.2867\n",
      "=> EPOCH 4\n",
      "   % Time:  3820 | Iteration:  3500 | Batch:   56/1148 | Train loss: 0.2027 | Val loss: 0.2866\n",
      "   % Time:  3932 | Iteration:  3600 | Batch:  156/1148 | Train loss: 0.2008 | Val loss: 0.2845\n",
      "   % Time:  4042 | Iteration:  3700 | Batch:  256/1148 | Train loss: 0.1962 | Val loss: 0.2853\n",
      "   % Time:  4152 | Iteration:  3800 | Batch:  356/1148 | Train loss: 0.2031 | Val loss: 0.2847\n",
      "   % Time:  4262 | Iteration:  3900 | Batch:  456/1148 | Train loss: 0.1974 | Val loss: 0.2813\n",
      "   % Time:  4372 | Iteration:  4000 | Batch:  556/1148 | Train loss: 0.1973 | Val loss: 0.2811\n",
      "   % Time:  4482 | Iteration:  4100 | Batch:  656/1148 | Train loss: 0.2006 | Val loss: 0.2808\n",
      "   % Time:  4593 | Iteration:  4200 | Batch:  756/1148 | Train loss: 0.2021 | Val loss: 0.2807\n",
      "   % Time:  4703 | Iteration:  4300 | Batch:  856/1148 | Train loss: 0.1931 | Val loss: 0.2798\n",
      "   % Time:  4813 | Iteration:  4400 | Batch:  956/1148 | Train loss: 0.1998 | Val loss: 0.2765\n",
      "   % Time:  4924 | Iteration:  4500 | Batch: 1056/1148 | Train loss: 0.1974 | Val loss: 0.2773\n",
      "=> EPOCH 5\n",
      "   % Time:  5035 | Iteration:  4600 | Batch:    8/1148 | Train loss: 0.1986 | Val loss: 0.2756\n",
      "   % Time:  5145 | Iteration:  4700 | Batch:  108/1148 | Train loss: 0.1843 | Val loss: 0.2729\n",
      "   % Time:  5255 | Iteration:  4800 | Batch:  208/1148 | Train loss: 0.1881 | Val loss: 0.2738\n",
      "   % Time:  5366 | Iteration:  4900 | Batch:  308/1148 | Train loss: 0.1851 | Val loss: 0.2719\n",
      "   % Time:  5478 | Iteration:  5000 | Batch:  408/1148 | Train loss: 0.1830 | Val loss: 0.2719\n",
      "   % Time:  5589 | Iteration:  5100 | Batch:  508/1148 | Train loss: 0.1901 | Val loss: 0.2736\n",
      "   % Time:  5698 | Iteration:  5200 | Batch:  608/1148 | Train loss: 0.1869 | Val loss: 0.2704\n",
      "   % Time:  5809 | Iteration:  5300 | Batch:  708/1148 | Train loss: 0.1878 | Val loss: 0.2693\n",
      "   % Time:  5920 | Iteration:  5400 | Batch:  808/1148 | Train loss: 0.1879 | Val loss: 0.2700\n",
      "   % Time:  6030 | Iteration:  5500 | Batch:  908/1148 | Train loss: 0.1859 | Val loss: 0.2684\n",
      "   % Time:  6139 | Iteration:  5600 | Batch: 1008/1148 | Train loss: 0.1835 | Val loss: 0.2681\n",
      "   % Time:  6247 | Iteration:  5700 | Batch: 1108/1148 | Train loss: 0.1873 | Val loss: 0.2681\n",
      "=> EPOCH 6\n",
      "   % Time:  6358 | Iteration:  5800 | Batch:   60/1148 | Train loss: 0.1870 | Val loss: 0.2660\n",
      "   % Time:  6468 | Iteration:  5900 | Batch:  160/1148 | Train loss: 0.1756 | Val loss: 0.2673\n",
      "   % Time:  6578 | Iteration:  6000 | Batch:  260/1148 | Train loss: 0.1759 | Val loss: 0.2651\n",
      "   % Time:  6688 | Iteration:  6100 | Batch:  360/1148 | Train loss: 0.1726 | Val loss: 0.2662\n",
      "   % Time:  6799 | Iteration:  6200 | Batch:  460/1148 | Train loss: 0.1795 | Val loss: 0.2655\n",
      "   % Time:  6908 | Iteration:  6300 | Batch:  560/1148 | Train loss: 0.1754 | Val loss: 0.2650\n",
      "   % Time:  7018 | Iteration:  6400 | Batch:  660/1148 | Train loss: 0.1738 | Val loss: 0.2651\n",
      "   % Time:  7128 | Iteration:  6500 | Batch:  760/1148 | Train loss: 0.1741 | Val loss: 0.2627\n",
      "   % Time:  7238 | Iteration:  6600 | Batch:  860/1148 | Train loss: 0.1759 | Val loss: 0.2625\n",
      "   % Time:  7348 | Iteration:  6700 | Batch:  960/1148 | Train loss: 0.1763 | Val loss: 0.2615\n",
      "   % Time:  7458 | Iteration:  6800 | Batch: 1060/1148 | Train loss: 0.1728 | Val loss: 0.2610\n",
      "=> EPOCH 7\n",
      "   % Time:  7566 | Iteration:  6900 | Batch:   12/1148 | Train loss: 0.1769 | Val loss: 0.2614\n",
      "   % Time:  7673 | Iteration:  7000 | Batch:  112/1148 | Train loss: 0.1626 | Val loss: 0.2596\n",
      "   % Time:  7784 | Iteration:  7100 | Batch:  212/1148 | Train loss: 0.1666 | Val loss: 0.2619\n",
      "   % Time:  7894 | Iteration:  7200 | Batch:  312/1148 | Train loss: 0.1682 | Val loss: 0.2598\n",
      "   % Time:  8004 | Iteration:  7300 | Batch:  412/1148 | Train loss: 0.1698 | Val loss: 0.2599\n",
      "   % Time:  8113 | Iteration:  7400 | Batch:  512/1148 | Train loss: 0.1689 | Val loss: 0.2584\n",
      "   % Time:  8223 | Iteration:  7500 | Batch:  612/1148 | Train loss: 0.1686 | Val loss: 0.2584\n",
      "   % Time:  8333 | Iteration:  7600 | Batch:  712/1148 | Train loss: 0.1683 | Val loss: 0.2589\n",
      "   % Time:  8443 | Iteration:  7700 | Batch:  812/1148 | Train loss: 0.1714 | Val loss: 0.2580\n",
      "   % Time:  8554 | Iteration:  7800 | Batch:  912/1148 | Train loss: 0.1662 | Val loss: 0.2584\n",
      "   % Time:  8663 | Iteration:  7900 | Batch: 1012/1148 | Train loss: 0.1698 | Val loss: 0.2574\n",
      "   % Time:  8772 | Iteration:  8000 | Batch: 1112/1148 | Train loss: 0.1660 | Val loss: 0.2586\n",
      "=> EPOCH 8\n",
      "   % Time:  8882 | Iteration:  8100 | Batch:   64/1148 | Train loss: 0.1593 | Val loss: 0.2568\n",
      "   % Time:  8993 | Iteration:  8200 | Batch:  164/1148 | Train loss: 0.1640 | Val loss: 0.2552\n",
      "   % Time:  9103 | Iteration:  8300 | Batch:  264/1148 | Train loss: 0.1581 | Val loss: 0.2564\n",
      "   % Time:  9214 | Iteration:  8400 | Batch:  364/1148 | Train loss: 0.1595 | Val loss: 0.2560\n",
      "   % Time:  9324 | Iteration:  8500 | Batch:  464/1148 | Train loss: 0.1591 | Val loss: 0.2559\n",
      "   % Time:  9434 | Iteration:  8600 | Batch:  564/1148 | Train loss: 0.1606 | Val loss: 0.2555\n",
      "   % Time:  9542 | Iteration:  8700 | Batch:  664/1148 | Train loss: 0.1557 | Val loss: 0.2555\n",
      "=> Adjust learning rate to: 0.0035\n",
      "   % Time:  9649 | Iteration:  8800 | Batch:  764/1148 | Train loss: 0.1621 | Val loss: 0.2526\n",
      "   % Time:  9756 | Iteration:  8900 | Batch:  864/1148 | Train loss: 0.1588 | Val loss: 0.2509\n",
      "   % Time:  9866 | Iteration:  9000 | Batch:  964/1148 | Train loss: 0.1587 | Val loss: 0.2501\n",
      "   % Time:  9976 | Iteration:  9100 | Batch: 1064/1148 | Train loss: 0.1569 | Val loss: 0.2503\n",
      "=> EPOCH 9\n",
      "   % Time: 10087 | Iteration:  9200 | Batch:   16/1148 | Train loss: 0.1599 | Val loss: 0.2503\n",
      "   % Time: 10197 | Iteration:  9300 | Batch:  116/1148 | Train loss: 0.1487 | Val loss: 0.2508\n",
      "   % Time: 10307 | Iteration:  9400 | Batch:  216/1148 | Train loss: 0.1525 | Val loss: 0.2500\n",
      "   % Time: 10417 | Iteration:  9500 | Batch:  316/1148 | Train loss: 0.1476 | Val loss: 0.2498\n",
      "   % Time: 10525 | Iteration:  9600 | Batch:  416/1148 | Train loss: 0.1519 | Val loss: 0.2503\n",
      "   % Time: 10633 | Iteration:  9700 | Batch:  516/1148 | Train loss: 0.1517 | Val loss: 0.2494\n",
      "   % Time: 10740 | Iteration:  9800 | Batch:  616/1148 | Train loss: 0.1502 | Val loss: 0.2507\n",
      "   % Time: 10851 | Iteration:  9900 | Batch:  716/1148 | Train loss: 0.1555 | Val loss: 0.2494\n",
      "   % Time: 10960 | Iteration: 10000 | Batch:  816/1148 | Train loss: 0.1480 | Val loss: 0.2500\n",
      "   % Time: 11070 | Iteration: 10100 | Batch:  916/1148 | Train loss: 0.1500 | Val loss: 0.2498\n",
      "   % Time: 11180 | Iteration: 10200 | Batch: 1016/1148 | Train loss: 0.1502 | Val loss: 0.2499\n",
      "=> Adjust learning rate to: 0.00175\n",
      "   % Time: 11287 | Iteration: 10300 | Batch: 1116/1148 | Train loss: 0.1501 | Val loss: 0.2488\n",
      "=> EPOCH 10\n",
      "   % Time: 11397 | Iteration: 10400 | Batch:   68/1148 | Train loss: 0.1424 | Val loss: 0.2492\n",
      "   % Time: 11508 | Iteration: 10500 | Batch:  168/1148 | Train loss: 0.1471 | Val loss: 0.2489\n",
      "   % Time: 11618 | Iteration: 10600 | Batch:  268/1148 | Train loss: 0.1440 | Val loss: 0.2492\n",
      "   % Time: 11728 | Iteration: 10700 | Batch:  368/1148 | Train loss: 0.1450 | Val loss: 0.2486\n",
      "   % Time: 11837 | Iteration: 10800 | Batch:  468/1148 | Train loss: 0.1438 | Val loss: 0.2487\n",
      "   % Time: 11948 | Iteration: 10900 | Batch:  568/1148 | Train loss: 0.1460 | Val loss: 0.2488\n",
      "   % Time: 12058 | Iteration: 11000 | Batch:  668/1148 | Train loss: 0.1509 | Val loss: 0.2488\n",
      "   % Time: 12167 | Iteration: 11100 | Batch:  768/1148 | Train loss: 0.1457 | Val loss: 0.2487\n",
      "   % Time: 12275 | Iteration: 11200 | Batch:  868/1148 | Train loss: 0.1438 | Val loss: 0.2484\n",
      "   % Time: 12383 | Iteration: 11300 | Batch:  968/1148 | Train loss: 0.1445 | Val loss: 0.2482\n",
      "   % Time: 12492 | Iteration: 11400 | Batch: 1068/1148 | Train loss: 0.1441 | Val loss: 0.2485\n",
      "=> EPOCH 11\n",
      "   % Time: 12602 | Iteration: 11500 | Batch:   20/1148 | Train loss: 0.1507 | Val loss: 0.2483\n",
      "   % Time: 12712 | Iteration: 11600 | Batch:  120/1148 | Train loss: 0.1468 | Val loss: 0.2488\n",
      "   % Time: 12822 | Iteration: 11700 | Batch:  220/1148 | Train loss: 0.1430 | Val loss: 0.2486\n",
      "   % Time: 12930 | Iteration: 11800 | Batch:  320/1148 | Train loss: 0.1407 | Val loss: 0.2485\n",
      "=> Adjust learning rate to: 0.000875\n",
      "   % Time: 13040 | Iteration: 11900 | Batch:  420/1148 | Train loss: 0.1469 | Val loss: 0.2483\n",
      "   % Time: 13150 | Iteration: 12000 | Batch:  520/1148 | Train loss: 0.1425 | Val loss: 0.2482\n",
      "   % Time: 13260 | Iteration: 12100 | Batch:  620/1148 | Train loss: 0.1413 | Val loss: 0.2480\n",
      "   % Time: 13370 | Iteration: 12200 | Batch:  720/1148 | Train loss: 0.1416 | Val loss: 0.2481\n",
      "   % Time: 13480 | Iteration: 12300 | Batch:  820/1148 | Train loss: 0.1467 | Val loss: 0.2478\n",
      "   % Time: 13589 | Iteration: 12400 | Batch:  920/1148 | Train loss: 0.1408 | Val loss: 0.2480\n",
      "   % Time: 13699 | Iteration: 12500 | Batch: 1020/1148 | Train loss: 0.1443 | Val loss: 0.2479\n",
      "   % Time: 13809 | Iteration: 12600 | Batch: 1120/1148 | Train loss: 0.1423 | Val loss: 0.2476\n",
      "=> EPOCH 12\n",
      "   % Time: 13917 | Iteration: 12700 | Batch:   72/1148 | Train loss: 0.1462 | Val loss: 0.2477\n",
      "   % Time: 14024 | Iteration: 12800 | Batch:  172/1148 | Train loss: 0.1389 | Val loss: 0.2480\n",
      "   % Time: 14132 | Iteration: 12900 | Batch:  272/1148 | Train loss: 0.1428 | Val loss: 0.2479\n",
      "   % Time: 14240 | Iteration: 13000 | Batch:  372/1148 | Train loss: 0.1424 | Val loss: 0.2478\n",
      "   % Time: 14347 | Iteration: 13100 | Batch:  472/1148 | Train loss: 0.1413 | Val loss: 0.2477\n",
      "=> Adjust learning rate to: 0.0004375\n",
      "   % Time: 14455 | Iteration: 13200 | Batch:  572/1148 | Train loss: 0.1399 | Val loss: 0.2477\n",
      "   % Time: 14565 | Iteration: 13300 | Batch:  672/1148 | Train loss: 0.1441 | Val loss: 0.2476\n",
      "   % Time: 14674 | Iteration: 13400 | Batch:  772/1148 | Train loss: 0.1412 | Val loss: 0.2476\n",
      "   % Time: 14784 | Iteration: 13500 | Batch:  872/1148 | Train loss: 0.1438 | Val loss: 0.2476\n",
      "   % Time: 14894 | Iteration: 13600 | Batch:  972/1148 | Train loss: 0.1428 | Val loss: 0.2476\n",
      "   % Time: 15001 | Iteration: 13700 | Batch: 1072/1148 | Train loss: 0.1394 | Val loss: 0.2476\n",
      "=> EPOCH 13\n",
      "   % Time: 15109 | Iteration: 13800 | Batch:   24/1148 | Train loss: 0.1434 | Val loss: 0.2476\n",
      "   % Time: 15217 | Iteration: 13900 | Batch:  124/1148 | Train loss: 0.1401 | Val loss: 0.2475\n",
      "   % Time: 15323 | Iteration: 14000 | Batch:  224/1148 | Train loss: 0.1375 | Val loss: 0.2476\n",
      "   % Time: 15433 | Iteration: 14100 | Batch:  324/1148 | Train loss: 0.1399 | Val loss: 0.2477\n",
      "   % Time: 15543 | Iteration: 14200 | Batch:  424/1148 | Train loss: 0.1431 | Val loss: 0.2477\n",
      "   % Time: 15652 | Iteration: 14300 | Batch:  524/1148 | Train loss: 0.1374 | Val loss: 0.2477\n",
      "   % Time: 15763 | Iteration: 14400 | Batch:  624/1148 | Train loss: 0.1444 | Val loss: 0.2477\n",
      "=> Adjust learning rate to: 0.00021875\n",
      "   % Time: 15873 | Iteration: 14500 | Batch:  724/1148 | Train loss: 0.1415 | Val loss: 0.2477\n",
      "   % Time: 15983 | Iteration: 14600 | Batch:  824/1148 | Train loss: 0.1383 | Val loss: 0.2476\n",
      "   % Time: 16093 | Iteration: 14700 | Batch:  924/1148 | Train loss: 0.1427 | Val loss: 0.2476\n",
      "   % Time: 16203 | Iteration: 14800 | Batch: 1024/1148 | Train loss: 0.1394 | Val loss: 0.2476\n",
      "   % Time: 16313 | Iteration: 14900 | Batch: 1124/1148 | Train loss: 0.1388 | Val loss: 0.2476\n",
      "=> EPOCH 14\n",
      "   % Time: 16424 | Iteration: 15000 | Batch:   76/1148 | Train loss: 0.1387 | Val loss: 0.2476\n",
      "   % Time: 16534 | Iteration: 15100 | Batch:  176/1148 | Train loss: 0.1429 | Val loss: 0.2475\n",
      "   % Time: 16643 | Iteration: 15200 | Batch:  276/1148 | Train loss: 0.1386 | Val loss: 0.2475\n",
      "   % Time: 16753 | Iteration: 15300 | Batch:  376/1148 | Train loss: 0.1424 | Val loss: 0.2476\n",
      "   % Time: 16864 | Iteration: 15400 | Batch:  476/1148 | Train loss: 0.1403 | Val loss: 0.2476\n",
      "   % Time: 16974 | Iteration: 15500 | Batch:  576/1148 | Train loss: 0.1393 | Val loss: 0.2475\n",
      "   % Time: 17081 | Iteration: 15600 | Batch:  676/1148 | Train loss: 0.1401 | Val loss: 0.2475\n",
      "   % Time: 17188 | Iteration: 15700 | Batch:  776/1148 | Train loss: 0.1365 | Val loss: 0.2476\n",
      "   % Time: 17295 | Iteration: 15800 | Batch:  876/1148 | Train loss: 0.1400 | Val loss: 0.2475\n",
      "   % Time: 17403 | Iteration: 15900 | Batch:  976/1148 | Train loss: 0.1394 | Val loss: 0.2476\n",
      "   % Time: 17510 | Iteration: 16000 | Batch: 1076/1148 | Train loss: 0.1399 | Val loss: 0.2476\n",
      "=> Adjust learning rate to: 0.000109375\n",
      "=> EPOCH 15\n",
      "   % Time: 17618 | Iteration: 16100 | Batch:   28/1148 | Train loss: 0.1393 | Val loss: 0.2476\n",
      "   % Time: 17725 | Iteration: 16200 | Batch:  128/1148 | Train loss: 0.1411 | Val loss: 0.2476\n",
      "   % Time: 17833 | Iteration: 16300 | Batch:  228/1148 | Train loss: 0.1403 | Val loss: 0.2476\n",
      "   % Time: 17940 | Iteration: 16400 | Batch:  328/1148 | Train loss: 0.1380 | Val loss: 0.2476\n",
      "   % Time: 18049 | Iteration: 16500 | Batch:  428/1148 | Train loss: 0.1382 | Val loss: 0.2476\n",
      "=> Adjust learning rate to: 5.46875e-05\n",
      "   % Time: 18157 | Iteration: 16600 | Batch:  528/1148 | Train loss: 0.1379 | Val loss: 0.2476\n",
      "   % Time: 18266 | Iteration: 16700 | Batch:  628/1148 | Train loss: 0.1391 | Val loss: 0.2476\n",
      "   % Time: 18377 | Iteration: 16800 | Batch:  728/1148 | Train loss: 0.1427 | Val loss: 0.2476\n",
      "   % Time: 18487 | Iteration: 16900 | Batch:  828/1148 | Train loss: 0.1431 | Val loss: 0.2476\n",
      "   % Time: 18597 | Iteration: 17000 | Batch:  928/1148 | Train loss: 0.1420 | Val loss: 0.2476\n",
      "   % Time: 18707 | Iteration: 17100 | Batch: 1028/1148 | Train loss: 0.1395 | Val loss: 0.2476\n",
      "   % Time: 18817 | Iteration: 17200 | Batch: 1128/1148 | Train loss: 0.1419 | Val loss: 0.2476\n",
      "=> EPOCH 16\n",
      "   % Time: 18928 | Iteration: 17300 | Batch:   80/1148 | Train loss: 0.1393 | Val loss: 0.2476\n",
      "   % Time: 19038 | Iteration: 17400 | Batch:  180/1148 | Train loss: 0.1409 | Val loss: 0.2476\n",
      "   % Time: 19148 | Iteration: 17500 | Batch:  280/1148 | Train loss: 0.1405 | Val loss: 0.2476\n",
      "   % Time: 19258 | Iteration: 17600 | Batch:  380/1148 | Train loss: 0.1386 | Val loss: 0.2476\n",
      "   % Time: 19365 | Iteration: 17700 | Batch:  480/1148 | Train loss: 0.1386 | Val loss: 0.2476\n",
      "   % Time: 19472 | Iteration: 17800 | Batch:  580/1148 | Train loss: 0.1415 | Val loss: 0.2476\n",
      "   % Time: 19580 | Iteration: 17900 | Batch:  680/1148 | Train loss: 0.1402 | Val loss: 0.2476\n",
      "   % Time: 19688 | Iteration: 18000 | Batch:  780/1148 | Train loss: 0.1391 | Val loss: 0.2476\n",
      "   % Time: 19797 | Iteration: 18100 | Batch:  880/1148 | Train loss: 0.1398 | Val loss: 0.2475\n",
      "   % Time: 19907 | Iteration: 18200 | Batch:  980/1148 | Train loss: 0.1388 | Val loss: 0.2476\n",
      "   % Time: 20018 | Iteration: 18300 | Batch: 1080/1148 | Train loss: 0.1410 | Val loss: 0.2476\n",
      "=> EPOCH 17\n",
      "   % Time: 20128 | Iteration: 18400 | Batch:   32/1148 | Train loss: 0.1399 | Val loss: 0.2476\n",
      "   % Time: 20238 | Iteration: 18500 | Batch:  132/1148 | Train loss: 0.1398 | Val loss: 0.2476\n",
      "   % Time: 20348 | Iteration: 18600 | Batch:  232/1148 | Train loss: 0.1387 | Val loss: 0.2476\n",
      "=> Adjust learning rate to: 2.734375e-05\n",
      "   % Time: 20458 | Iteration: 18700 | Batch:  332/1148 | Train loss: 0.1424 | Val loss: 0.2476\n",
      "   % Time: 20568 | Iteration: 18800 | Batch:  432/1148 | Train loss: 0.1386 | Val loss: 0.2476\n",
      "   % Time: 20677 | Iteration: 18900 | Batch:  532/1148 | Train loss: 0.1388 | Val loss: 0.2476\n",
      "   % Time: 20787 | Iteration: 19000 | Batch:  632/1148 | Train loss: 0.1385 | Val loss: 0.2476\n",
      "   % Time: 20897 | Iteration: 19100 | Batch:  732/1148 | Train loss: 0.1394 | Val loss: 0.2476\n",
      "   % Time: 21007 | Iteration: 19200 | Batch:  832/1148 | Train loss: 0.1400 | Val loss: 0.2476\n",
      "   % Time: 21114 | Iteration: 19300 | Batch:  932/1148 | Train loss: 0.1391 | Val loss: 0.2476\n",
      "   % Time: 21221 | Iteration: 19400 | Batch: 1032/1148 | Train loss: 0.1390 | Val loss: 0.2476\n",
      "   % Time: 21328 | Iteration: 19500 | Batch: 1132/1148 | Train loss: 0.1389 | Val loss: 0.2476\n",
      "=> EPOCH 18\n",
      "   % Time: 21436 | Iteration: 19600 | Batch:   84/1148 | Train loss: 0.1403 | Val loss: 0.2476\n",
      "   % Time: 21544 | Iteration: 19700 | Batch:  184/1148 | Train loss: 0.1388 | Val loss: 0.2476\n",
      "=> Adjust learning rate to: 1.3671875e-05\n",
      "   % Time: 21651 | Iteration: 19800 | Batch:  284/1148 | Train loss: 0.1390 | Val loss: 0.2476\n",
      "   % Time: 21758 | Iteration: 19900 | Batch:  384/1148 | Train loss: 0.1396 | Val loss: 0.2476\n",
      "   % Time: 21866 | Iteration: 20000 | Batch:  484/1148 | Train loss: 0.1401 | Val loss: 0.2476\n",
      "   % Time: 21973 | Iteration: 20100 | Batch:  584/1148 | Train loss: 0.1392 | Val loss: 0.2476\n",
      "   % Time: 22083 | Iteration: 20200 | Batch:  684/1148 | Train loss: 0.1383 | Val loss: 0.2476\n",
      "=> Adjust learning rate to: 6.8359375e-06\n"
     ]
    }
   ],
   "source": [
    "if 1 == 0:  # change to True to train\n",
    "    iteration = n_total = train_loss = n_bad_loss = 0\n",
    "    stop = False\n",
    "    best_val_loss = 10\n",
    "    init = time.time()\n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        train(config, train_iter, model, criterion, optimizer, epoch)\n",
    "        if stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T10:25:17.491418Z",
     "start_time": "2017-04-25T19:20:49.986131+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme error rate (PER): 9.13\n",
      "Word error rate (WER): 39.78\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(config.best_model))\n",
    "test(test_iter, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T08:08:16.358336Z",
     "start_time": "2017-04-25T17:08:16.077900+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> codified\n",
      "= K AA1 D AH0 F AY2 D\n",
      "< K AA1 D AH0 F AY2 D\n",
      "\n",
      "> sitton\n",
      "= S IH1 T AH0 N\n",
      "< S IH1 T AH0 N\n",
      "\n",
      "> silesia\n",
      "= S IH0 L IY1 Z AH0\n",
      "< S AH0 L IY1 ZH AH0\n",
      "\n",
      "> taut\n",
      "= T AO1 T\n",
      "< T AO1 T\n",
      "\n",
      "> paloma\n",
      "= P AA0 L OW1 M AH0\n",
      "< P AH0 L OW1 M AH0\n",
      "\n",
      "> authement\n",
      "= AW1 TH M AH0 N T\n",
      "< AO1 TH AH0 M AH0 N T\n",
      "\n",
      "> scleroderma\n",
      "= S K L IH2 R AH0 D ER1 M AH0\n",
      "< S K L EH2 R OW0 D ER1 M AH0\n",
      "\n",
      "> breadth\n",
      "= B R EH1 D TH\n",
      "< B R EH1 D TH\n",
      "\n",
      "> sweigard\n",
      "= S W AY1 G ER0 D\n",
      "< S W EY1 G ER0 D\n",
      "\n",
      "> gurr\n",
      "= G ER1\n",
      "< G ER1\n",
      "\n",
      "> solangi\n",
      "= S OW0 L AA1 N JH IY0\n",
      "< S OW0 L AA1 N JH IY0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_iter.init_epoch()\n",
    "for i, batch in enumerate(test_iter):\n",
    "    show(batch, model)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### acknowledgment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is done in collaboration with [Hoang Le](https://github.com/hminle) and [Hoang Nguyen](https://github.com/hoangnguyen3892)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
