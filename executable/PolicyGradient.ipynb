{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Source: https://github.com/pytorch/examples/tree/master/reinforcement_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:55:27.841398Z",
     "start_time": "2017-04-08T02:55:27.838062+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import argparse\n",
    "from itertools import count # useful for counting loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:55:28.024970Z",
     "start_time": "2017-04-08T02:55:27.843496+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:55:28.230071Z",
     "start_time": "2017-04-08T02:55:28.027323+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:55:28.249498Z",
     "start_time": "2017-04-08T02:55:28.232362+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd934077b70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = {\n",
    "    'gamma': 0.99,  # discount\n",
    "    'render': False,  # draw or not, False for notebook\n",
    "    'seed': 7,  # seed for reprocedure\n",
    "    'log_interval': 10\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:55:28.300838Z",
     "start_time": "2017-04-08T02:55:28.251608+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-08 02:55:28,285] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init the environment and seed\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:55:28.323184Z",
     "start_time": "2017-04-08T02:55:28.303004+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"State is 4-dimensional vector and there are 2 actions.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return the score of each action (left or right)\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:55:28.367210Z",
     "start_time": "2017-04-08T02:55:28.325469+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# utilities\n",
    "def select_action(model, state):\n",
    "    # in PyTorch, we always use minibatch, so use unsqueeze to make\n",
    "    # fake batch 1 dimension\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = model(Variable(state))\n",
    "    action = probs.multinomial()\n",
    "    model.saved_actions.append(action)\n",
    "    return action.data\n",
    "\n",
    "def finish_episode(args, model, optimizer):\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (\n",
    "        rewards.std() + np.finfo(np.float32).eps)\n",
    "    # The following is REINFORCE update, using StochasticVariable node\n",
    "    # in the paper https://arxiv.org/abs/1506.05254 for more details.\n",
    "    # For StochasticVariable, to do backward, we need the reward value,\n",
    "    # so we have to reinforce reward value to the action!\n",
    "    # See more at http://incompleteideas.net/sutton/williams-92.pdf\n",
    "    # \"REward Increment = Nonnegative Factor x Offset Reinforcement\n",
    "    # x Characteristic Eligibility,\"\n",
    "    for action, r in zip(model.saved_actions, rewards):\n",
    "        action.reinforce(r)  # PyTorch implementation for Variable\n",
    "    optimizer.zero_grad()\n",
    "    # see http://pytorch.org/docs/autograd.html\n",
    "    # ?highlight=backward#torch.autograd.backward\n",
    "    autograd.backward(model.saved_actions,\n",
    "                      [None for _ in model.saved_actions])\n",
    "    optimizer.step()  # update parameters\n",
    "    del model.rewards[:]  # reset to empty list\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:55:28.409269Z",
     "start_time": "2017-04-08T02:55:28.369398+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(args, model, optimizer, running_reward, i_episode):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):  # Don't infinite loop while learning\n",
    "        action = select_action(model, state)\n",
    "        state, reward, done, _ = env.step(action[0,0])\n",
    "        if args.render:\n",
    "            env.render()\n",
    "        model.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode(args, model, optimizer)\n",
    "    if i_episode % args.log_interval == 0:\n",
    "        print(\"=> EPISODE {} | Last length: {} | Average length: {:.2f}\"\n",
    "              .format(i_episode, t, running_reward))\n",
    "    return running_reward, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:55:28.445944Z",
     "start_time": "2017-04-08T02:55:28.411465+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# init model and define optimizer\n",
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:06.543856Z",
     "start_time": "2017-04-08T02:55:28.448051+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPISODE 10 | Last length: 14 | Average length: 11.88\n",
      "=> EPISODE 20 | Last length: 27 | Average length: 12.28\n",
      "=> EPISODE 30 | Last length: 15 | Average length: 12.81\n",
      "=> EPISODE 40 | Last length: 38 | Average length: 13.47\n",
      "=> EPISODE 50 | Last length: 20 | Average length: 15.32\n",
      "=> EPISODE 60 | Last length: 63 | Average length: 18.94\n",
      "=> EPISODE 70 | Last length: 58 | Average length: 22.25\n",
      "=> EPISODE 80 | Last length: 132 | Average length: 26.05\n",
      "=> EPISODE 90 | Last length: 27 | Average length: 27.45\n",
      "=> EPISODE 100 | Last length: 23 | Average length: 27.29\n",
      "=> EPISODE 110 | Last length: 21 | Average length: 27.14\n",
      "=> EPISODE 120 | Last length: 38 | Average length: 29.51\n",
      "=> EPISODE 130 | Last length: 82 | Average length: 33.54\n",
      "=> EPISODE 140 | Last length: 180 | Average length: 40.27\n",
      "=> EPISODE 150 | Last length: 104 | Average length: 52.51\n",
      "=> EPISODE 160 | Last length: 24 | Average length: 52.36\n",
      "=> EPISODE 170 | Last length: 33 | Average length: 50.36\n",
      "=> EPISODE 180 | Last length: 34 | Average length: 48.44\n",
      "=> EPISODE 190 | Last length: 45 | Average length: 47.24\n",
      "=> EPISODE 200 | Last length: 44 | Average length: 46.88\n",
      "=> EPISODE 210 | Last length: 61 | Average length: 47.58\n",
      "=> EPISODE 220 | Last length: 60 | Average length: 48.80\n",
      "=> EPISODE 230 | Last length: 54 | Average length: 50.07\n",
      "=> EPISODE 240 | Last length: 65 | Average length: 50.72\n",
      "=> EPISODE 250 | Last length: 72 | Average length: 52.11\n",
      "=> EPISODE 260 | Last length: 84 | Average length: 53.70\n",
      "=> EPISODE 270 | Last length: 119 | Average length: 56.92\n",
      "=> EPISODE 280 | Last length: 150 | Average length: 62.45\n",
      "=> EPISODE 290 | Last length: 98 | Average length: 68.86\n",
      "=> EPISODE 300 | Last length: 80 | Average length: 70.19\n",
      "=> EPISODE 310 | Last length: 47 | Average length: 68.77\n",
      "=> EPISODE 320 | Last length: 48 | Average length: 67.14\n",
      "=> EPISODE 330 | Last length: 57 | Average length: 65.90\n",
      "=> EPISODE 340 | Last length: 38 | Average length: 64.97\n",
      "=> EPISODE 350 | Last length: 92 | Average length: 65.24\n",
      "=> EPISODE 360 | Last length: 66 | Average length: 66.68\n",
      "=> EPISODE 370 | Last length: 115 | Average length: 69.63\n",
      "=> EPISODE 380 | Last length: 108 | Average length: 76.18\n",
      "=> EPISODE 390 | Last length: 80 | Average length: 77.70\n",
      "=> EPISODE 400 | Last length: 117 | Average length: 78.83\n",
      "=> EPISODE 410 | Last length: 64 | Average length: 78.10\n",
      "=> EPISODE 420 | Last length: 79 | Average length: 79.16\n",
      "=> EPISODE 430 | Last length: 207 | Average length: 84.07\n",
      "=> EPISODE 440 | Last length: 167 | Average length: 87.91\n",
      "=> EPISODE 450 | Last length: 58 | Average length: 87.67\n",
      "=> EPISODE 460 | Last length: 57 | Average length: 86.70\n",
      "=> EPISODE 470 | Last length: 89 | Average length: 85.46\n",
      "=> EPISODE 480 | Last length: 92 | Average length: 85.02\n",
      "=> EPISODE 490 | Last length: 83 | Average length: 88.04\n",
      "=> EPISODE 500 | Last length: 202 | Average length: 94.75\n",
      "=> EPISODE 510 | Last length: 154 | Average length: 110.25\n",
      "=> EPISODE 520 | Last length: 130 | Average length: 113.28\n",
      "=> EPISODE 530 | Last length: 125 | Average length: 115.48\n",
      "=> EPISODE 540 | Last length: 164 | Average length: 117.99\n",
      "=> EPISODE 550 | Last length: 266 | Average length: 126.14\n",
      "Solved! Running reward is now 252.16755459433054 and the last episode runs to 9999 time steps!\n"
     ]
    }
   ],
   "source": [
    "# training, using running_reward is criterion for stop\n",
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    running_reward, t = train(args, model, optimizer,\n",
    "                              running_reward, i_episode)\n",
    "    if running_reward > 200:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\"\n",
    "              .format(running_reward, t))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:25.138966Z",
     "start_time": "2017-04-08T02:56:24.981129+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "from itertools import count\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:25.143796Z",
     "start_time": "2017-04-08T02:56:25.140987+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:25.372317Z",
     "start_time": "2017-04-08T02:56:25.145824+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:25.390755Z",
     "start_time": "2017-04-08T02:56:25.374621+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f64ec03cb70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"render\": False,\n",
    "    \"seed\": 7,\n",
    "    \"log_interval\": 10\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:25.432743Z",
     "start_time": "2017-04-08T02:56:25.392536+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-08 02:56:25,417] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:25.450335Z",
     "start_time": "2017-04-08T02:56:25.434599+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['action', 'value'])\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.action_head = nn.Linear(128, 2)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.action_head(x)\n",
    "        state_values = self.value_head(x)\n",
    "        return F.softmax(action_scores), state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:25.496618Z",
     "start_time": "2017-04-08T02:56:25.452657+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_action(model, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs, state_value = model(Variable(state))\n",
    "    action = probs.multinomial()\n",
    "    model.saved_actions.append(SavedAction(action, state_value))\n",
    "    return action.data\n",
    "\n",
    "def finish_episode(args, model, optimizer):\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    value_loss = Variable(torch.Tensor([0]))\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / rewards.std()\n",
    "    # indeed, this is still REINFORCE, as in page 273 of Sutton book\n",
    "    # http://incompleteideas.net/sutton/book/bookdraft2016sep.pdf\n",
    "    for (action, value), r in zip(saved_actions, rewards):\n",
    "        reward = r - value.data[0, 0] # Tensor -> numpy\n",
    "        action.reinforce(reward)\n",
    "        value_loss += F.smooth_l1_loss(value, Variable(torch.Tensor([r])))\n",
    "    optimizer.zero_grad()\n",
    "    final_nodes = [value_loss] + list(map(lambda p: p.action, saved_actions))\n",
    "    gradients = [torch.ones(1)] + [None] * len(saved_actions)\n",
    "    autograd.backward(final_nodes, gradients)\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:25.533545Z",
     "start_time": "2017-04-08T02:56:25.498559+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(args, model, optimizer, running_reward, i_episode):\n",
    "    state = env.reset()\n",
    "    for t in range(10000): # Don't infinite loop while learning\n",
    "        action = select_action(model, state)\n",
    "        state, reward, done, _ = env.step(action[0,0])\n",
    "        if args.render:\n",
    "            env.render()\n",
    "        model.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode(args, model, optimizer)\n",
    "    if i_episode % args.log_interval == 0:\n",
    "        print(\"EPISODE {} | Last length: {} | Average length: {:.2f}\"\n",
    "              .format(i_episode, t, running_reward))\n",
    "    return running_reward, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:25.581036Z",
     "start_time": "2017-04-08T02:56:25.535572+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T17:56:44.503342Z",
     "start_time": "2017-04-08T02:56:25.583297+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 10 | Last length: 10 | Average length: 10.14\n",
      "EPISODE 20 | Last length: 15 | Average length: 10.18\n",
      "EPISODE 30 | Last length: 9 | Average length: 10.89\n",
      "EPISODE 40 | Last length: 36 | Average length: 11.65\n",
      "EPISODE 50 | Last length: 16 | Average length: 13.09\n",
      "EPISODE 60 | Last length: 47 | Average length: 15.02\n",
      "EPISODE 70 | Last length: 157 | Average length: 22.00\n",
      "EPISODE 80 | Last length: 201 | Average length: 30.53\n",
      "EPISODE 90 | Last length: 646 | Average length: 47.55\n",
      "Solved! Running reward is now 228.5757622076188 and the last episode runs to 4846 time steps!\n"
     ]
    }
   ],
   "source": [
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    running_reward, t = train(args, model, optimizer,\n",
    "                              running_reward, i_episode)\n",
    "    if running_reward > 200:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\"\n",
    "              .format(running_reward, t))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
