{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/pytorch/examples/tree/master/reinforcement_learning and https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from itertools import count # useful for counting loop\n",
    "import argparse\n",
    "import gym\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    \"gamma\": 0.99, # discount\n",
    "    \"seed\": 543, # seed for reprocedure\n",
    "    \"render\": False, # draw or not\n",
    "    \"log_interval\": 10, # when to print results\n",
    "}\n",
    "args = argparse.Namespace(**parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-21 22:14:55,190] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f41bf2c2960>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init the environment and seeds\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"State is 4-dimensional vector and there are 2 actions.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return the score of each action (left or right)\n",
    "        # state-action value / Q-learning\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init model and define optimizer\n",
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilities\n",
    "def select_action(state):\n",
    "    # in PyTorch, we always use minibatch, so use unsqueeze to make\n",
    "    # fake batch 1 dimension\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = model(Variable(state))\n",
    "    action = probs.multinomial()\n",
    "    model.saved_actions.append(action)\n",
    "    return action.data\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / rewards.std()\n",
    "    # the following is REINFORCE update, using StochasticVariable node\n",
    "    # see the paper https://arxiv.org/abs/1506.05254 for more details\n",
    "    for action, r in zip(model.saved_actions, rewards):\n",
    "        action.reinforce(r) # PyTorch implementation for Variable\n",
    "    optimizer.zero_grad()\n",
    "    autograd.backward(model.saved_actions, [None for _ in model.saved_actions])\n",
    "    optimizer.step() # update parameters\n",
    "    del model.rewards[:] # reset to empty list\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast length:    13\tAverage length: 10.64\n",
      "Episode 20\tLast length:    24\tAverage length: 11.37\n",
      "Episode 30\tLast length:   115\tAverage length: 15.63\n",
      "Episode 40\tLast length:    17\tAverage length: 19.16\n",
      "Episode 50\tLast length:    77\tAverage length: 22.33\n",
      "Episode 60\tLast length:    52\tAverage length: 24.56\n",
      "Episode 70\tLast length:    67\tAverage length: 28.63\n",
      "Episode 80\tLast length:   277\tAverage length: 41.01\n",
      "Episode 90\tLast length:   136\tAverage length: 84.41\n",
      "Episode 100\tLast length:    85\tAverage length: 84.43\n",
      "Episode 110\tLast length:   368\tAverage length: 90.95\n",
      "Solved! Running reward is now 292.54680160988335 and the last episode runs to 9999 time steps!\n"
     ]
    }
   ],
   "source": [
    "# training, using running_reward is criterion for stop\n",
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    for t in range(10000): # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action[0,0])\n",
    "        if args.render:\n",
    "            env.render()\n",
    "        model.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode()\n",
    "    if i_episode % args.log_interval == 0:\n",
    "        print(\"Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}\"\n",
    "              .format(i_episode, t, running_reward))\n",
    "    if running_reward > 200:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\"\n",
    "              .format(running_reward, t))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"seed\": 543,\n",
    "    \"render\": False,\n",
    "    \"log_interval\": 10,\n",
    "}\n",
    "args = argparse.Namespace(**parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-21 22:49:57,651] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f41bf2c2960>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['action', 'value'])\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.action_head = nn.Linear(128, 2)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.action_head(x)\n",
    "        state_values = self.value_head(x)\n",
    "        return F.softmax(action_scores), state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs, state_value = model(Variable(state))\n",
    "    action = probs.multinomial()\n",
    "    model.saved_actions.append(SavedAction(action, state_value))\n",
    "    return action.data\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    value_loss = 0\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / rewards.std()\n",
    "    for (action, value), r in zip(saved_actions, rewards):\n",
    "        action.reinforce(r - value.data.squeeze())\n",
    "        value_loss += F.smooth_l1_loss(value, Variable(torch.Tensor([r])))\n",
    "    optimizer.zero_grad()\n",
    "    final_nodes = [value_loss] + list(map(lambda p: p.action, saved_actions))\n",
    "    gradients = [torch.ones(1)] + [None] * len(saved_actions)\n",
    "    autograd.backward(final_nodes, gradients)\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast length:   132\tAverage length: 14.91\n",
      "Episode 20\tLast length:    21\tAverage length: 15.53\n",
      "Episode 30\tLast length:    50\tAverage length: 16.97\n",
      "Episode 40\tLast length:   221\tAverage length: 22.16\n",
      "Episode 50\tLast length:    51\tAverage length: 30.68\n",
      "Episode 60\tLast length:    97\tAverage length: 34.25\n",
      "Episode 70\tLast length:   258\tAverage length: 111.09\n",
      "Episode 80\tLast length:    95\tAverage length: 109.82\n",
      "Solved! Running reward is now 221.82367067917187 and the last episode runs to 9999 time steps!\n"
     ]
    }
   ],
   "source": [
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    for t in range(10000): # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action[0,0])\n",
    "        if args.render:\n",
    "            env.render()\n",
    "        model.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode()\n",
    "    if i_episode % args.log_interval == 0:\n",
    "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "            i_episode, t, running_reward))\n",
    "    if running_reward > 200:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) with Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
