{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Source: https://devblogs.nvidia.com/parallelforall/recursive-neural-networks-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:42:08.512926Z",
     "start_time": "2017-04-11T21:42:08.508859+09:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import itertools\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:42:09.518927Z",
     "start_time": "2017-04-11T21:42:08.514858+09:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:42:09.570057Z",
     "start_time": "2017-04-11T21:42:09.520947+09:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch/torchtext SNLI example')\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--d_embed', type=int, default=300)\n",
    "    parser.add_argument('--d_proj', type=int, default=300)\n",
    "    parser.add_argument('--d_hidden', type=int, default=300)\n",
    "    parser.add_argument('--d_mlp', type=int, default=600)\n",
    "    parser.add_argument('--n_mlp_layers', type=int, default=3)\n",
    "    parser.add_argument('--d_tracker', type=int, default=None)\n",
    "    parser.add_argument('--n_layers', type=int, default=1)\n",
    "    parser.add_argument('--log_every', type=int, default=50)\n",
    "    parser.add_argument('--lr', type=float, default=.001)\n",
    "    parser.add_argument('--lr_decay_by', type=float, default=1)\n",
    "    parser.add_argument('--lr_decay_every', type=float, default=1)\n",
    "    parser.add_argument('--dev_every', type=int, default=1000)\n",
    "    parser.add_argument('--save_every', type=int, default=1000)\n",
    "    parser.add_argument('--embed_dropout', type=float, default=0.2)\n",
    "    parser.add_argument('--mlp_dropout', type=float, default=0.2)\n",
    "    parser.add_argument('--rnn_dropout', type=float, default=0.2)\n",
    "    parser.add_argument('--no-bidirectional', action='store_false', dest='birnn')\n",
    "    parser.add_argument('--preserve-case', action='store_false', dest='lower')\n",
    "    parser.add_argument('--no-projection', action='store_false', dest='projection')\n",
    "    parser.add_argument('--train_embed', action='store_false', dest='fix_emb')\n",
    "    parser.add_argument('--predict_transitions', action='store_true', dest='predict')\n",
    "    parser.add_argument('--spinn', action='store_true', dest='spinn')\n",
    "    parser.add_argument('--gpu', type=int, default=0)\n",
    "    parser.add_argument('--save_path', type=str, default='results')\n",
    "    parser.add_argument('--data_cache', type=str, default=os.path.join(os.getcwd(), '.data_cache'))\n",
    "    parser.add_argument('--vector_cache', type=str, default=os.path.join(os.getcwd(), '.vector_cache/input_vectors.pt'))\n",
    "    parser.add_argument('--word_vectors', type=str, default='glove.42B')\n",
    "    parser.add_argument('--resume_snapshot', type=str, default='')\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:42:09.616507Z",
     "start_time": "2017-04-11T21:42:09.572145+09:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/spinn/',\n",
    "    'epochs': 50,\n",
    "    'batch_size': 128,\n",
    "    'd_embed': 300,\n",
    "    'd_proj': 300,\n",
    "    'd_hidden': 300,\n",
    "    'd_mlp': 600,\n",
    "    'n_mlp_layers': 3,\n",
    "    'd_tracker': None,\n",
    "    'n_layers': 1,\n",
    "    'log_every': 50,\n",
    "    'lr': .001,\n",
    "    'lr_decay_by': 1,\n",
    "    'lr_decay_every': 1,\n",
    "    'dev_every': 1000,\n",
    "    'save_every': 1000,\n",
    "    'embed_dropout': 0.2,\n",
    "    'mlp_dropout': 0.2,\n",
    "    'rnn_dropout': 0.2,\n",
    "    'birnn': True,\n",
    "    'lower': True,\n",
    "    'projection': True,\n",
    "    'fix_emb': True,\n",
    "    'predict': True,\n",
    "    'spinn': True,\n",
    "    'gpu': 0,\n",
    "    'save_path': '../intermediate/spinn/results/',\n",
    "    'data_cache': '../intermediate/spinn/.data_cache',\n",
    "    'vector_cache': '../intermediate/spinn/.vector_cache/input_vectors.pt',\n",
    "    'word_vectors': 'glove.42B',\n",
    "    'resume_snapshot': ''\n",
    "}\n",
    "args = argparse.Namespace(**parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:42:09.921449Z",
     "start_time": "2017-04-11T21:42:09.618586+09:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tree_lstm(c1, c2, lstm_in):\n",
    "    a, i, f1, f2, o = lstm_in.chunk(5, 1)\n",
    "    c = a.tanh() * i.sigmoid() + f1.sigmoid() * c1 + f2.sigmoid() * c2\n",
    "    h = o.sigmoid() * c.tanh()\n",
    "    return h, c\n",
    "\n",
    "\n",
    "def bundle(lstm_iter):\n",
    "    if lstm_iter is None:\n",
    "        return None\n",
    "    lstm_iter = tuple(lstm_iter)\n",
    "    if lstm_iter[0] is None:\n",
    "        return None\n",
    "    return torch.cat(lstm_iter, 0).chunk(2, 1)\n",
    "\n",
    "\n",
    "def unbundle(state):\n",
    "    if state is None:\n",
    "        return itertools.repeat(None)\n",
    "    return torch.split(torch.cat(state, 1), 1, 0)\n",
    "\n",
    "\n",
    "class Reduce(nn.Module):\n",
    "    \"\"\"TreeLSTM composition module for SPINN.\n",
    "    The TreeLSTM has two or three inputs: the first two are the left and right\n",
    "    children being composed; the third is the current state of the tracker\n",
    "    LSTM if one is present in the SPINN model.\n",
    "    Args:\n",
    "        size: The size of the model state.\n",
    "        tracker_size: The size of the tracker LSTM hidden state, or None if no\n",
    "            tracker is present.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, tracker_size=None):\n",
    "        super(Reduce, self).__init__()\n",
    "        self.left = nn.Linear(size, 5 * size)\n",
    "        self.right = nn.Linear(size, 5 * size, bias=False)\n",
    "        if tracker_size is not None:\n",
    "            self.track = nn.Linear(tracker_size, 5 * size, bias=False)\n",
    "\n",
    "    def forward(self, left_in, right_in, tracking=None):\n",
    "        \"\"\"Perform batched TreeLSTM composition.\n",
    "        This implements the REDUCE operation of a SPINN in parallel for a\n",
    "        batch of nodes. The batch size is flexible; only provide this function\n",
    "        the nodes that actually need to be REDUCEd.\n",
    "        The TreeLSTM has two or three inputs: the first two are the left and\n",
    "        right children being composed; the third is the current state of the\n",
    "        tracker LSTM if one is present in the SPINN model. All are provided as\n",
    "        iterables and batched internally into tensors.\n",
    "        Additionally augments each new node with pointers to its children.\n",
    "        Args:\n",
    "            left_in: Iterable of ``B`` ~autograd.Variable objects containing\n",
    "                ``c`` and ``h`` concatenated for the left child of each node\n",
    "                in the batch.\n",
    "            right_in: Iterable of ``B`` ~autograd.Variable objects containing\n",
    "                ``c`` and ``h`` concatenated for the right child of each node\n",
    "                in the batch.\n",
    "            tracking: Iterable of ``B`` ~autograd.Variable objects containing\n",
    "                ``c`` and ``h`` concatenated for the tracker LSTM state of\n",
    "                each node in the batch, or None.\n",
    "        Returns:\n",
    "            out: Tuple of ``B`` ~autograd.Variable objects containing ``c`` and\n",
    "                ``h`` concatenated for the LSTM state of each new node. These\n",
    "                objects are also augmented with ``left`` and ``right``\n",
    "                attributes.\n",
    "        \"\"\"\n",
    "        left, right = bundle(left_in), bundle(right_in)\n",
    "        tracking = bundle(tracking)\n",
    "        lstm_in = self.left(left[0])\n",
    "        lstm_in += self.right(right[0])\n",
    "        if hasattr(self, 'track'):\n",
    "            lstm_in += self.track(tracking[0])\n",
    "        out = unbundle(tree_lstm(left[1], right[1], lstm_in))\n",
    "        # for o, l, r in zip(out, left_in, right_in):\n",
    "        #     o.left, o.right = l, r\n",
    "        return out\n",
    "\n",
    "\n",
    "class Tracker(nn.Module):\n",
    "\n",
    "    def __init__(self, size, tracker_size, predict):\n",
    "        super(Tracker, self).__init__()\n",
    "        self.rnn = nn.LSTMCell(3 * size, tracker_size)\n",
    "        if predict:\n",
    "            self.transition = nn.Linear(tracker_size, 4)\n",
    "        self.state_size = tracker_size\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.state = None\n",
    "\n",
    "    def forward(self, bufs, stacks):\n",
    "        buf = bundle(buf[-1] for buf in bufs)[0]\n",
    "        stack1 = bundle(stack[-1] for stack in stacks)[0]\n",
    "        stack2 = bundle(stack[-2] for stack in stacks)[0]\n",
    "        x = torch.cat((buf, stack1, stack2), 1)\n",
    "        if self.state is None:\n",
    "            self.state = 2 * [Variable(\n",
    "                x.data.new(x.size(0), self.state_size).zero_())] \n",
    "        self.state = self.rnn(x, self.state)\n",
    "        if hasattr(self, 'transition'):\n",
    "            return unbundle(self.state), self.transition(self.state[0])\n",
    "        return unbundle(self.state), None\n",
    "\n",
    "class SPINN(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(SPINN, self).__init__()\n",
    "        self.config = config\n",
    "        assert config.d_hidden == config.d_proj / 2\n",
    "        self.reduce = Reduce(config.d_hidden, config.d_tracker)\n",
    "        if config.d_tracker is not None:\n",
    "            self.tracker = Tracker(config.d_hidden, config.d_tracker,\n",
    "                                   predict=config.predict)\n",
    "\n",
    "    def forward(self, buffers, transitions):\n",
    "        buffers = [list(torch.split(b.squeeze(1), 1, 0))\n",
    "                   for b in torch.split(buffers, 1, 1)]\n",
    "        stacks = [[buf[0], buf[0]] for buf in buffers]\n",
    "\n",
    "        if hasattr(self, 'tracker'):\n",
    "            self.tracker.reset_state()\n",
    "        else:\n",
    "            assert transitions is not None\n",
    "\n",
    "        if transitions is not None:\n",
    "            num_transitions = transitions.size(0)\n",
    "            # trans_loss, trans_acc = 0, 0\n",
    "        else:\n",
    "            num_transitions = len(buffers[0]) * 2 - 3\n",
    "\n",
    "        for i in range(num_transitions):\n",
    "            if transitions is not None:\n",
    "                trans = transitions[i]\n",
    "            if hasattr(self, 'tracker'):\n",
    "                tracker_states, trans_hyp = self.tracker(buffers, stacks)\n",
    "                if trans_hyp is not None:\n",
    "                    trans = trans_hyp.max(1)[1]\n",
    "                    # if transitions is not None:\n",
    "                    #     trans_loss += F.cross_entropy(trans_hyp, trans)\n",
    "                    #     trans_acc += (trans_preds.data == trans.data).mean()\n",
    "                    # else:\n",
    "                    #     trans = trans_preds\n",
    "            else:\n",
    "                tracker_states = itertools.repeat(None)\n",
    "            lefts, rights, trackings = [], [], []\n",
    "            batch = zip(trans.data, buffers, stacks, tracker_states)\n",
    "            for transition, buf, stack, tracking in batch:\n",
    "                if transition == 3:  # shift\n",
    "                    stack.append(buf.pop())\n",
    "                elif transition == 2:  # reduce\n",
    "                    rights.append(stack.pop())\n",
    "                    lefts.append(stack.pop())\n",
    "                    trackings.append(tracking)\n",
    "            if rights:\n",
    "                reduced = iter(self.reduce(lefts, rights, trackings))\n",
    "                for transition, stack in zip(trans.data, stacks):\n",
    "                    if transition == 2:\n",
    "                        stack.append(next(reduced))\n",
    "        # if trans_loss is not 0:\n",
    "        return bundle([stack.pop() for stack in stacks])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:42:10.112625Z",
     "start_time": "2017-04-11T21:42:09.923518+09:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Bottle(nn.Module):\n",
    "\n",
    "    def forward(self, input):\n",
    "        if len(input.size()) <= 2:\n",
    "            return super(Bottle, self).forward(input)\n",
    "        size = input.size()[:2]\n",
    "        out = super(Bottle, self).forward(input.view(size[0] * size[1], -1))\n",
    "        return out.view(*size, -1)\n",
    "\n",
    "\n",
    "class Linear(Bottle, nn.Linear):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BatchNorm(Bottle, nn.BatchNorm1d):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Feature(nn.Module):\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(Feature, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(size * 4)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, prem, hypo):\n",
    "        return self.dropout(self.bn(torch.cat(\n",
    "            [prem, hypo, prem - hypo, prem * hypo], 1)))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.config = config\n",
    "        input_size = config.d_proj if config.projection else config.d_embed\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=config.d_hidden,\n",
    "                           num_layers=config.n_layers, dropout=config.rnn_dropout,\n",
    "                           bidirectional=config.birnn)\n",
    "\n",
    "    def forward(self, inputs, _):\n",
    "        batch_size = inputs.size()[1]\n",
    "        state_shape = self.config.n_cells, batch_size, self.config.d_hidden\n",
    "        h0 = c0 = Variable(inputs.data.new(*state_shape).zero_())\n",
    "        outputs, (ht, ct) = self.rnn(inputs, (h0, c0))\n",
    "        return ht[-1] if not self.config.birnn else ht[-2:].transpose(0, 1).contiguous().view(batch_size, -1)\n",
    "\n",
    "\n",
    "class SNLIClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(SNLIClassifier, self).__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(config.n_embed, config.d_embed)\n",
    "        self.projection = Linear(config.d_embed, config.d_proj)\n",
    "        self.embed_bn = BatchNorm(config.d_proj)\n",
    "        self.embed_dropout = nn.Dropout(p=config.embed_dropout)\n",
    "        self.encoder = SPINN(config) if config.spinn else Encoder(config)\n",
    "        feat_in_size = config.d_hidden * (\n",
    "            2 if self.config.birnn and not self.config.spinn else 1)\n",
    "        self.feature = Feature(feat_in_size, config.mlp_dropout)\n",
    "        self.mlp_dropout = nn.Dropout(p=config.mlp_dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        mlp_in_size = 4 * feat_in_size\n",
    "        mlp = [nn.Linear(mlp_in_size, config.d_mlp), self.relu,\n",
    "               nn.BatchNorm1d(config.d_mlp), self.mlp_dropout]\n",
    "        for i in range(config.n_mlp_layers - 1):\n",
    "            mlp.extend([nn.Linear(config.d_mlp, config.d_mlp), self.relu,\n",
    "                        nn.BatchNorm1d(config.d_mlp), self.mlp_dropout])\n",
    "        mlp.append(nn.Linear(config.d_mlp, config.d_out))\n",
    "        self.out = nn.Sequential(*mlp)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        prem_embed = self.embed(batch.premise)\n",
    "        hypo_embed = self.embed(batch.hypothesis)\n",
    "        if self.config.fix_emb:\n",
    "            prem_embed = Variable(prem_embed.data)\n",
    "            hypo_embed = Variable(hypo_embed.data)\n",
    "        if self.config.projection:\n",
    "            prem_embed = self.projection(prem_embed)  # no relu\n",
    "            hypo_embed = self.projection(hypo_embed)\n",
    "        prem_embed = self.embed_dropout(self.embed_bn(prem_embed))\n",
    "        hypo_embed = self.embed_dropout(self.embed_bn(hypo_embed))\n",
    "        if hasattr(batch, 'premise_transitions'):\n",
    "            prem_trans = batch.premise_transitions\n",
    "            hypo_trans = batch.hypothesis_transitions\n",
    "        else:\n",
    "            prem_trans = hypo_trans = None\n",
    "        premise = self.encoder(prem_embed, prem_trans)\n",
    "        hypothesis = self.encoder(hypo_embed, hypo_trans)\n",
    "        scores = self.out(self.feature(premise, hypothesis))\n",
    "        #print(premise[0][:5], hypothesis[0][:5])\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:42:10.178685Z",
     "start_time": "2017-04-11T21:42:10.114585+09:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(args, train_iter, dev_iter, model, criterion, opt, epoch):\n",
    "    train_iter.init_epoch()\n",
    "    n_correct = n_total = train_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        model.train(); opt.zero_grad()\n",
    "        for pg in opt.param_groups:\n",
    "            pg['lr'] = args.lr * (args.lr_decay_by ** (\n",
    "                iterations / len(train_iter) / args.lr_decay_every))\n",
    "        iterations += 1 \n",
    "        answer = model(batch)\n",
    "        #print(nn.functional.softmax(answer[0]).data.tolist(), batch.label.data[0])\n",
    "        n_correct += (torch.max(answer, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "        n_total += batch.batch_size\n",
    "        train_acc = 100. * n_correct/n_total\n",
    "        loss = criterion(answer, batch.label)\n",
    "        loss.backward(); opt.step(); train_loss += loss.data[0] * batch.batch_size\n",
    "        if iterations % args.save_every == 0:\n",
    "            snapshot_prefix = os.path.join(args.save_path, 'snapshot')\n",
    "            snapshot_path = snapshot_prefix + '_acc_{:.4f}_loss_{:.6f}_iter_{}_model.pt'.format(train_acc, train_loss / n_total, iterations)\n",
    "            torch.save(model.state_dict(), snapshot_path)\n",
    "            for f in glob.glob(snapshot_prefix + '*'):\n",
    "                if f != snapshot_path:\n",
    "                    os.remove(f)\n",
    "        if iterations % args.dev_every == 0:\n",
    "            model.eval(); dev_iter.init_epoch()\n",
    "            n_dev_correct = dev_loss = 0\n",
    "            for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "                 answer = model(dev_batch)\n",
    "                 n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).sum()\n",
    "                 dev_loss += criterion(answer, dev_batch.label).data[0] * dev_batch.batch_size\n",
    "            dev_acc = 100. * n_dev_correct / len(dev)\n",
    "            print(dev_log_template.format(time.time()-start,\n",
    "                epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                100. * (1+batch_idx) / len(train_iter), train_loss / n_total, dev_loss / len(dev), train_acc, dev_acc))\n",
    "            n_correct = n_total = train_loss = 0\n",
    "            if dev_acc > best_dev_acc:\n",
    "                best_dev_acc = dev_acc\n",
    "                snapshot_prefix = os.path.join(args.save_path, 'best_snapshot')\n",
    "                snapshot_path = snapshot_prefix + '_devacc_{}_devloss_{}__iter_{}_model.pt'.format(dev_acc, dev_loss / len(dev), iterations)\n",
    "                torch.save(model.state_dict(), snapshot_path)\n",
    "                for f in glob.glob(snapshot_prefix + '*'):\n",
    "                    if f != snapshot_path:\n",
    "                        os.remove(f)\n",
    "        elif iterations % args.log_every == 0:\n",
    "            print(log_template.format(time.time()-start,\n",
    "                epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                100. * (1+batch_idx) / len(train_iter), train_loss / n_total, ' '*8, n_correct / n_total*100, ' '*12))\n",
    "            n_correct = n_total = train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:42:10.237049Z",
     "start_time": "2017-04-11T21:42:10.180535+09:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:47:24.928945Z",
     "start_time": "2017-04-11T21:42:10.239126+09:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading\n",
      "extracting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "glove.42B.300d: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading word vectors from http://nlp.stanford.edu/data/glove.42B.300d.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "glove.42B.300d:  35%|███▌      | 662M/1.88G [03:01<05:32, 3.65MB/s]    \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-088bd1ec3ada>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mload_vectors\u001b[0;34m(self, wv_dir, wv_type, wv_dim, unk_init)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mwv_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_word_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mload_word_vectors\u001b[0;34m(root, wv_type, dim)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extracting word vectors into {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mread\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "if args.spinn:\n",
    "    inputs = datasets.snli.ParsedTextField(lower=args.lower)\n",
    "    transitions = datasets.snli.ShiftReduceField()\n",
    "else:\n",
    "    inputs = data.Field(lower=args.lower)\n",
    "    transitions = None\n",
    "answers = data.Field(sequential=False)\n",
    "\n",
    "train, dev, test = datasets.SNLI.splits(inputs, answers, transitions)\n",
    "\n",
    "inputs.build_vocab(train, dev, test)\n",
    "if args.word_vectors:\n",
    "    if os.path.isfile(args.vector_cache):\n",
    "        inputs.vocab.vectors = torch.load(args.vector_cache)\n",
    "    else:\n",
    "        inputs.vocab.load_vectors(wv_dir=args.data_cache, wv_type=args.word_vectors, wv_dim=args.d_embed)\n",
    "        os.makedirs(os.path.dirname(args.vector_cache), exist_ok=True)\n",
    "        torch.save(inputs.vocab.vectors, args.vector_cache)\n",
    "answers.build_vocab(train)\n",
    "\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, dev, test), batch_size=args.batch_size, device=args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:47:24.929547Z",
     "start_time": "2017-04-11T12:42:24.465Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = args\n",
    "config.n_embed = len(inputs.vocab)\n",
    "config.d_out = len(answers.vocab)\n",
    "config.n_cells = config.n_layers\n",
    "\n",
    "if config.birnn:\n",
    "    config.n_cells *= 2\n",
    "\n",
    "if config.spinn:\n",
    "    config.lr = 2e-3 # 3e-4\n",
    "    config.lr_decay_by = 0.75\n",
    "    config.lr_decay_every = 1 #0.6\n",
    "    config.regularization = 0 #3e-6\n",
    "    config.mlp_dropout = 0.07\n",
    "    config.embed_dropout = 0.08 # 0.17\n",
    "    config.n_mlp_layers = 2\n",
    "    config.d_tracker = 64\n",
    "    config.d_mlp = 1024\n",
    "    config.d_hidden = 300\n",
    "    config.d_embed = 300\n",
    "    config.d_proj = 600\n",
    "    torch.backends.cudnn.enabled = False\n",
    "else:\n",
    "    config.regularization = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:47:24.929949Z",
     "start_time": "2017-04-11T12:42:24.466Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SNLIClassifier(config)\n",
    "if config.spinn:\n",
    "    model.out[len(model.out._modules) - 1].weight.data.uniform_(-0.005, 0.005)\n",
    "if args.word_vectors:\n",
    "    model.embed.weight.data = inputs.vocab.vectors\n",
    "if args.gpu != -1:\n",
    "    model.cuda()\n",
    "if args.resume_snapshot:\n",
    "    model.load_state_dict(torch.load(args.resume_snapshot))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = optim.Adam(model.parameters(), lr=args.lr)\n",
    "opt = optim.RMSprop(model.parameters(), lr=config.lr, alpha=0.9, eps=1e-6,\n",
    "                weight_decay=config.regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-11T12:47:24.930351Z",
     "start_time": "2017-04-11T12:42:24.468Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "start = time.time()\n",
    "best_dev_acc = -1\n",
    "train_iter.repeat = False\n",
    "header = '  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy'\n",
    "dev_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{:12.4f}'.split(','))\n",
    "log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))\n",
    "os.makedirs(args.save_path, exist_ok=True)\n",
    "print(header)\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(args, train_iter, dev_iter, model, criterion, opt, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
