{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Source: https://devblogs.nvidia.com/parallelforall/recursive-neural-networks-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:28:44.417007Z",
     "start_time": "2017-04-17T16:28:44.413018+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import itertools\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:28:45.490819Z",
     "start_time": "2017-04-17T16:28:44.419292+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:28:45.522656Z",
     "start_time": "2017-04-17T16:28:45.493499+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/snli/',\n",
    "    'epochs': 1,  # default: 50\n",
    "    'batch_size': 128,\n",
    "    'd_embed': 300,\n",
    "    'd_proj': 300,\n",
    "    'd_hidden': 300,\n",
    "    'd_mlp': 600,\n",
    "    'n_mlp_layers': 3,\n",
    "    'd_tracker': None,\n",
    "    'n_layers': 1,\n",
    "    'log_every': 50,\n",
    "    'lr': .001,\n",
    "    'lr_decay_by': 1,\n",
    "    'lr_decay_every': 1,\n",
    "    'dev_every': 1000,\n",
    "    'save_every': 1000,\n",
    "    'embed_dropout': 0.2,\n",
    "    'mlp_dropout': 0.2,\n",
    "    'rnn_dropout': 0.2,\n",
    "    'birnn': True,\n",
    "    'lower': True,\n",
    "    'projection': True,\n",
    "    'fix_emb': True,\n",
    "    'predict': False,  # error when use\n",
    "    'spinn': True,\n",
    "    'gpu': 0,  # -1 for cpu\n",
    "    'save_path': '../intermediate/spinn/results/',\n",
    "    'data_cache': '../intermediate/spinn/.data_cache',\n",
    "    'vector_cache': '../intermediate/spinn/.vector_cache/input_vectors.pt',\n",
    "    'word_vectors': 'glove.42B',\n",
    "    'resume_snapshot': ''\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "if not os.path.isdir(args.data_path):\n",
    "    os.makedirs(args.data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:28:45.849863Z",
     "start_time": "2017-04-17T16:28:45.525299+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tree_lstm(c1, c2, lstm_in):\n",
    "    a, i, f1, f2, o = lstm_in.chunk(5, 1)\n",
    "    c = a.tanh() * i.sigmoid() + f1.sigmoid() * c1 + f2.sigmoid() * c2\n",
    "    h = o.sigmoid() * c.tanh()\n",
    "    return h, c\n",
    "\n",
    "\n",
    "def bundle(lstm_iter):\n",
    "    if lstm_iter is None:\n",
    "        return None\n",
    "    lstm_iter = tuple(lstm_iter)\n",
    "    if lstm_iter[0] is None:\n",
    "        return None\n",
    "    return torch.cat(lstm_iter, 0).chunk(2, 1)\n",
    "\n",
    "\n",
    "def unbundle(state):\n",
    "    if state is None:\n",
    "        return itertools.repeat(None)\n",
    "    return torch.split(torch.cat(state, 1), 1, 0)\n",
    "\n",
    "\n",
    "class Reduce(nn.Module):\n",
    "    \"\"\"TreeLSTM composition module for SPINN.\n",
    "    The TreeLSTM has two or three inputs: the first two are the left and\n",
    "    right children being composed; the third is the current state of the\n",
    "    tracker LSTM if one is present in the SPINN model.\n",
    "    Args:\n",
    "        size: The size of the model state.\n",
    "        tracker_size: The size of the tracker LSTM hidden state, or None if\n",
    "            no tracker is present.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, tracker_size=None):\n",
    "        super(Reduce, self).__init__()\n",
    "        self.left = nn.Linear(size, 5 * size)\n",
    "        self.right = nn.Linear(size, 5 * size, bias=False)\n",
    "        if tracker_size is not None:\n",
    "            self.track = nn.Linear(tracker_size, 5 * size, bias=False)\n",
    "\n",
    "    def forward(self, left_in, right_in, tracking=None):\n",
    "        \"\"\"Perform batched TreeLSTM composition.\n",
    "        This implements the REDUCE operation of a SPINN in parallel for a\n",
    "        batch of nodes. The batch size is flexible; only provide this\n",
    "        function the nodes that actually need to be REDUCEd.\n",
    "        The TreeLSTM has two or three inputs: the first two are the left and\n",
    "        right children being composed; the third is the current state of the\n",
    "        tracker LSTM if one is present in the SPINN model. All are provided\n",
    "        as iterables and batched internally into tensors.\n",
    "        Additionally augments each new node with pointers to its children.\n",
    "        Args:\n",
    "            left_in: Iterable of ``B`` ~autograd.Variable objects containing\n",
    "                ``c`` and ``h`` concatenated for the left child of each node\n",
    "                in the batch.\n",
    "            right_in: Iterable of ``B`` ~autograd.Variable objects containing\n",
    "                ``c`` and ``h`` concatenated for the right child of each node\n",
    "                in the batch.\n",
    "            tracking: Iterable of ``B`` ~autograd.Variable objects containing\n",
    "                ``c`` and ``h`` concatenated for the tracker LSTM state of\n",
    "                each node in the batch, or None.\n",
    "        Returns:\n",
    "            out: Tuple of ``B`` ~autograd.Variable objects containing ``c``\n",
    "                and ``h`` concatenated for the LSTM state of each new node.\n",
    "                These objects are also augmented with ``left`` and ``right``\n",
    "                attributes.\n",
    "        \"\"\"\n",
    "        left, right = bundle(left_in), bundle(right_in)\n",
    "        tracking = bundle(tracking)\n",
    "        lstm_in = self.left(left[0])\n",
    "        lstm_in += self.right(right[0])\n",
    "        if hasattr(self, 'track'):\n",
    "            lstm_in += self.track(tracking[0])\n",
    "        out = unbundle(tree_lstm(left[1], right[1], lstm_in))\n",
    "        # for o, l, r in zip(out, left_in, right_in):\n",
    "        #     o.left, o.right = l, r\n",
    "        return out\n",
    "\n",
    "\n",
    "class Tracker(nn.Module):\n",
    "\n",
    "    def __init__(self, size, tracker_size, predict):\n",
    "        super(Tracker, self).__init__()\n",
    "        self.rnn = nn.LSTMCell(3 * size, tracker_size)\n",
    "        if predict:\n",
    "            self.transition = nn.Linear(tracker_size, 4)\n",
    "        self.state_size = tracker_size\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.state = None\n",
    "\n",
    "    def forward(self, bufs, stacks):\n",
    "        buf = bundle(buf[-1] for buf in bufs)[0]\n",
    "        stack1 = bundle(stack[-1] for stack in stacks)[0]\n",
    "        stack2 = bundle(stack[-2] for stack in stacks)[0]\n",
    "        x = torch.cat((buf, stack1, stack2), 1)\n",
    "        if self.state is None:\n",
    "            self.state = 2 * [Variable(\n",
    "                x.data.new(x.size(0), self.state_size).zero_())]\n",
    "        self.state = self.rnn(x, self.state)\n",
    "        if hasattr(self, 'transition'):\n",
    "            return unbundle(self.state), self.transition(self.state[0])\n",
    "        return unbundle(self.state), None\n",
    "\n",
    "\n",
    "class SPINN(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(SPINN, self).__init__()\n",
    "        self.config = config\n",
    "        assert config.d_hidden == config.d_proj / 2\n",
    "        self.reduce = Reduce(config.d_hidden, config.d_tracker)\n",
    "        if config.d_tracker is not None:\n",
    "            self.tracker = Tracker(config.d_hidden, config.d_tracker,\n",
    "                                   predict=config.predict)\n",
    "\n",
    "    def forward(self, buffers, transitions):\n",
    "        buffers = [list(torch.split(b.squeeze(1), 1, 0))\n",
    "                   for b in torch.split(buffers, 1, 1)]\n",
    "        stacks = [[buf[0], buf[0]] for buf in buffers]\n",
    "\n",
    "        if hasattr(self, 'tracker'):\n",
    "            self.tracker.reset_state()\n",
    "        else:\n",
    "            assert transitions is not None\n",
    "\n",
    "        if transitions is not None:\n",
    "            num_transitions = transitions.size(0)\n",
    "            # trans_loss, trans_acc = 0, 0\n",
    "        else:\n",
    "            num_transitions = len(buffers[0]) * 2 - 3\n",
    "        \n",
    "        for i in range(num_transitions):\n",
    "            if transitions is not None:\n",
    "                trans = transitions[i]\n",
    "            if hasattr(self, 'tracker'):\n",
    "                tracker_states, trans_hyp = self.tracker(buffers, stacks)\n",
    "                if trans_hyp is not None:\n",
    "                    trans = trans_hyp.max(1)[1]\n",
    "                    # if transitions is not None:\n",
    "                    #     trans_loss += F.cross_entropy(trans_hyp, trans)\n",
    "                    #     trans_acc += (trans_preds.data\n",
    "                    #         == trans.data).mean()\n",
    "                    # else:\n",
    "                    #     trans = trans_preds\n",
    "            else:\n",
    "                tracker_states = itertools.repeat(None)\n",
    "            lefts, rights, trackings = [], [], []\n",
    "            batch = zip(trans.data, buffers, stacks, tracker_states)\n",
    "            for transition, buf, stack, tracking in batch:\n",
    "                if transition == 3:  # shift\n",
    "                    stack.append(buf.pop())\n",
    "                elif transition == 2:  # reduce\n",
    "                    rights.append(stack.pop())\n",
    "                    lefts.append(stack.pop())\n",
    "                    trackings.append(tracking)\n",
    "            if rights:\n",
    "                reduced = iter(self.reduce(lefts, rights, trackings))\n",
    "                for transition, stack in zip(trans.data, stacks):\n",
    "                    if transition == 2:\n",
    "                        stack.append(next(reduced))\n",
    "\n",
    "        # if trans_loss is not 0:\n",
    "        return bundle([stack.pop() for stack in stacks])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:28:46.042593Z",
     "start_time": "2017-04-17T16:28:45.852268+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Bottle(nn.Module):\n",
    "\n",
    "    def forward(self, input):\n",
    "        if len(input.size()) <= 2:\n",
    "            return super(Bottle, self).forward(input)\n",
    "        size = input.size()[:2]\n",
    "        out = super(Bottle, self).forward(input.view(size[0] * size[1], -1))\n",
    "        return out.view(*size, -1)\n",
    "\n",
    "\n",
    "class Linear(Bottle, nn.Linear):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BatchNorm(Bottle, nn.BatchNorm1d):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Feature(nn.Module):\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(Feature, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(size * 4)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, prem, hypo):\n",
    "        return self.dropout(self.bn(torch.cat(\n",
    "            [prem, hypo, prem - hypo, prem * hypo], 1)))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.config = config\n",
    "        input_size = config.d_proj if config.projection else config.d_embed\n",
    "        self.rnn = nn.LSTM(input_size=input_size,\n",
    "                           hidden_size=config.d_hidden,\n",
    "                           num_layers=config.n_layers,\n",
    "                           dropout=config.rnn_dropout,\n",
    "                           bidirectional=config.birnn)\n",
    "\n",
    "    def forward(self, inputs, _):\n",
    "        batch_size = inputs.size()[1]\n",
    "        state_shape = self.config.n_cells, batch_size, self.config.d_hidden\n",
    "        h0 = c0 = Variable(inputs.data.new(*state_shape).zero_())\n",
    "        outputs, (ht, ct) = self.rnn(inputs, (h0, c0))\n",
    "        return (ht[-1] if not self.config.birnn else ht[-2:].transpose(0, 1)\n",
    "                    .contiguous().view(batch_size, -1))\n",
    "\n",
    "\n",
    "class SNLIClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(SNLIClassifier, self).__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(config.n_embed, config.d_embed)\n",
    "        self.projection = Linear(config.d_embed, config.d_proj)\n",
    "        self.embed_bn = BatchNorm(config.d_proj)\n",
    "        self.embed_dropout = nn.Dropout(p=config.embed_dropout)\n",
    "        self.encoder = SPINN(config) if config.spinn else Encoder(config)\n",
    "        feat_in_size = config.d_hidden * (\n",
    "            2 if self.config.birnn and not self.config.spinn else 1)\n",
    "        self.feature = Feature(feat_in_size, config.mlp_dropout)\n",
    "        self.mlp_dropout = nn.Dropout(p=config.mlp_dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        mlp_in_size = 4 * feat_in_size\n",
    "        mlp = [nn.Linear(mlp_in_size, config.d_mlp), self.relu,\n",
    "               nn.BatchNorm1d(config.d_mlp), self.mlp_dropout]\n",
    "        for i in range(config.n_mlp_layers - 1):\n",
    "            mlp.extend([nn.Linear(config.d_mlp, config.d_mlp), self.relu,\n",
    "                        nn.BatchNorm1d(config.d_mlp), self.mlp_dropout])\n",
    "        mlp.append(nn.Linear(config.d_mlp, config.d_out))\n",
    "        self.out = nn.Sequential(*mlp)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        prem_embed = self.embed(batch.premise)\n",
    "        hypo_embed = self.embed(batch.hypothesis)\n",
    "        if self.config.fix_emb:\n",
    "            prem_embed = Variable(prem_embed.data)\n",
    "            hypo_embed = Variable(hypo_embed.data)\n",
    "        if self.config.projection:\n",
    "            prem_embed = self.projection(prem_embed)  # no relu\n",
    "            hypo_embed = self.projection(hypo_embed)\n",
    "        prem_embed = self.embed_dropout(self.embed_bn(prem_embed))\n",
    "        hypo_embed = self.embed_dropout(self.embed_bn(hypo_embed))\n",
    "        if hasattr(batch, 'premise_transitions'):\n",
    "            prem_trans = batch.premise_transitions\n",
    "            hypo_trans = batch.hypothesis_transitions\n",
    "        else:\n",
    "            prem_trans = hypo_trans = None\n",
    "        premise = self.encoder(prem_embed, prem_trans)\n",
    "        hypothesis = self.encoder(hypo_embed, hypo_trans)\n",
    "        scores = self.out(self.feature(premise, hypothesis))\n",
    "        #print(premise[0][:5], hypothesis[0][:5])\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:28:46.139866Z",
     "start_time": "2017-04-17T16:28:46.045011+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(args, epoch):\n",
    "    # global train_iter\n",
    "    # global dev_iter\n",
    "    global iterations\n",
    "    global best_dev_acc\n",
    "    train_iter.init_epoch()\n",
    "    n_correct = n_total = train_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        model.train(); opt.zero_grad()\n",
    "        for pg in opt.param_groups:\n",
    "            pg['lr'] = args.lr * (args.lr_decay_by ** (\n",
    "                iterations / len(train_iter) / args.lr_decay_every))\n",
    "        iterations += 1 \n",
    "        answer = model(batch)\n",
    "        #print(nn.functional.softmax(answer[0]).data.tolist(),\n",
    "        #    batch.label.data[0])\n",
    "        n_correct += (torch.max(answer, 1)[1].view(batch.label.size()).data\n",
    "                      == batch.label.data).sum()\n",
    "        n_total += batch.batch_size\n",
    "        train_acc = 100. * n_correct/n_total\n",
    "        loss = criterion(answer, batch.label)\n",
    "        loss.backward(); opt.step()\n",
    "        train_loss += loss.data[0] * batch.batch_size\n",
    "        if iterations % args.save_every == 0:\n",
    "            snapshot_prefix = os.path.join(args.save_path, 'snapshot')\n",
    "            snapshot_path = (snapshot_prefix + \n",
    "                             '_acc_{:.4f}_loss_{:.6f}_iter_{}_model.pt'\n",
    "                             .format(train_acc, train_loss / n_total,\n",
    "                                     iterations))\n",
    "            torch.save(model.state_dict(), snapshot_path)\n",
    "            for f in glob.glob(snapshot_prefix + '*'):\n",
    "                if f != snapshot_path:\n",
    "                    os.remove(f)\n",
    "        if iterations % args.dev_every == 0:\n",
    "            model.eval(); dev_iter.init_epoch()\n",
    "            n_dev_correct = dev_loss = 0\n",
    "            for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "                answer = model(dev_batch)\n",
    "                n_dev_correct += (torch.max(answer, 1)[1]\n",
    "                    .view(dev_batch.label.size())\n",
    "                    .data == dev_batch.label.data).sum()\n",
    "                dev_loss += criterion(answer,\n",
    "                    dev_batch.label).data[0] * dev_batch.batch_size\n",
    "            dev_acc = 100. * n_dev_correct / len(dev_data)\n",
    "            print(dev_log_template.format(time.time()-start,\n",
    "                epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                100. * (1+batch_idx) / len(train_iter), train_loss / n_total,\n",
    "                dev_loss / len(dev_data), train_acc, dev_acc))\n",
    "            n_correct = n_total = train_loss = 0\n",
    "            if dev_acc > best_dev_acc:\n",
    "                best_dev_acc = dev_acc\n",
    "                snapshot_prefix = os.path.join(args.save_path,\n",
    "                                               'best_snapshot')\n",
    "                snapshot_path = (snapshot_prefix +\n",
    "                                 '_devacc_{}_devloss_{}__iter_{}_model.pt'\n",
    "                                 .format(dev_acc, dev_loss / len(dev_data), \n",
    "                                         iterations))\n",
    "                torch.save(model.state_dict(), snapshot_path)\n",
    "                for f in glob.glob(snapshot_prefix + '*'):\n",
    "                    if f != snapshot_path:\n",
    "                        os.remove(f)\n",
    "        elif iterations % args.log_every == 0:\n",
    "            print(log_template.format(time.time()-start,\n",
    "                epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                100. * (1+batch_idx) / len(train_iter), train_loss / n_total,\n",
    "                ' '*8, n_correct / n_total*100, ' '*12))\n",
    "            n_correct = n_total = train_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:28:46.217485Z",
     "start_time": "2017-04-17T16:28:46.142314+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if args.spinn:\n",
    "    inputs = datasets.snli.ParsedTextField(lower=args.lower)\n",
    "    transitions = datasets.snli.ShiftReduceField()\n",
    "else:\n",
    "    inputs = data.Field(lower=args.lower)\n",
    "    transitions = None\n",
    "\n",
    "answers = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:31:55.656824Z",
     "start_time": "2017-04-17T16:28:46.220112+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# download snli data from data_path\n",
    "# transitions is shift-reduce field (parser)\n",
    "# inputs: pair of sentence (hypothesis and origin)\n",
    "# answers: neutral, ...\n",
    "train_data, dev_data, test_data = datasets.SNLI.splits(inputs, answers,\n",
    "                                                       transitions,\n",
    "                                                       root=args.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:32:08.436093Z",
     "start_time": "2017-04-17T16:31:55.658995+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build vocab for inputs Field\n",
    "inputs.build_vocab(train_data, dev_data, test_data)\n",
    "# build vocab for answers Field\n",
    "answers.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:32:08.464392Z",
     "start_time": "2017-04-17T16:32:08.438595+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load word vectors\n",
    "if args.word_vectors:\n",
    "    if os.path.isfile(args.vector_cache):\n",
    "        inputs.vocab.vectors = torch.load(args.vector_cache)\n",
    "    else:\n",
    "        inputs.vocab.load_vectors(wv_dir=args.data_cache,\n",
    "                                  wv_type=args.word_vectors,\n",
    "                                  wv_dim=args.d_embed)\n",
    "        os.makedirs(os.path.dirname(args.vector_cache), exist_ok=True)\n",
    "        torch.save(inputs.vocab.vectors, args.vector_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:32:08.501449Z",
     "start_time": "2017-04-17T16:32:08.466817+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# make iteration\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_data, dev_data, test_data), batch_size=args.batch_size,\n",
    "    device=args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:32:08.530430Z",
     "start_time": "2017-04-17T16:32:08.504294+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = args\n",
    "config.n_embed = len(inputs.vocab)\n",
    "config.d_out = len(answers.vocab)\n",
    "config.n_cells = config.n_layers\n",
    "\n",
    "if config.birnn:\n",
    "    config.n_cells *= 2\n",
    "\n",
    "if config.spinn:\n",
    "    config.lr = 2e-3 # 3e-4\n",
    "    config.lr_decay_by = 0.75\n",
    "    config.lr_decay_every = 1 #0.6\n",
    "    config.regularization = 0 #3e-6\n",
    "    config.mlp_dropout = 0.07\n",
    "    config.embed_dropout = 0.08 # 0.17\n",
    "    config.n_mlp_layers = 2\n",
    "    config.d_tracker = 64\n",
    "    config.d_mlp = 1024\n",
    "    config.d_hidden = 300\n",
    "    config.d_embed = 300\n",
    "    config.d_proj = 600\n",
    "    #torch.backends.cudnn.enabled = True\n",
    "else:\n",
    "    config.regularization = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T07:32:11.997945Z",
     "start_time": "2017-04-17T16:32:08.532908+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = SNLIClassifier(config)\n",
    "if config.spinn:\n",
    "    model.out[len(model.out._modules) - 1].weight.data.uniform_(-0.005,\n",
    "                                                                0.005)\n",
    "if args.word_vectors:\n",
    "    model.embed.weight.data = inputs.vocab.vectors\n",
    "if args.gpu != -1:\n",
    "    model.cuda()\n",
    "if args.resume_snapshot:\n",
    "    model.load_state_dict(torch.load(args.resume_snapshot))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = optim.Adam(model.parameters(), lr=args.lr)\n",
    "opt = optim.RMSprop(model.parameters(), lr=config.lr, alpha=0.9, eps=1e-6,\n",
    "                    weight_decay=config.regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T09:59:01.152035Z",
     "start_time": "2017-04-17T16:32:12.000529+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time Epoch Iteration    Progress (%Epoch)     Loss Dev/Loss     Accuracy Dev/Accuracy\n",
      "   106     0        50    50/4292        1% 1.210458               40.8594             \n",
      "   203     0       100   100/4292        2% 1.021304               50.1719             \n",
      "   304     0       150   150/4292        3% 0.979100               52.3438             \n",
      "   401     0       200   200/4292        5% 0.934748               56.6406             \n",
      "   506     0       250   250/4292        6% 0.910377               57.5781             \n",
      "   601     0       300   300/4292        7% 0.900572               58.9531             \n",
      "   700     0       350   350/4292        8% 0.842309               62.2031             \n",
      "   799     0       400   400/4292        9% 0.854539               61.7188             \n",
      "   893     0       450   450/4292       10% 0.839756               62.1875             \n",
      "   998     0       500   500/4292       12% 0.810422               65.4375             \n",
      "  1095     0       550   550/4292       13% 0.799220               64.5000             \n",
      "  1198     0       600   600/4292       14% 0.778205               66.6250             \n",
      "  1294     0       650   650/4292       15% 0.791936               65.5469             \n",
      "  1394     0       700   700/4292       16% 0.790956               65.2969             \n",
      "  1491     0       750   750/4292       17% 0.774214               66.1406             \n",
      "  1592     0       800   800/4292       19% 0.765715               66.9688             \n",
      "  1698     0       850   850/4292       20% 0.762952               67.1719             \n",
      "  1791     0       900   900/4292       21% 0.759205               67.3750             \n",
      "  1897     0       950   950/4292       22% 0.733377               68.7188             \n",
      "  2056     0      1000  1000/4292       23% 0.716036 0.697412      69.3594      69.9655\n",
      "  2149     0      1050  1050/4292       24% 0.732126               68.5000             \n",
      "  2252     0      1100  1100/4292       26% 0.729788               68.8594             \n",
      "  2349     0      1150  1150/4292       27% 0.718057               69.3906             \n",
      "  2452     0      1200  1200/4292       28% 0.714888               69.1094             \n",
      "  2553     0      1250  1250/4292       29% 0.717263               69.3281             \n",
      "  2651     0      1300  1300/4292       30% 0.729667               68.4531             \n",
      "  2747     0      1350  1350/4292       31% 0.696243               70.2344             \n",
      "  2851     0      1400  1400/4292       33% 0.716940               69.5781             \n",
      "  2951     0      1450  1450/4292       34% 0.688064               70.6875             \n",
      "  3048     0      1500  1500/4292       35% 0.698600               70.6875             \n",
      "  3145     0      1550  1550/4292       36% 0.704727               69.3281             \n",
      "  3248     0      1600  1600/4292       37% 0.695634               70.8750             \n",
      "  3350     0      1650  1650/4292       38% 0.690152               71.1094             \n",
      "  3448     0      1700  1700/4292       40% 0.667475               72.0156             \n",
      "  3550     0      1750  1750/4292       41% 0.699444               71.0000             \n",
      "  3647     0      1800  1800/4292       42% 0.656927               72.1250             \n",
      "  3747     0      1850  1850/4292       43% 0.709438               70.3438             \n",
      "  3849     0      1900  1900/4292       44% 0.675077               71.5156             \n",
      "  3940     0      1950  1950/4292       45% 0.667331               72.0469             \n",
      "  4110     0      2000  2000/4292       47% 0.697809 0.625695      70.6250      74.0500\n",
      "  4210     0      2050  2050/4292       48% 0.677222               71.3125             \n",
      "  4310     0      2100  2100/4292       49% 0.644549               72.7500             \n",
      "  4412     0      2150  2150/4292       50% 0.667121               71.7500             \n",
      "  4508     0      2200  2200/4292       51% 0.656097               72.4375             \n",
      "  4609     0      2250  2250/4292       52% 0.648166               72.7969             \n",
      "  4708     0      2300  2300/4292       54% 0.647033               73.0469             \n",
      "  4806     0      2350  2350/4292       55% 0.657648               72.1562             \n",
      "  4909     0      2400  2400/4292       56% 0.654822               73.0000             \n",
      "  5010     0      2450  2450/4292       57% 0.639938               73.4219             \n",
      "  5108     0      2500  2500/4292       58% 0.667741               72.3594             \n",
      "  5207     0      2550  2550/4292       59% 0.666686               71.6250             \n",
      "  5309     0      2600  2600/4292       61% 0.650852               72.6562             \n",
      "  5406     0      2650  2650/4292       62% 0.679556               72.6719             \n",
      "  5508     0      2700  2700/4292       63% 0.655956               71.9531             \n",
      "  5611     0      2750  2750/4292       64% 0.638086               73.9219             \n",
      "  5709     0      2800  2800/4292       65% 0.650275               73.2812             \n",
      "  5812     0      2850  2850/4292       66% 0.667082               72.5781             \n",
      "  5903     0      2900  2900/4292       68% 0.607451               75.0156             \n",
      "  6005     0      2950  2950/4292       69% 0.629892               74.1875             \n",
      "  6167     0      3000  3000/4292       70% 0.640033 0.571510      73.6562      77.0270\n",
      "  6267     0      3050  3050/4292       71% 0.649895               72.8281             \n",
      "  6366     0      3100  3100/4292       72% 0.633438               74.0156             \n",
      "  6468     0      3150  3150/4292       73% 0.639660               73.2344             \n",
      "  6566     0      3200  3200/4292       75% 0.641108               73.8750             \n",
      "  6676     0      3250  3250/4292       76% 0.645400               73.0312             \n",
      "  6764     0      3300  3300/4292       77% 0.617128               74.4375             \n",
      "  6866     0      3350  3350/4292       78% 0.633882               73.3438             \n",
      "  6966     0      3400  3400/4292       79% 0.620138               74.4844             \n",
      "  7068     0      3450  3450/4292       80% 0.623680               74.3594             \n",
      "  7164     0      3500  3500/4292       82% 0.619588               74.2812             \n",
      "  7269     0      3550  3550/4292       83% 0.635854               73.6250             \n",
      "  7365     0      3600  3600/4292       84% 0.626489               74.0938             \n",
      "  7461     0      3650  3650/4292       85% 0.611794               74.6250             \n",
      "  7564     0      3700  3700/4292       86% 0.633411               73.7500             \n",
      "  7674     0      3750  3750/4292       87% 0.616602               74.7969             \n",
      "  7763     0      3800  3800/4292       89% 0.641962               73.4375             \n",
      "  7863     0      3850  3850/4292       90% 0.629198               74.4375             \n",
      "  7961     0      3900  3900/4292       91% 0.616730               74.6094             \n",
      "  8061     0      3950  3950/4292       92% 0.598551               75.2969             \n",
      "  8223     0      4000  4000/4292       93% 0.640365 0.561504      73.4531      77.6875\n",
      "  8330     0      4050  4050/4292       94% 0.634272               74.5625             \n",
      "  8426     0      4100  4100/4292       96% 0.606421               75.0469             \n",
      "  8522     0      4150  4150/4292       97% 0.604030               74.6094             \n",
      "  8623     0      4200  4200/4292       98% 0.615877               74.5938             \n",
      "  8717     0      4250  4250/4292       99% 0.595588               75.6875             \n"
     ]
    }
   ],
   "source": [
    "iterations = 0\n",
    "start = time.time()\n",
    "best_dev_acc = -1\n",
    "train_iter.repeat = False\n",
    "header = ('  Time Epoch Iteration    Progress (%Epoch)     Loss Dev/Loss'\n",
    "          '     Accuracy Dev/Accuracy')\n",
    "dev_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} '\n",
    "                            '{:>7.0f}%,{:>8.6f},{:>8.6f},{:>12.4f},{:>12.4f}'\n",
    "                            .split(','))\n",
    "log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} '\n",
    "                        '{:>7.0f}%,{:>8.6f},{},{:>12.4f},{}'.split(','))\n",
    "os.makedirs(args.save_path, exist_ok=True)\n",
    "print(header)\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(args, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
