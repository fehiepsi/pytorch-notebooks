{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "#import torchvision.datasets as datasets\n",
    "import mnist; importlib.reload(mnist)\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 10,\n",
    "    \"no_cuda\": False,\n",
    "    \"seed\": 7,\n",
    "    \"log_interval\": 10,\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**parser)\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/petar/GoMNIST/master/data/train-images-idx3-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/petar/GoMNIST/master/data/train-labels-idx1-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/petar/GoMNIST/master/data/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/petar/GoMNIST/master/data/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = DataLoader(\n",
    "    mnist.MNIST('../data', train=True, download=True,\n",
    "                transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(\n",
    "    mnist.MNIST('../data', train=False,\n",
    "                transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400) # image size = 28x28\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if args.cuda:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3)) # sigmoid force output in (0,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, _ in test_loader:\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 549.272766\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 309.734894\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 236.502090\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 219.546692\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 219.768967\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 206.733002\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 205.604736\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 194.939911\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 190.394043\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 188.267822\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 178.500061\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 168.829834\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 166.308289\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 161.739166\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 169.172302\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 167.673874\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 159.213394\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 162.529449\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 152.397125\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 156.624039\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 157.280853\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 150.802307\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 150.249359\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 147.849258\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 144.035187\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 146.629059\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 147.325348\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 138.698746\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 143.384750\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 136.126648\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 140.941956\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 141.631668\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 134.470520\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 138.197418\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 136.738403\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 137.243790\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 142.506668\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 128.507935\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 132.077652\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 134.704788\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 131.311310\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 134.106384\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 129.713852\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 132.379883\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 132.632324\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 129.413177\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 133.680969\n",
      "====> Epoch: 1 Average loss: 163.4164\n",
      "====> Test set loss: 126.6625\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 132.056671\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 125.497520\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 126.558144\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 128.416550\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 121.469307\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 127.367584\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 127.041451\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 127.338989\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 126.728195\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 125.042526\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 125.619446\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 130.296463\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 127.284843\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 123.410217\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 118.279396\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 120.632126\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 123.232498\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 120.447128\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 118.737000\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 118.215767\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 118.301086\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 117.273750\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 120.001724\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 118.081863\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 121.493690\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 116.126282\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 121.526703\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 123.366714\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 118.280670\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 117.315765\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 116.290276\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 118.251236\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 118.208138\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 120.722427\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 120.832825\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 119.337471\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 118.987625\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 120.806412\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 113.922050\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 116.851326\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 121.095650\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 115.909019\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 113.589142\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 119.837097\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 120.370895\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 119.990219\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 120.048401\n",
      "====> Epoch: 2 Average loss: 121.0003\n",
      "====> Test set loss: 115.3682\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 120.317169\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 115.757690\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 116.978210\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 115.771935\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 118.235504\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 115.232094\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 117.130203\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 115.947037\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 119.111664\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 115.508789\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 115.260574\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 114.171227\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 109.785599\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 114.881752\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 109.784401\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 117.768600\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 117.922531\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 115.533005\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 112.582886\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 115.686661\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 113.292717\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 119.292221\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 114.898468\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 115.155029\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 111.074799\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 113.875153\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 113.510780\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 116.972321\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 113.163803\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 109.054710\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 111.585976\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 114.752609\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 111.280930\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 112.464645\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 112.935249\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 111.858566\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 109.515450\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 114.573776\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 112.493332\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 109.636139\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 113.642372\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 110.744995\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 114.658165\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 114.255623\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 112.860428\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 111.548935\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 106.959770\n",
      "====> Epoch: 3 Average loss: 114.4288\n",
      "====> Test set loss: 112.0114\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 115.921379\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 113.263222\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 106.116714\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 108.443474\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 110.989861\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 107.827217\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 113.937805\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 111.354530\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 117.870575\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 106.674408\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 113.192360\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 110.061066\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 113.344963\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 110.560112\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 110.054726\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 111.949776\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 112.190628\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 111.801422\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 109.534805\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 106.952667\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 109.343079\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 107.784317\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 113.108124\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 109.510292\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 114.317291\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 115.916901\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 111.046967\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 113.815811\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 112.177315\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 113.137665\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 111.006454\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 112.410736\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 113.412468\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 113.088844\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 106.881958\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 110.093918\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 114.325607\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 109.603943\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 113.921371\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 111.529510\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 112.136192\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 111.296539\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 105.137894\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 108.486221\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 111.438644\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 112.824203\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 111.322884\n",
      "====> Epoch: 4 Average loss: 111.4935\n",
      "====> Test set loss: 109.6539\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 108.428528\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 112.732513\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 115.808235\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 112.790558\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 110.114410\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 108.384460\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 110.409416\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 107.419464\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 109.340088\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 114.475174\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 109.663071\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 107.600746\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 113.757904\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 108.385910\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 109.534058\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 110.101852\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 108.181137\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 111.802643\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 106.666824\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 108.758018\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 110.564125\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 109.859940\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 109.543167\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 109.634018\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 107.124947\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 113.305687\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 108.259926\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 111.872101\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 103.686859\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 110.729881\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 111.601288\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 108.798462\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 110.991791\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 111.168831\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 107.901237\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 111.774002\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 107.975822\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 109.367050\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 113.146576\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 108.566818\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 109.629066\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 108.343338\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 103.622116\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 112.920456\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 109.085266\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 110.015434\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 108.734505\n",
      "====> Epoch: 5 Average loss: 109.7255\n",
      "====> Test set loss: 108.3754\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 106.633026\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 111.235115\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 106.869827\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 110.719650\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 112.238037\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 106.879448\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 104.630798\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 107.122635\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 109.783386\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 108.828072\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 110.098557\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 107.594421\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 107.399330\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 109.264923\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 107.270096\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 109.199188\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 105.274643\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 107.325912\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 106.448959\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 109.281853\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 109.444572\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 111.185577\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 109.074806\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 111.696114\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 109.483788\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 112.069641\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 109.819778\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 106.237366\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 109.285500\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 111.887672\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 107.062927\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 108.118149\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 112.107269\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 110.580292\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 111.869690\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 105.187241\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 107.698364\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 105.762085\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 108.020554\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 105.783630\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 107.646217\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 108.035034\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 109.250023\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 105.363434\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 110.795944\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 106.831139\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 109.998642\n",
      "====> Epoch: 6 Average loss: 108.5362\n",
      "====> Test set loss: 107.4576\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 105.778946\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 105.975540\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 108.821365\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 102.299721\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 105.835327\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 108.252747\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 108.369110\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 103.929688\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 106.934540\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 107.557976\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 104.733719\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 109.249901\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 107.592484\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 107.328049\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 109.650085\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 107.114212\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 108.373337\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 112.818329\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 107.587540\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 106.532501\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 108.143959\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 110.093277\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 109.903908\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 106.137230\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 111.954895\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 108.216492\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 106.005638\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 105.526291\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 109.767273\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 105.230415\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 105.096039\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 102.765335\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 103.124069\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 103.990326\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 108.759575\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 104.703758\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 107.186905\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 106.252090\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 111.493973\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 105.705032\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 109.978745\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 105.813034\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 110.206154\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 107.807770\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 112.780273\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 107.668480\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 108.989273\n",
      "====> Epoch: 7 Average loss: 107.6884\n",
      "====> Test set loss: 106.8255\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 110.261475\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 107.219208\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 108.487228\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 105.065971\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 106.923019\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 106.870026\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 102.874054\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 104.597183\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 104.398026\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 107.953056\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 104.938011\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 109.844559\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 109.564186\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 104.538414\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 111.694923\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 103.994606\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 105.394295\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 105.175652\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 105.851257\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 106.807350\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 104.928177\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 110.472504\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 106.846909\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 109.247917\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 104.964355\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 102.537003\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 102.296249\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 102.874962\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 109.839905\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 107.843788\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 107.736702\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 106.909172\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 108.614914\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 107.516029\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 110.127357\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 104.689926\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 108.410049\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 106.661598\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 110.366302\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 106.246902\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 106.262383\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 105.230057\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 108.334740\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 109.322609\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 104.789948\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 109.584839\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 104.831810\n",
      "====> Epoch: 8 Average loss: 107.0436\n",
      "====> Test set loss: 106.2184\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 107.625870\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 106.723709\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 102.432388\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 104.166649\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 103.369553\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 109.340935\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 105.177383\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 106.874893\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 107.564201\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 107.675209\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 109.975830\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 107.081322\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 106.642738\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 104.204559\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 108.458511\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 106.793633\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 106.263596\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 106.034691\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 111.669891\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 104.363403\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 108.662598\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 107.762970\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 109.206726\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 106.082680\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 108.832962\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 105.157227\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 107.789825\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 110.855896\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 108.798973\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 103.893608\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 102.045654\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 106.599373\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 106.906303\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 107.824165\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 109.386887\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 108.535080\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 109.254364\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 105.058640\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 108.805733\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 107.406204\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 106.937828\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 106.644638\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 107.252991\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 105.389679\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 104.926323\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 104.352272\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 109.711029\n",
      "====> Epoch: 9 Average loss: 106.5493\n",
      "====> Test set loss: 105.8384\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 101.220787\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 106.416069\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 106.652878\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 105.152931\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 107.645233\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 106.252281\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 105.956413\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 109.341545\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 106.959351\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 109.580215\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 104.967636\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 108.101990\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 106.726425\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 105.646011\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 107.325394\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 105.865234\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 105.849152\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 102.499641\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 106.143822\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 104.936203\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 105.261032\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 103.887390\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 106.783112\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 105.085938\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 105.974632\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 104.651245\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 103.212387\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 109.129478\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 110.359718\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 105.046555\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 103.946526\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 106.380066\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 102.590843\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 104.665985\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 101.108841\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 105.167236\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 107.358490\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 107.683105\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 103.826263\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 110.788895\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 106.360291\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 106.614197\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 103.860382\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 106.913116\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 107.543686\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 104.569901\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 107.067245\n",
      "====> Epoch: 10 Average loss: 106.0951\n",
      "====> Test set loss: 105.4246\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data, _ in test_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    data = data.cuda()\n",
    "\n",
    "data = Variable(data, volatile=True)\n",
    "recon_batch, mu, logvar = model(data)\n",
    "recon = recon_batch.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = data.data[0, 0].cpu().numpy()\n",
    "recon_sample = recon.data[0,0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f43f57a7400>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAAD8CAYAAACsCeyFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVEwQUjFBSMUEIRUThFRMEFIxQUj1H569jFkhAwM0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f43f57d0c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.imshow(sample, vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.subplot(212)\n",
    "plt.imshow(recon_sample, vmin=0, vmax=1, cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
