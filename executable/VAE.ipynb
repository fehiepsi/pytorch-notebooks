{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mnist' from '/home/fehiepsi/Projects/pytorch-notebooks/executable/mnist.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/mnist/'\n",
    "    'epochs': 10,\n",
    "    'batch_size': 128,\n",
    "    \"cuda\": True,\n",
    "    \"seed\": 7,\n",
    "    \"worker\": 4,\n",
    "    \"log_interval\": 10\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400) # image size = 28x28\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if args.cuda:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3)) # sigmoid force output in (0,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, _ in test_loader:\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    mnist.MNIST(args.data_path, train=True, download=True,\n",
    "                transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=args.workers)\n",
    "test_loader = DataLoader(\n",
    "    mnist.MNIST(args.data_path, train=False,\n",
    "                transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=args.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "reconstruction_function = nn.BCELoss()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    reconstruction_function.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 549.163147\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 310.190155\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 237.475967\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 218.875137\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 220.173553\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 208.233978\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 206.940262\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 195.648148\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 192.501144\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 189.642365\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 182.816895\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 170.538513\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 169.197968\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 163.083633\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 168.747955\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 167.840469\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 160.396820\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 162.171478\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 153.103333\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 156.732697\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 158.013260\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 150.096634\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 149.840500\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 149.607040\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 145.616455\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 148.535431\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 147.663528\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 140.553711\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 143.657333\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 136.776245\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 142.280258\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 142.220276\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 134.094666\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 137.437286\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 136.628479\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 136.270767\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 141.784836\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 127.801376\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 131.264053\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 132.182388\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 131.851395\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 134.190826\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 129.227219\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 131.367630\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 132.360016\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 129.650314\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 133.130325\n",
      "====> Epoch: 1 Average loss: 163.8347\n",
      "====> Test set loss: 127.0414\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 126.950150\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 126.861267\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 128.686005\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 125.695435\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 126.475510\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 125.975342\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 125.390701\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 124.466103\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 119.632942\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 126.268440\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 125.244881\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 119.765366\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 126.588936\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 127.932816\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 122.089668\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 125.927841\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 120.163971\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 115.449814\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 124.764442\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 116.592148\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 122.082291\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 122.222847\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 120.302292\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 121.099495\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 120.770569\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 120.555595\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 113.644913\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 118.529663\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 117.039124\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 123.607483\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 118.372528\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 120.248581\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 114.936249\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 116.308228\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 121.187737\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 115.399773\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 121.990288\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 119.526947\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 113.263779\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 119.673187\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 117.416603\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 115.153664\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 122.168373\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 115.405182\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 121.106308\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 116.585358\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 118.580910\n",
      "====> Epoch: 2 Average loss: 121.5329\n",
      "====> Test set loss: 115.7741\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 116.161652\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 120.037064\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 116.536942\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 109.659164\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 118.167595\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 113.271881\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 113.496750\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 118.994095\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 111.951141\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 113.702744\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 117.282639\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 115.142036\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 114.470886\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 109.360664\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 112.352722\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 113.586212\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 114.930069\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 114.632370\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 112.821983\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 111.907707\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 115.949661\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 111.204552\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 119.020737\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 112.400352\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 114.218094\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 113.366158\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 118.686935\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 114.398552\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 110.301971\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 116.759148\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 113.079567\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 114.396423\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 116.682037\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 116.068718\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 115.751663\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 115.981041\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 115.987778\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 111.656937\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 113.933792\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 114.950844\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 116.738007\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 113.163208\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 112.061172\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 111.051407\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 111.841576\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 108.625870\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 110.953606\n",
      "====> Epoch: 3 Average loss: 114.5840\n",
      "====> Test set loss: 111.8274\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 114.894005\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 118.030525\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 109.906448\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 110.404251\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 113.272697\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 113.335754\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 111.826416\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 116.289017\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 106.615036\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 112.722687\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 110.791084\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 110.430374\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 111.956116\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 115.393471\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 106.421806\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 113.222061\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 116.466888\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 111.689903\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 117.033096\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 107.978539\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 105.452454\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 105.709732\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 112.982559\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 110.373703\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 110.437881\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 111.107330\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 108.298416\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 109.831963\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 106.798523\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 114.434464\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 111.643555\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 114.030334\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 110.081535\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 114.001175\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 111.569031\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 113.364548\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 112.273842\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 108.664108\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 110.558426\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 113.804321\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 110.274994\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 106.290787\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 110.718887\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 105.196884\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 107.656166\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 116.403656\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 112.845596\n",
      "====> Epoch: 4 Average loss: 111.4384\n",
      "====> Test set loss: 109.5646\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 110.196594\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 111.763847\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 114.248878\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 109.250923\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 107.968887\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 107.343750\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 106.888512\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 108.625359\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 112.582382\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 112.248009\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 110.476288\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 109.325851\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 107.496651\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 105.871643\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 106.391556\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 108.546669\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 106.881233\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 104.666077\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 109.652496\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 107.767715\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 111.170387\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 107.924835\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 107.520889\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 107.995888\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 112.823196\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 108.935089\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 110.825638\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 112.463768\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 109.040749\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 109.879074\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 111.962402\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 108.630104\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 106.000214\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 109.419090\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 106.492630\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 111.211929\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 106.854813\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 111.192238\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 109.953262\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 108.516876\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 111.669380\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 109.367630\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 110.390549\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 111.605072\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 108.972000\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 113.570152\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 109.387527\n",
      "====> Epoch: 5 Average loss: 109.6471\n",
      "====> Test set loss: 108.4133\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 108.309486\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 107.415985\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 109.141373\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 109.527313\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 108.554947\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 103.781433\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 108.634308\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 108.401978\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 112.697830\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 109.394379\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 106.451050\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 113.464699\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 102.704414\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 112.092400\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 109.899971\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 111.493408\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 110.682678\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 113.594376\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 109.963181\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 111.827217\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 110.496666\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 108.755692\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 112.108810\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 111.166183\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 109.680191\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 107.562538\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 107.952011\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 104.437050\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 105.686768\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 113.855385\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 107.793434\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 108.171425\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 105.247238\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 111.684120\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 107.246506\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 111.080063\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 106.781616\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 106.755241\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 107.820312\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 109.322159\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 104.396973\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 108.492126\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 109.628014\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 108.380409\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 107.865280\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 108.640129\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 107.944717\n",
      "====> Epoch: 6 Average loss: 108.3969\n",
      "====> Test set loss: 107.6443\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 104.334488\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 110.102333\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 105.599243\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 103.845085\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 106.529823\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 109.888557\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 111.603043\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 106.510597\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 104.138702\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 108.661415\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 110.206879\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 110.378815\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 111.470688\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 108.175369\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 109.129868\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 107.016876\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 108.887772\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 107.598412\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 107.416794\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 105.539001\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 103.221527\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 112.886177\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 103.735870\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 108.210602\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 105.001907\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 108.433304\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 109.060448\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 105.755280\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 107.780624\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 105.942665\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 108.095413\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 104.757324\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 108.034439\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 108.719055\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 104.680588\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 106.405380\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 110.958549\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 112.303780\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 108.975800\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 105.521942\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 105.332039\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 108.017456\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 106.598625\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 107.657684\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 106.869019\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 111.359116\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 105.081749\n",
      "====> Epoch: 7 Average loss: 107.5933\n",
      "====> Test set loss: 106.7663\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 107.172485\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 107.744682\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 106.065948\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 108.325211\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 102.840347\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 105.854988\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 107.621925\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 106.919136\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 105.684158\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 103.160721\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 109.325897\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 106.453629\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 107.136261\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 108.423996\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 109.266319\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 106.328476\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 104.487747\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 104.978775\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 107.229477\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 107.817657\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 110.071632\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 105.570175\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 106.062485\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 108.083084\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 106.533508\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 107.730324\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 105.140190\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 108.993164\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 110.247437\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 104.094765\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 103.558380\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 107.972145\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 108.592377\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 105.580780\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 105.782059\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 109.886169\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 105.838486\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 104.738686\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 106.221031\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 108.881950\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 107.988716\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 107.255432\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 103.779167\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 107.195557\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 105.404747\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 107.182457\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 107.539345\n",
      "====> Epoch: 8 Average loss: 106.9087\n",
      "====> Test set loss: 106.2682\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 103.256111\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 106.527115\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 107.552811\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 107.561577\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 107.539299\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 103.581871\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 105.839455\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 109.042976\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 109.459457\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 110.611633\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 106.857132\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 108.965820\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 105.309227\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 110.048965\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 109.944504\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 107.758751\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 104.374008\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 102.948586\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 106.618645\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 106.688126\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 107.968758\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 105.164566\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 104.497025\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 105.770889\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 107.992424\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 107.448914\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 103.635307\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 103.606796\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 101.430992\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 107.483284\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 105.911659\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 106.674187\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 106.955696\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 104.825714\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 106.343727\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 108.406754\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 107.012634\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 104.497429\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 108.004715\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 108.788231\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 105.743759\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 106.380569\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 104.007080\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 107.449249\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 106.560486\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 105.264236\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 105.980820\n",
      "====> Epoch: 9 Average loss: 106.4143\n",
      "====> Test set loss: 105.5411\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 106.705788\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 106.901093\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 103.863480\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 108.123398\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 110.112579\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 104.858475\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 103.513458\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 105.383553\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 109.228218\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 103.342339\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 104.542984\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 106.935242\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 105.025383\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 109.335083\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 105.090179\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 101.331635\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 107.330307\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 106.345428\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 106.090912\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 104.654961\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 103.408089\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 106.020981\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 107.555878\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 109.916779\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 103.094864\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 107.017097\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 108.924713\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 105.379532\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 110.706573\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 105.909325\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 105.195335\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 107.608337\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 106.414490\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 108.041412\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 109.223557\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 102.659889\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 102.645767\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 106.233307\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 106.259544\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 106.053024\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 104.183014\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 111.999489\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 105.405151\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 102.164467\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 106.718346\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 108.298126\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 109.720627\n",
      "====> Epoch: 10 Average loss: 106.0002\n",
      "====> Test set loss: 105.4717\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data, _ = iter(test_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    data = data.cuda()\n",
    "\n",
    "data = Variable(data, volatile=True)\n",
    "recon_batch, mu, logvar = model(data)\n",
    "recon = recon_batch.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample = data.data[0, 0].cpu().numpy()\n",
    "recon_sample = recon.data[0,0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f45168e9160>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAAD8CAYAAACsCeyFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQlT+DwuDaSxRecrzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4510079da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.imshow(sample, vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.subplot(212)\n",
    "plt.imshow(recon_sample, vmin=0, vmax=1, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Conditional VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython.core.debugger import Tracer; debug_here = Tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchnet.meter import AverageValueMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mnist' from '/home/fehiepsi/Projects/pytorch-notebooks/executable/mnist.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mnist; importlib.reload(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "intermediate_path = os.path.join(\"..\", \"intermediate\", \"vae\")\n",
    "if not os.path.exists(intermediate_path):\n",
    "    os.makedirs(intermediate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mb_size = 64\n",
    "epochs = 100\n",
    "Z_dim = 100\n",
    "X_dim = 784\n",
    "y_dim = 10\n",
    "h_dim = 128 # number of hidden unit\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff143262af8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    mnist.MNIST('../data', train=True, download=True,\n",
    "                transform=transforms.ToTensor()),\n",
    "    batch_size=mb_size, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    mnist.MNIST('../data', train=False,\n",
    "                transform=transforms.ToTensor()),\n",
    "    batch_size=mb_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Wxh = xavier_init(size=[X_dim + y_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Whz_mu = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Whz_var = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Q(X, c):\n",
    "    inputs = torch.cat([X, c], 1)\n",
    "    h = F.relu(inputs @ Wxh + bxh.repeat(inputs.size(0), 1))\n",
    "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n",
    "    z_logvar = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
    "    return z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sample_z(mu, log_var):\n",
    "    eps = Variable(torch.randn(mu.size(0), Z_dim))\n",
    "    return mu + torch.exp(log_var / 2) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def P(z, c):\n",
    "    inputs = torch.cat([z, c], 1)\n",
    "    h = F.relu(inputs @ Wzh + bzh.repeat(inputs.size(0), 1))\n",
    "    X = F.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\n",
    "          Wzh, bzh, Whx, bhx]\n",
    "\n",
    "solver = optim.Adam(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    losses = AverageValueMeter()\n",
    "    \n",
    "    for i, (X, c) in enumerate(train_loader):\n",
    "        X = X.view(-1, 784)\n",
    "        c_onehot = torch.zeros(c.size(0), y_dim).scatter_(1, c.unsqueeze(1), 1)\n",
    "        X = Variable(X)\n",
    "        c_onehot = Variable(c_onehot)\n",
    "\n",
    "        # Forward\n",
    "        z_mu, z_logvar = Q(X, c_onehot)\n",
    "        z = sample_z(z_mu, z_logvar)\n",
    "        X_sample = P(z, c_onehot)\n",
    "\n",
    "        # Loss\n",
    "        recon_loss = F.binary_cross_entropy(X_sample, X, size_average=False)\n",
    "        kl_loss = 0.5 * torch.sum(torch.exp(z_logvar) + z_mu**2 - 1 - z_logvar)\n",
    "        loss = recon_loss + kl_loss\n",
    "        \n",
    "        # Backward\n",
    "        for p in params:\n",
    "            p.grad.data.zero_()\n",
    "        loss.backward()\n",
    "        solver.step()\n",
    "        \n",
    "        losses.add(loss.data[0], X.size(0))\n",
    "    \n",
    "    print(\"Epoch: {0}/{1}\\tLoss: {2:.4f}\"\n",
    "          .format(epoch, epochs, losses.value()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot(samples, epoch):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis(\"off\")\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect(\"equal\")\n",
    "        plt.imshow(sample.reshape(28, 28), cmap=\"Greys_r\")\n",
    "\n",
    "    out_path = os.path.join(intermediate_path, \"out\")\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "    out_filepath = os.path.join(out_path, \n",
    "                                \"{}.png\".format(str(epoch).zfill(3)))\n",
    "    plt.savefig(out_filepath, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\tLoss: 183.9763\n",
      "Epoch: 2/100\tLoss: 135.6942\n",
      "Epoch: 3/100\tLoss: 123.3090\n",
      "Epoch: 4/100\tLoss: 116.8960\n",
      "Epoch: 5/100\tLoss: 113.2857\n",
      "Epoch: 6/100\tLoss: 111.1799\n",
      "Epoch: 7/100\tLoss: 109.7620\n",
      "Epoch: 8/100\tLoss: 108.6592\n",
      "Epoch: 9/100\tLoss: 107.9670\n",
      "Epoch: 10/100\tLoss: 107.3998\n",
      "Epoch: 11/100\tLoss: 106.8704\n",
      "Epoch: 12/100\tLoss: 106.4485\n",
      "Epoch: 13/100\tLoss: 106.1208\n",
      "Epoch: 14/100\tLoss: 105.8686\n",
      "Epoch: 15/100\tLoss: 105.6030\n",
      "Epoch: 16/100\tLoss: 105.3722\n",
      "Epoch: 17/100\tLoss: 105.1703\n",
      "Epoch: 18/100\tLoss: 104.9846\n",
      "Epoch: 19/100\tLoss: 104.8853\n",
      "Epoch: 20/100\tLoss: 104.6661\n",
      "Epoch: 21/100\tLoss: 104.5560\n",
      "Epoch: 22/100\tLoss: 104.4186\n",
      "Epoch: 23/100\tLoss: 104.2742\n",
      "Epoch: 24/100\tLoss: 104.2110\n",
      "Epoch: 25/100\tLoss: 104.0873\n",
      "Epoch: 26/100\tLoss: 103.9460\n",
      "Epoch: 27/100\tLoss: 103.8903\n",
      "Epoch: 28/100\tLoss: 103.7722\n",
      "Epoch: 29/100\tLoss: 103.7146\n",
      "Epoch: 30/100\tLoss: 103.6487\n",
      "Epoch: 31/100\tLoss: 103.5393\n",
      "Epoch: 32/100\tLoss: 103.4941\n",
      "Epoch: 33/100\tLoss: 103.3744\n",
      "Epoch: 34/100\tLoss: 103.3083\n",
      "Epoch: 35/100\tLoss: 103.2738\n",
      "Epoch: 36/100\tLoss: 103.1467\n",
      "Epoch: 37/100\tLoss: 103.1140\n",
      "Epoch: 38/100\tLoss: 103.0779\n",
      "Epoch: 39/100\tLoss: 103.0274\n",
      "Epoch: 40/100\tLoss: 102.9632\n",
      "Epoch: 41/100\tLoss: 102.9443\n",
      "Epoch: 42/100\tLoss: 102.8833\n",
      "Epoch: 43/100\tLoss: 102.8265\n",
      "Epoch: 44/100\tLoss: 102.7826\n",
      "Epoch: 45/100\tLoss: 102.7831\n",
      "Epoch: 46/100\tLoss: 102.6811\n",
      "Epoch: 47/100\tLoss: 102.6455\n",
      "Epoch: 48/100\tLoss: 102.6682\n",
      "Epoch: 49/100\tLoss: 102.5984\n",
      "Epoch: 50/100\tLoss: 102.5914\n",
      "Epoch: 51/100\tLoss: 102.5424\n",
      "Epoch: 52/100\tLoss: 102.5007\n",
      "Epoch: 53/100\tLoss: 102.5242\n",
      "Epoch: 54/100\tLoss: 102.4114\n",
      "Epoch: 55/100\tLoss: 102.4117\n",
      "Epoch: 56/100\tLoss: 102.3713\n",
      "Epoch: 57/100\tLoss: 102.3462\n",
      "Epoch: 58/100\tLoss: 102.3334\n",
      "Epoch: 59/100\tLoss: 102.3013\n",
      "Epoch: 60/100\tLoss: 102.2776\n",
      "Epoch: 61/100\tLoss: 102.2044\n",
      "Epoch: 62/100\tLoss: 102.2202\n",
      "Epoch: 63/100\tLoss: 102.1665\n",
      "Epoch: 64/100\tLoss: 102.1878\n",
      "Epoch: 65/100\tLoss: 102.1732\n",
      "Epoch: 66/100\tLoss: 102.1111\n",
      "Epoch: 67/100\tLoss: 102.1665\n",
      "Epoch: 68/100\tLoss: 102.1138\n",
      "Epoch: 69/100\tLoss: 102.0452\n",
      "Epoch: 70/100\tLoss: 102.0319\n",
      "Epoch: 71/100\tLoss: 102.0395\n",
      "Epoch: 72/100\tLoss: 102.0262\n",
      "Epoch: 73/100\tLoss: 101.9988\n",
      "Epoch: 74/100\tLoss: 101.9577\n",
      "Epoch: 75/100\tLoss: 101.9787\n",
      "Epoch: 76/100\tLoss: 101.9326\n",
      "Epoch: 77/100\tLoss: 101.9041\n",
      "Epoch: 78/100\tLoss: 101.9163\n",
      "Epoch: 79/100\tLoss: 101.9433\n",
      "Epoch: 80/100\tLoss: 101.8609\n",
      "Epoch: 81/100\tLoss: 101.8344\n",
      "Epoch: 82/100\tLoss: 101.8264\n",
      "Epoch: 83/100\tLoss: 101.8008\n",
      "Epoch: 84/100\tLoss: 101.8290\n",
      "Epoch: 85/100\tLoss: 101.7915\n",
      "Epoch: 86/100\tLoss: 101.7874\n",
      "Epoch: 87/100\tLoss: 101.7911\n",
      "Epoch: 88/100\tLoss: 101.7234\n",
      "Epoch: 89/100\tLoss: 101.7318\n",
      "Epoch: 90/100\tLoss: 101.7265\n",
      "Epoch: 91/100\tLoss: 101.6982\n",
      "Epoch: 92/100\tLoss: 101.6926\n",
      "Epoch: 93/100\tLoss: 101.6289\n",
      "Epoch: 94/100\tLoss: 101.6162\n",
      "Epoch: 95/100\tLoss: 101.6166\n",
      "Epoch: 96/100\tLoss: 101.6117\n",
      "Epoch: 97/100\tLoss: 101.5757\n",
      "Epoch: 98/100\tLoss: 101.5911\n",
      "Epoch: 99/100\tLoss: 101.5719\n",
      "Epoch: 100/100\tLoss: 101.5661\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "    c = torch.from_numpy(np.random.randint(0, 10, mb_size))\n",
    "    c_onehot = torch.zeros(c.size(0), y_dim).scatter_(1, c.unsqueeze(1), 1)\n",
    "    z = torch.randn(mb_size, Z_dim)\n",
    "    samples = P(Variable(z), Variable(c_onehot)).data.numpy()[:16]\n",
    "    plot(samples, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Denoising VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchnet.meter import AverageValueMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "intermediate_path = os.path.join(\"..\", \"intermediate\", \"dvae\")\n",
    "if not os.path.exists(intermediate_path):\n",
    "    os.makedirs(intermediate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mb_size = 64\n",
    "epochs = 100\n",
    "Z_dim = 100\n",
    "X_dim = 784\n",
    "y_dim = 10\n",
    "h_dim = 128 # number of hidden unit\n",
    "lr = 1e-3\n",
    "noise_factor = .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa71d24ab10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    mnist.MNIST('../data', train=True, download=True,\n",
    "                transform=transforms.ToTensor()),\n",
    "    batch_size=mb_size, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    mnist.MNIST('../data', train=False,\n",
    "                transform=transforms.ToTensor()),\n",
    "    batch_size=mb_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whz_mu = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "Whz_var = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Q(X):\n",
    "    h = F.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n",
    "    z_var = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
    "    return z_mu, z_var\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = Variable(torch.randn(mu.size(0), Z_dim))\n",
    "    return mu + torch.exp(log_var / 2) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def P(z):\n",
    "    h = F.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
    "    X = F.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\n",
    "          Wzh, bzh, Whx, bhx]\n",
    "\n",
    "solver = optim.Adam(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    losses = AverageValueMeter()\n",
    "    \n",
    "    for i, (X, _) in enumerate(train_loader):\n",
    "        X = X.view(-1, 784)\n",
    "        X = Variable(X)\n",
    "        \n",
    "        # Add noise\n",
    "        X_noise = X + noise_factor * Variable(torch.randn(X.size()))\n",
    "        X_noise.data.clamp_(0., 1.)\n",
    "\n",
    "        # Forward\n",
    "        z_mu, z_logvar = Q(X_noise)\n",
    "        z = sample_z(z_mu, z_logvar)\n",
    "        X_sample = P(z)\n",
    "\n",
    "        # Loss\n",
    "        recon_loss = F.binary_cross_entropy(X_sample, X, size_average=False)\n",
    "        kl_loss = 0.5 * torch.sum(torch.exp(z_logvar) + z_mu**2 - 1 - z_logvar)\n",
    "        loss = recon_loss + kl_loss\n",
    "        \n",
    "        # Backward\n",
    "        for p in params:\n",
    "            p.grad.data.zero_()\n",
    "        loss.backward()\n",
    "        solver.step()\n",
    "        \n",
    "        losses.add(loss.data[0], X.size(0))\n",
    "    \n",
    "    print(\"Epoch: {0}/{1}\\tLoss: {2:.4f}\"\n",
    "          .format(epoch, epochs, losses.value()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot(samples, epoch):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis(\"off\")\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect(\"equal\")\n",
    "        plt.imshow(sample.reshape(28, 28), cmap=\"Greys_r\")\n",
    "\n",
    "    out_path = os.path.join(intermediate_path, \"out\")\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "    out_filepath = os.path.join(out_path, \n",
    "                                \"{}.png\".format(str(epoch).zfill(3)))\n",
    "    plt.savefig(out_filepath, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\tLoss: 194.1767\n",
      "Epoch: 2/100\tLoss: 147.8750\n",
      "Epoch: 3/100\tLoss: 135.4952\n",
      "Epoch: 4/100\tLoss: 128.0893\n",
      "Epoch: 5/100\tLoss: 123.8210\n",
      "Epoch: 6/100\tLoss: 121.3719\n",
      "Epoch: 7/100\tLoss: 119.9865\n",
      "Epoch: 8/100\tLoss: 118.9402\n",
      "Epoch: 9/100\tLoss: 118.2988\n",
      "Epoch: 10/100\tLoss: 117.7279\n",
      "Epoch: 11/100\tLoss: 117.2896\n",
      "Epoch: 12/100\tLoss: 116.9489\n",
      "Epoch: 13/100\tLoss: 116.7146\n",
      "Epoch: 14/100\tLoss: 116.4792\n",
      "Epoch: 15/100\tLoss: 116.2661\n",
      "Epoch: 16/100\tLoss: 116.0227\n",
      "Epoch: 17/100\tLoss: 115.8309\n",
      "Epoch: 18/100\tLoss: 115.6137\n",
      "Epoch: 19/100\tLoss: 115.5089\n",
      "Epoch: 20/100\tLoss: 115.3506\n",
      "Epoch: 21/100\tLoss: 115.2817\n",
      "Epoch: 22/100\tLoss: 115.1794\n",
      "Epoch: 23/100\tLoss: 115.1692\n",
      "Epoch: 24/100\tLoss: 115.0326\n",
      "Epoch: 25/100\tLoss: 114.9706\n",
      "Epoch: 26/100\tLoss: 114.9115\n",
      "Epoch: 27/100\tLoss: 114.8957\n",
      "Epoch: 28/100\tLoss: 114.8248\n",
      "Epoch: 29/100\tLoss: 114.7825\n",
      "Epoch: 30/100\tLoss: 114.7367\n",
      "Epoch: 31/100\tLoss: 114.6962\n",
      "Epoch: 32/100\tLoss: 114.6539\n",
      "Epoch: 33/100\tLoss: 114.5837\n",
      "Epoch: 34/100\tLoss: 114.5739\n",
      "Epoch: 35/100\tLoss: 114.5569\n",
      "Epoch: 36/100\tLoss: 114.5258\n",
      "Epoch: 37/100\tLoss: 114.4705\n",
      "Epoch: 38/100\tLoss: 114.4477\n",
      "Epoch: 39/100\tLoss: 114.4396\n",
      "Epoch: 40/100\tLoss: 114.4377\n",
      "Epoch: 41/100\tLoss: 114.3382\n",
      "Epoch: 42/100\tLoss: 114.3301\n",
      "Epoch: 43/100\tLoss: 114.3370\n",
      "Epoch: 44/100\tLoss: 114.2664\n",
      "Epoch: 45/100\tLoss: 114.2651\n",
      "Epoch: 46/100\tLoss: 114.1990\n",
      "Epoch: 47/100\tLoss: 114.1916\n",
      "Epoch: 48/100\tLoss: 114.1766\n",
      "Epoch: 49/100\tLoss: 114.1443\n",
      "Epoch: 50/100\tLoss: 114.1189\n",
      "Epoch: 51/100\tLoss: 114.1030\n",
      "Epoch: 52/100\tLoss: 114.1618\n",
      "Epoch: 53/100\tLoss: 114.1074\n",
      "Epoch: 54/100\tLoss: 114.0632\n",
      "Epoch: 55/100\tLoss: 114.0023\n",
      "Epoch: 56/100\tLoss: 113.9762\n",
      "Epoch: 57/100\tLoss: 113.9958\n",
      "Epoch: 58/100\tLoss: 113.9801\n",
      "Epoch: 59/100\tLoss: 113.9570\n",
      "Epoch: 60/100\tLoss: 113.9673\n",
      "Epoch: 61/100\tLoss: 113.9586\n",
      "Epoch: 62/100\tLoss: 113.9111\n",
      "Epoch: 63/100\tLoss: 113.8756\n",
      "Epoch: 64/100\tLoss: 113.8379\n",
      "Epoch: 65/100\tLoss: 113.9185\n",
      "Epoch: 66/100\tLoss: 113.8947\n",
      "Epoch: 67/100\tLoss: 113.8809\n",
      "Epoch: 68/100\tLoss: 113.8265\n",
      "Epoch: 69/100\tLoss: 113.8354\n",
      "Epoch: 70/100\tLoss: 113.7931\n",
      "Epoch: 71/100\tLoss: 113.7897\n",
      "Epoch: 72/100\tLoss: 113.8356\n",
      "Epoch: 73/100\tLoss: 113.7327\n",
      "Epoch: 74/100\tLoss: 113.7265\n",
      "Epoch: 75/100\tLoss: 113.7319\n",
      "Epoch: 76/100\tLoss: 113.7318\n",
      "Epoch: 77/100\tLoss: 113.6777\n",
      "Epoch: 78/100\tLoss: 113.7350\n",
      "Epoch: 79/100\tLoss: 113.6594\n",
      "Epoch: 80/100\tLoss: 113.6933\n",
      "Epoch: 81/100\tLoss: 113.6886\n",
      "Epoch: 82/100\tLoss: 113.6507\n",
      "Epoch: 83/100\tLoss: 113.6577\n",
      "Epoch: 84/100\tLoss: 113.6347\n",
      "Epoch: 85/100\tLoss: 113.5928\n",
      "Epoch: 86/100\tLoss: 113.6118\n",
      "Epoch: 87/100\tLoss: 113.5803\n",
      "Epoch: 88/100\tLoss: 113.6090\n",
      "Epoch: 89/100\tLoss: 113.6073\n",
      "Epoch: 90/100\tLoss: 113.5435\n",
      "Epoch: 91/100\tLoss: 113.5949\n",
      "Epoch: 92/100\tLoss: 113.5660\n",
      "Epoch: 93/100\tLoss: 113.5496\n",
      "Epoch: 94/100\tLoss: 113.5261\n",
      "Epoch: 95/100\tLoss: 113.5355\n",
      "Epoch: 96/100\tLoss: 113.4854\n",
      "Epoch: 97/100\tLoss: 113.4771\n",
      "Epoch: 98/100\tLoss: 113.4600\n",
      "Epoch: 99/100\tLoss: 113.4164\n",
      "Epoch: 100/100\tLoss: 113.4832\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "    z = torch.randn(mb_size, Z_dim)\n",
    "    samples = P(Variable(z)).data.numpy()[:16]\n",
    "    plot(samples, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
